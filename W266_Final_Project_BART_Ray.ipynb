{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vagabond008/mids-266-Dipika-Jack-Ray/blob/main/W266_Final_Project_BART_Ray.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvILg5xsh_KH"
      },
      "source": [
        "### 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U1qL94HGneD9"
      },
      "outputs": [],
      "source": [
        "# install packages\n",
        "!pip install -q transformers==4.37.2\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q evaluate\n",
        "!pip install -q tensorflow==2.15\n",
        "!pip install -q rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G4uHjAq0l9Uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d21c26-8220-4145-9438-0772675e246b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow v2.15.0\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"Tensorflow v{tf.__version__}\")\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from transformers import BartTokenizer, TFBartForConditionalGeneration, AutoTokenizer\n",
        "\n",
        "from google.colab import drive\n",
        "#from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw0AnPIYkaF8",
        "outputId": "63e9646d-e41a-4bd9-8a5a-e87f409c061f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 10 20:11:47 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   42C    P8              17W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4N5QZwQmbb5",
        "outputId": "fb6d2dbe-a807-4494-911c-0003c4dbc652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount Colab to Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SaMha8DNmjFg"
      },
      "outputs": [],
      "source": [
        "# verify data exists in Google Drive dir\n",
        "#!ls 'drive/My Drive/W266'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjySjw3dmp3h"
      },
      "source": [
        "### 2. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p8snJGQumomB"
      },
      "outputs": [],
      "source": [
        "# set display to max width\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# load cleaned Reddit ds\n",
        "# df = pd.read_csv('drive/My Drive/W266/reddit_subset_cleaned.csv')\n",
        "# df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "864y8nsXmmTB"
      },
      "outputs": [],
      "source": [
        "# df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zhXBi0CAy_qS"
      },
      "outputs": [],
      "source": [
        "# Initialize pairs list\n",
        "pairs = []\n",
        "\n",
        "# # Iterate over each row in the DataFrame\n",
        "# for index, row in df.iterrows():\n",
        "#     post = row['post']\n",
        "#     title = row['title']\n",
        "#     pairs.append({'post': post, 'title': title})\n",
        "\n",
        "# # Inspect random text pairs\n",
        "# for text in range(2):\n",
        "#     print(np.random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hEH99r-Ndvcn"
      },
      "outputs": [],
      "source": [
        "# Split data: 80% Train, 15% Test, 5% Validation\n",
        "\n",
        "generate_pairs = False\n",
        "\n",
        "if generate_pairs:\n",
        "\n",
        "  np.random.shuffle(pairs)\n",
        "  num_valid_samples = int(0.10 * len(pairs))\n",
        "  num_train_samples = len(pairs) - 2 * num_valid_samples #allocating 80% of dataset for training\n",
        "\n",
        "  train_pairs = pairs[:num_train_samples]\n",
        "  valid_pairs = pairs[num_train_samples : int(num_train_samples + num_valid_samples * 1.5)]\n",
        "  test_pairs = pairs[int(num_train_samples + num_valid_samples * 1.5):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ai4T6lpOdvcn"
      },
      "outputs": [],
      "source": [
        "# Regenerate train, validaiton, test file\n",
        "generate_files = False\n",
        "\n",
        "# Save splits to separate csv files, to load only part at a time later\n",
        "train_file = 'drive/My Drive/W266/train_pairs.csv'\n",
        "valid_file = 'drive/My Drive/W266/valid_pairs.csv'\n",
        "test_file = 'drive/My Drive/W266/test_pairs.csv'\n",
        "\n",
        "if generate_files:\n",
        "  pd.DataFrame(train_pairs).to_csv(train_file, index=False)\n",
        "  pd.DataFrame(valid_pairs).to_csv(valid_file, index=False)\n",
        "  pd.DataFrame(test_pairs).to_csv(test_file, index=False)\n",
        "\n",
        "# Load the CSV files into lists of dictionaries\n",
        "train_pairs = pd.read_csv(train_file).to_dict('records')\n",
        "valid_pairs = pd.read_csv(valid_file).to_dict('records')\n",
        "test_pairs = pd.read_csv(test_file).to_dict('records')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{len(pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(valid_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "id": "qdipV6-f5e2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25babfb1-303c-452b-ea5e-432ab8457c93"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 total pairs\n",
            "20000 training pairs\n",
            "3750 validation pairs\n",
            "1250 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYAewiENdvcn",
        "outputId": "11d227cd-9703-429e-ccb4-0dac149372da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training pairs:\n",
            "[{'post': 'my goal is to detect an object that is being flashed in front of a camera therefore the input is a video converted into images frames and the sequence matters i am trying to figure out how to go about training it usually video object detection algorithms detect objects in each frame my problem is that the objects i am trying to classify are similar and the object is not fully visible in any single frame because of a hand holding it in order to correctly tell what the object is you have to look at multiple frames i found which looks at the data sequentially but i am not sure how to map it to my situation anyone else knows about any implementations of a similar problem or potential ways of solving it', 'title': 'video object detection detecting object in the video frames sequentially'}, {'post': 'hi everyone for my master is thesis at the vrije universiteit amsterdam i am researching the environment for it professionals to report wrongdoing related to software examples of wrongdoing related to software are privacy and security issues discrimination happening within the software eg favoring certain users by an algorithm or allowing hate speech against specific user groups enabling fraud or corruption endangering someone is health and safety eg motivating addictive behavior triggering depressing thoughts and damaging the environment eg algorithms with too many co emissions if you are an it professional eg software engineer software tester product owner software consultant data scientist cs researcher please help me out by filling out the survey it takes minutes to complete thanks', 'title': 'reporting wrongdoing related to software'}, {'post': 'hello i am a postgrad in neuroscience and this is my first time using ml for my thesis the basis of my project is to compare the accuracy of random forest svm and decision trees on clinical prediction of depression with that of logistic regression based on brain imaging measures fractional anisotropy and mean diffusivity of white matter tracts with a sample size of around k to k people so far i am a bit overwhelmed and i am sorry if this question seems naïve but i was wondering how to handle missing values and outliers from what i understand each model has different methods impute data delete rows etc that fit best but i am still not sure i feel confident enough to understand the nuances of it so i was considering to just delete the rows with missing data and winsorizing outliers is there a better alternative given my notsodeep knowledge background thank you', 'title': 'missing values and outliers question'}, {'post': 'hi i am working on a project for my portfolio trying to predict employee turnover binary via logistic regression there are about variables in the dataset that could potentially be related to turnover in my experience i studied psychological science it is best practice to only include variables in your model that you have reason to believe are related to your outcome variable for example through previous research or subject matter expertise however i see in youtube tutorials of logistic regression people just throwing every variable directly in the model should i be exploring the relationship between each possible predictor and the outcome variable to establish a relationship first before building my model for example via running a chisquare of independence test on a potential categorical predictor and my outcome variable or is this redundant as the logistic regression model will sus out the nonsignificant predictors tldr what are the best practices for feature selection in logistic regression should i do exploratory data analysis for every possible predictor before building my model to rationalize their inclusion in the model or just throw them all in thank you for bearing with me', 'title': 'to explore relationships between predictors and outcome variable before building model or not'}, {'post': 'i must warn before i begin that i am coming from the hard science sphere and am trying to integrate some novel data science so some of the things i talk about might make no sense please ask questions i have a database of variables that are collected continuously over time this database is essentially measuring the conditions of the interplanetary magnetic field too complex to model which drives numerous phenomena that occur in the earth is atmosphere etc these phenomena are of interest to researchers but only occur ever so often to find these you have to manually search years and years of data something that is simply not too practical i want to develop some sort of method to take a number of instances of a phenomenon and search the continuous database for more instances i think that this needs to be split into two tasks discover common patterns dips spikes sign changes or even more complex shapes between the inputted instances it will also have to discern the variables of interest because not all in the database will drive a single phenomenon moreover if the program can then output these relationships such as variables appear to drive the phenomenon this will be of interest to users use these relationships to search the continuous stream for more instances like i mentioned i am not very experienced in data science of course i am not asking anyone here to develop this for me or anything but it would be really helpful if someone could point me in the right direction eventually i would like to develop some sort of robust methodology that can be applied to many scientific fields', 'title': 'identifying patterns in time series data with multiple variables'}]\n",
            "\n",
            "Validation pairs:\n",
            "[{'post': 'i just came up with a data sciencey project idea but i need help with the math side of it i will take studies that have found a correlation between cannabis use in undeveloped brains and decreased cognitive capacity and for the sake of this example i will input variables average frequency of consumption and quantity consumed i will take this through a python script and i will output a mathematical function that connects two percentages the probability first percentage x that you will be y less capable than the average nonsmoker person in either shortterm memory attention span etc the problem is i know literally no statistics so i am kinda struggling with the math side is this what you would call a linear regression what is the actual algorithm for outputting this mathematical function i first thought i would have to just do some sort of arithmetic mean but it got more complicated when you had more values for example i can have this data set a bigger sample would give more accuracy but i am gonna give a sample of so this example is easier let f be the average frequency of use in days how much days do they wait in between smoke sessions let q be the average quantity they consume in each session in grams or whatever unit of measure does not matter user f q user f q user f q each user will also have a variable representing how much less capable they are than the average person on a certain mental task in reality i am probably gonna have a list of variables for each user so i compute a separate mathematical function for each mental ability memory attention span etc but for the sake of this example let is say they have just one more variable representing how worse they are from the average person at attention span let it be a user a user a user a now i have to use all this data to compute a mathematical function f s gt s f q p f q are from r p is from which takes in the percentage of probability that the output is gonna happen as well as f and q and outputs the percentage decrease in attention span from the normal person basically a now that i think of it it should work in reverse too inputting the percentage decrease and outputting the probability that will happen so this function also has to be bijective actually it is not bijective per se because i just reverse a and p i add a in the domain set s and take out p in the output anyway now what i need help with is computing the larger function that takes in two variables and outputs the function f and that larger function is fed before two lists of variables to have the proper data that will be the script i will write i am stumped because it is taking in three lists of variables and outputs if it was just two outputting it would be easier for example user f a user f a user f a it is way easier to do it now having to output just a if i input or i know what i am going to get if i output anything else between and i just have to find out the relation between the two closest variables from to f halved and a tripled this would mean that most likely if f quarters then a would be multiplied by if f is divided by a would be multiplied by etc that is for any variable between and between and i do the same with the other data is this a correct approach either way if you also have to add the output p besides the output a i am stuck then you also add the input q and i am even more stuck so what do i have to do i know how to compute derivatives and to work with matrices and determinants in case that is needed', 'title': 'how do i compute this percentage from the available data is this a linear regression data science project'}, {'post': 'my client comes up with a huge volume of reddit data and she wants to know what people are talking about a particular product she is expecting some insights for their next product planning the data is around k conversations for just days after removing promo and spam the dead line is just a few days ahead now how should we do it the team is reading each conversations and tagging them manually which is absolutely not the right way and framing keywords reddit data has huge conversations so keywords often captures irrelevant data any suggestions on how to overcome this or my clientnot a technical person has unrealistic expectations is this even possible to gain insights for a general product from huge amount of very detailed user generated content', 'title': 'help text analytics extracting insights from a huge user generated data'}, {'post': 'went to school for stats did undergrad research in economics department i really enjoy r and python forming my work around larger questions filtering and cleaning large datasets building and tuning predictive models i really needed work and took a mobile development job a few years ago out of school since i had experience with swift and java and have been there since i think i hate just being given requirements and told build this thing with no context or background given i feel like all the other engineers love it and they only care about cleanbeautifulconcisewhatever code and making sure the marginspaddings are just perfect having beautiful ui drooling over newest mobile frameworks etc frankly past a certain point i do not really give a fuck about that i guess i am more of a storyteller and enjoy being given some data and some questions that needed to be answered or explored and going to town i am think i am more interested in solving business problems i like tuning and improving modelsalthough have not really done it in like years since i have been in mobile i got into a really good graduate program which will certainly get me into solid data science positions at many companies sometimes i feel that it is just a copout though and i will run into the same shit working in a data science position can i expect a change of pace from mobile development to data scienceengineering roll', 'title': 'can i expect change of pace from se to ds'}, {'post': 'is there anywhere where i can find a free copy of wikipedia is xml data dumps i have got a web app which as of now crawls wikipedia to get data about page links i know that data retrieval is forbidden under wikimedia is bot policy and also that they regularly release data dumps they ask you to host your own copy if you need to dynamically load pages from another website my entire website is running on a gb digitalocean droplet which is not nearly enough space for a wikipedia data dump so i would like to find somewhere where one of these data dumps is publicly hosted does this exist i am not concerned about images etc just about the text of pages including links', 'title': 'request wikipedia xml data dump hosted'}, {'post': 'hi guys i am a senior at carnegie mellon university thinking about grad school i got accepted into carnegie mellon is master is of statistical practice which is a one year program that will run me around k a lot of money for me i am still figuring out my career goals and was looking for advice on what to do ampxb i am interviewing for full time data analytics roles now i know i will want an mba in the future because my goal and core skillset lies in business not programming or data analytics but i do not know i do think data science is really cool and the master is program definitely prepares you for it i also do not think i will be able to get into a good oneyear program again in the future if i do not accept it now ampxb so my questions are is carnegie mellon is grad program in statistical practice wellrecognized by the industry will it help me get interviews is grad degree in statistics worth the time and money how hard is it to switch from data analytics to data science how do i know data science is for me honestly speaking i am partly interested in it because it is kind of the hot thing right now but currently i am not good at programming or statistical theory i can manage in my stats classes though ampxb thank you so much', 'title': 'grad school or industry'}]\n",
            "\n",
            "Test pairs:\n",
            "[{'post': 'some background i have been learning python for over a year now and i know some sql a bit of r and i have completed some small projects using data science data engineering practices i also know how to work in excel but i do not really have experience using databases i am totally willing to make the time and money investment in something like a bootcamp and i have the means to do fulltime training but i do not want to do this if there is a better faster way to get into the industry what i really want to know is what can i do that will get me a job in the field asap is there some specific bootcamp that will make this happen if so what are the best bootcamps or some particular tech skill i could learn that would basically guarantee that i am hireable very soon if i something like learned microsoft sql server or tableau and given my other skills would this be likely to get me hired i have been looking into bootcamps like thinkful springboard and data application lab the concern i have about these is actually that i already know a lot of the stuff they teach and i am worried that these will be a waste of time and not elevate me to where i want to be i also worry about it taking months to complete these programs as they estimate i would like to be finished in no more than about months thoughts', 'title': 'i want to transition into the data scienceanalyst or related field rather than asking whether i should choose a particular boootcamp or learn some language i would like to hear opinions on what path should i choose that will land me a job of some kind in the field as soon as possible'}, {'post': 'hi all i have created a python module to extract sngrams which is different from traditional ngrams as it embodies linguistic syntactic trees thus making it less arbitrary than traditional ngrams as it goes without saying quality of input feature affects model performance this will help you improve your model accuracy even further built on language models of spacy it can help especially for text classification information extraction query understanding machine translation question answering systems below is an example from sngramextractor import sngramextractor ampxb sngram_objsngramextractortextmeta_tagoriginaltrigram_flagyes texteconomic news have little effect on financial markets outputsngram_objget_sngram printtext print isngram bigramoutput isnbigram print isngram trigramoutput isntrigram economic news have little effect on financial markets sngram bigram cloud_every has_cloud lining_a lining_silver has_lining sngram trigram has_lining_silver ampxb textevery cloud has a silver lining outputsngram_objget_sngram printtext print isngram bigramoutput isnbigram print isngram trigramoutput isntrigram ampxb every cloud has a silver lining sngram bigram cloud_every has_cloud lining_a lining_silver has_lining sngram trigram has_lining_silver ampxb pypi', 'title': 'psngram linguistic features for improving machine learning and deep learning model accuracy for the first time in python new release'}, {'post': 'i am looking at some of the modeling programs the company has been using and there are options to smooth binned data layered linear on bin linear on avg linear on log avg and variable gradient are these methods commonly used in any algorithms i am not sure how i would implement it into r ampxb variable gradient is the most confusing it groups data from neighboring bins with a bin factor being determined the level of smoothing is determined by the radius all bins contribute to the calculation of the bin factor with decreasing weight as the distance from the target bin increases the weight is equal to radius distance radius ampxb once you calculate the overall factor a calculation is performed for each bin weighted by an exposure with various radii and a selected credibility level the calculation that yields the largest distance from is then selected as the factor to be used for that bin ampxb over multiple iterations of the model the factors are blended until the model converges and we receive a final coefficient for that bin ampxb since it is a program i have no clue what is going on in the background to calculate the convergence or what type of model this is it ends up being a multiplicative model so i am guessing it is a log linked glm', 'title': 'does anyone use any type of gradient smoothing for binned data'}, {'post': 'let is say i have created a fraud detection model i already have a process that cleans the data and stores the trainable data in a table in a database and now i want to set up a recurring job that retrains my model every x time period how do people go about this when we want to retrain the model does not that data have to be loaded into the environment that the new model is going to be retrained i guess i am confused at how exactly the model takes the data from the database and starts training on it do people use spark to load it into the environment with the modeltobe then start the retraining process does not that mean that the retraining environment has to have enough space to cover that data being brought in apologies if this has already been asked but i have not seen a clear answer from this subredditfrom what i have found on google thanks so much for any assistance', 'title': 'setting up a model for retraining in production'}, {'post': 'i am working with a dataset of patient performancedata and patient demographics for people with a medical condition i am trying to assess the effect of a treatment on the patients and as i am trying to replicate a randomized experiment from a observational study i am performing matching between the treatment and control group when i try to perform mahalanobis distance matching however i keep getting the following warning on matlab warning matrix is close to singular or badly scaled results may be inaccurate rcond e the warning was caused because the determinant of my covariance matrix was very close to so when i calculated the inverse of this matrix as i need to do for finding the mahalanobis distance the matrix element values become very close to as well after reading about this warning a little bit i found that when this warning is displayed the results of mahalanobis distance matching is not very reliable i thought this was because my matrix became very sparse after i introduced onehotencoding to split the multicategorical features into many different features and hence some of my new features were for category values that had very few occurrences less than to address this issue i got rid of the category values with very low occurrences less than or binned them together with other lowoccurrence category values into slightly larger bins so their chances of occurring increased and my datamatrix was less sparse so i reduced the number of features from to and i plotted the histograms for all my features to see their distributions and this is what it looked like note that the element values have been normalized to by subtracting the feature mean and then divided by the feature standard deviation it looked visually satisfactory to me but i still kept getting the same warning in matlab when i tried finding the determinant of my new covariance matrix it was still very small e in a further attempt to fix this issue i checked whether removing all of the categorical features if we consider the onehot encoded features made things any better by making the matrix less sparse the new featurehistogram looked like this this time when i tried mahalanobis distance matching the warning was removed because the determinant of the covariance matrix was larger e so i tried to see if i could include one of the removed multicategorical features without facing the same problem as these are important features i would prefer to include when doing my matching i added a categorical feature with possible values and all the values very well distributed see histogram below but alas simply including these new onehotencoded features seems to cause the determinant of the covariance matrix to become so small that i start getting the same warning in matlab is there any fix to this issue or is it simply impossible to reliably perform mahalanobis distance matching on this dataset without leaving out the multicategorical values', 'title': 'is mahalanobisdistance matching between points not compatible with onehotencoded datasets'}]\n"
          ]
        }
      ],
      "source": [
        "# Display the first few items of each list\n",
        "\n",
        "print(\"Training pairs:\")\n",
        "print(train_pairs[:5])\n",
        "\n",
        "print(\"\\nValidation pairs:\")\n",
        "print(valid_pairs[:5])\n",
        "\n",
        "print(\"\\nTest pairs:\")\n",
        "print(test_pairs[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUuyIqOWn6Qe"
      },
      "source": [
        "### 3. Preprocessor and Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ewc6vSrgdvcn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def shift_right(input_ids, sos_token_id=0):\n",
        "    # Create a tensor of start-of-sequence tokens\n",
        "    sos_tokens = tf.fill([input_ids.shape[0], 1], sos_token_id)\n",
        "\n",
        "    # Concatenate the start-of-sequence tokens to the beginning of the input_ids\n",
        "    # and remove the last token to keep the same length\n",
        "    shifted_input_ids = tf.concat([sos_tokens, input_ids[:, :-1]], axis=-1)\n",
        "\n",
        "    return shifted_input_ids.numpy()\n",
        "\n",
        "def preprocess_data(pairs, tokenizer, max_length=128):\n",
        "    post_text = [post for post, title in pairs]\n",
        "    post_encoded = tokenizer.batch_encode_plus(\n",
        "        post_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    post_input_ids = np.array(post_encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    post_attention_masks = np.array(post_encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "\n",
        "    title_text = [title for post, title in pairs]\n",
        "    title_encoded = tokenizer.batch_encode_plus(\n",
        "        title_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    label_ids = np.array(title_encoded['input_ids'], dtype='int32')\n",
        "\n",
        "    #decoder_input_ids = model._shift_right(label_ids)\n",
        "    decoder_input_ids = shift_right(label_ids)\n",
        "\n",
        "    return [post_input_ids, post_attention_masks, decoder_input_ids], label_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YXxUV2mqdvcn"
      },
      "outputs": [],
      "source": [
        "class SummarizeDataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 n_examples,\n",
        "                 data_filename,\n",
        "                 max_length=128,\n",
        "                 batch_size=16,\n",
        "                 shuffle=True):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.n_examples = n_examples\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples+1)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches in the full dataset\n",
        "        return self.n_examples // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = (idx + 1) * self.batch_size\n",
        "\n",
        "        # Indices to skip are the ones in the shuffled row_order before and\n",
        "        # after the chunk we'll use for this batch\n",
        "        batch_idx_skip = np.concatenate((self.row_order[:batch_start], self.row_order[batch_end:]))\n",
        "\n",
        "        #batch_idx_skip = self.row_order[:batch_start] + self.row_order[batch_end:]\n",
        "\n",
        "        df = pd.read_csv(self.data_filename, skiprows=lambda x: x in batch_idx_skip)\n",
        "        #df = pd.read_csv(self.data_filename, skiprows=batch_idx_skip)\n",
        "\n",
        "\n",
        "        pairs = df[['post', 'title']].values.astype(str).tolist()\n",
        "\n",
        "        DEBUG = False\n",
        "\n",
        "        if DEBUG:\n",
        "            print(f\"data_filename: {self.data_filename,}\")\n",
        "            print(f\"batch_end: {batch_end}\")\n",
        "            print(f\"batch_idx_skip: {batch_idx_skip}\")\n",
        "            print(f\"pairs: {pairs}\")\n",
        "\n",
        "            #print(f\"pairs: {pairs.head()}\")\n",
        "\n",
        "\n",
        "        batch_data, labels = preprocess_data(\n",
        "            pairs,\n",
        "            self.tokenizer,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        inputs = {\n",
        "            'input_ids': batch_data[0],\n",
        "            'attention_mask': batch_data[1],\n",
        "            'decoder_input_ids': batch_data[2]\n",
        "        }\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "\n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = list(np.random.permutation(self.row_order))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKsHHZWndvcn"
      },
      "source": [
        "### 4. Load Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8g2Xm_1dvcn",
        "outputId": "b7405813-3294-4086-924b-757327655218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "checkpoint = 'facebook/bart-large-cnn'\n",
        "model = TFBartForConditionalGeneration.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mjtnBPdedvco"
      },
      "outputs": [],
      "source": [
        "# Create the data generators for train and validation data, tensorflow version\n",
        "\n",
        "max_length = 64 #32\n",
        "batch_size = 16\n",
        "\n",
        "train_data_generator = SummarizeDataGenerator(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    n_examples=len(train_pairs),\n",
        "    data_filename=train_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_data_generator = SummarizeDataGenerator(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    n_examples=len(valid_pairs),\n",
        "    data_filename=valid_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_data_generator = SummarizeDataGenerator(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    n_examples=len(test_pairs),\n",
        "    data_filename=test_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FZ7OSc_5dvco"
      },
      "outputs": [],
      "source": [
        "# Keras Sequential model class\n",
        "\n",
        "def build_bart_training_model(model, max_length, learning_rate=0.001, dropout_rate=0.1):\n",
        "    input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(max_length), dtype=tf.int32, name='attention_mask')\n",
        "    decoder_input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='decoder_input_ids')\n",
        "\n",
        "    #logits = model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)[0]\n",
        "    bart_model = TFBartForConditionalGeneration.from_pretrained(checkpoint)\n",
        "\n",
        "    outputs = bart_model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "    #logits = outputs.logits\n",
        "    logits = outputs[0]\n",
        "    print(\"logits :\",logits.shape)\n",
        "\n",
        "    dropout_layer = layers.Dropout(rate=dropout_rate)\n",
        "    logits = dropout_layer(logits)\n",
        "\n",
        "    #p_logits = bart_model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)[0]\n",
        "    model = tf.keras.Model(inputs=[input_ids, attention_mask, decoder_input_ids], outputs=[logits])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBartForConditionalGeneration\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_bart_training_model(max_length, learning_rate=0.001, dropout_rate=0.1, num_frozen_layers=5):\n",
        "    input_ids = layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "    decoder_input_ids = layers.Input(shape=(max_length,), dtype=tf.int32, name='decoder_input_ids')\n",
        "\n",
        "    bart_model = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "    # Freeze the BART model layers\n",
        "    for layer in bart_model.layers[:num_frozen_layers]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    outputs = bart_model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    dropout_layer = layers.Dropout(rate=dropout_rate)\n",
        "    logits = dropout_layer(logits)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_ids, attention_mask, decoder_input_ids], outputs=[logits])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "bmWo8kOndCH_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lx13d_LMdvco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d631985-b21b-49ce-b3c7-0e508cf46477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Create a new BART model instance\n",
        "#bart_mod = build_bart_training_model(model, max_length, learning_rate=0.001)\n",
        "bart_mod = build_bart_training_model(max_length, learning_rate=0.0001, dropout_rate=0.2, num_frozen_layers=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = 'drive/MyDrive/W266/model_checkpoints/'"
      ],
      "metadata": {
        "id": "G8Pbg3OXkagI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HRO3qDmbdvco"
      },
      "outputs": [],
      "source": [
        "# Define Early Stopping parameters\n",
        "monitor = 'val_loss'\n",
        "min_delta = 0.001\n",
        "patience = 10 #5\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor=monitor, min_delta=min_delta, patience=patience)\n",
        "\n",
        "# Save model checkpoint callback to save model weights after each epoch\n",
        "checkpoint_filepath = checkpoint_dir + 'bart_weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "\n",
        "# Model checkpoint callback\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS5jUhYSdvco",
        "outputId": "fe33c739-8cfa-4677-9958-6e5b26c1823a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1250/1250 [==============================] - 590s 454ms/step - loss: 14.7992 - accuracy: 0.0611 - val_loss: 13.3232 - val_accuracy: 0.0942\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79a63b92d600>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "NUM_EPOCHS = 1\n",
        "\n",
        "# fit model\n",
        "bart_mod.fit(train_data_generator,\n",
        "                  validation_data=valid_data_generator,\n",
        "                  epochs=NUM_EPOCHS,\n",
        "                  callbacks=[early_stopping, model_checkpoint_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kk8M7p-dvco",
        "outputId": "602eb4bc-3f43-406d-b7d7-d68288e3c68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Write a concise headline summarizing the following text: Hello all - I have an upcoming live case interview at CVS for their data science role. Can yall please share your experience of how the interview went? Did it involve quantitative analysis or was it just qualitative in']\n"
          ]
        }
      ],
      "source": [
        "# Confirm that the BART model has been fine-tuned\n",
        "for test_input_text in ['Hello all - I have an upcoming live case interview at CVS for their data science role. Can yall please share your experience of how the interview went? \\\n",
        "                        Did it involve quantitative analysis or was it just qualitative in nature? \\\n",
        "                        Did they supplement the discussion with some data? \\\n",
        "                        Were they expecting a technical ML solution? Or did they only want to guage the candidates thought process and structured communication?']:\n",
        "\n",
        "    test_inputs = tokenizer([\"Write a concise headline summarizing the following text: \" + test_input_text], return_tensors='tf')\n",
        "    test_output_ids = model.generate(test_inputs['input_ids'], num_beams=4, max_length=56)\n",
        "\n",
        "    print([tokenizer.decode(out_ids, skip_special_tokens=True,\n",
        "                             clean_up_tokenization_spaces=False) for out_ids in test_output_ids])\n",
        "\n",
        "# Actual Title: CVS Data scientist case interview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EvbpuZxYdvco",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "7c7afedf-3724-4e95-def2-ca33cddaa828"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'drive/MyDrive/W266/model_checkpoints/bart_weights.03-0.77.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ea22814afcb0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'bart_weights.03-0.77.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbart_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'drive/MyDrive/W266/model_checkpoints/bart_weights.03-0.77.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ],
      "source": [
        "# Load the saved model weights\n",
        "\n",
        "checkpoint_filepath = checkpoint_dir + 'bart_weights.03-0.77.hdf5'\n",
        "bart_mod.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IFOvC13dvco"
      },
      "outputs": [],
      "source": [
        "# Confirm that model checkpoint still works\n",
        "\n",
        "for test_input_text in ['Hello all - I have an upcoming live case interview at CVS for their data science role. Can yall please share your experience of how the interview went? \\\n",
        "                        Did it involve quantitative analysis or was it just qualitative in nature? \\\n",
        "                        Did they supplement the discussion with some data? \\\n",
        "                        Were they expecting a technical ML solution? Or did they only want to guage the candidates thought process and structured communication?']:\n",
        "\n",
        "    test_inputs = tokenizer([test_input_text], return_tensors='tf')\n",
        "    test_output_ids = model.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([tokenizer.decode(out_ids, skip_special_tokens=True,\n",
        "                             clean_up_tokenization_spaces=False) for out_ids in test_output_ids])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bart_mod.summary()"
      ],
      "metadata": {
        "id": "iU58kqqHKG1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9YHSHiLdvco"
      },
      "source": [
        "## 5. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXAMPLES_NUM = len(test_pairs)"
      ],
      "metadata": {
        "id": "xfgwTxVjabdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Candidates: these are the actual Reddit titles from the test set\n",
        "EXAMPLES_NUM = 10\n",
        "# Start Compute Units: 114.38 @ 4.91 per hour\n",
        "# Start Compute Units: 100.77 @ 4.91 per hour\n",
        "\n",
        "#EXAMPLES_NUM = len(test_pairs)\n",
        "print(f\"Generating {EXAMPLES_NUM} samples ... \")\n",
        "\n",
        "# Generate input ids for each tokenized post\n",
        "test_posts = [tokenizer(item['post'], max_length=512, truncation=True, return_tensors='tf') for item in test_pairs[:EXAMPLES_NUM]]\n",
        "#test_posts = [p_tokenizer(item['post'], max_length=512, truncation=True, return_tensors='tf') for item in test_pairs[:]]\n",
        "\n",
        "# Generating output ids for each tokenized post\n",
        "start_time = time.time()\n",
        "test_output_ids = [model.generate(post['input_ids'],\n",
        "                                    num_beams=3,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    num_return_sequences=1,  # returns the # of sequences for each post\n",
        "                                    max_length=128) for post in test_posts]\n",
        "\n",
        "end_time = time.time()\n",
        "run_time = end_time - start_time\n",
        "\n",
        "print(f\"Generate Time elapsed: {run_time} seconds.\\n\")\n",
        "# Ending Compute Units: 114.38\n",
        "\n",
        "# initialize list of candidates\n",
        "candidates = []\n",
        "\n",
        "# Decode each output in the batch of generated outputs\n",
        "for out_ids in test_output_ids:\n",
        "    # Decode each output in the batch of generated outputs\n",
        "    candidates_batch = [p_tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for ids in out_ids]\n",
        "    candidates.extend(candidates_batch)  # Extend the main candidates list with the batch\n",
        "\n",
        "# Inspect references list\n",
        "for idx, candidate in enumerate(candidates):\n",
        "  print(\"Candidate #\",idx,\":\\t \",candidate)\n",
        "  #print(\"\\t\",candidate)"
      ],
      "metadata": {
        "id": "9llTZ8MQuWBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q_y8qgedvco"
      },
      "outputs": [],
      "source": [
        "# References: we will compare the generated titles against these actual test values\n",
        "start_time = time.time()\n",
        "\n",
        "# reference list\n",
        "references = [item['title'] for item in test_pairs[:EXAMPLES_NUM]]\n",
        "\n",
        "# original post\n",
        "references_post = [item['post'] for item in test_pairs[:EXAMPLES_NUM]]\n",
        "\n",
        "# Inspect references list\n",
        "for idx, reference in enumerate(references):\n",
        "    print(\"Reference #\",idx,\":\\t \",reference)\n",
        "\n",
        "end_time = time.time()\n",
        "run_time = end_time - start_time\n",
        "print(f\"Generate Time elapsed: {run_time} seconds.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoVGyPQJdvco"
      },
      "source": [
        "#### ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6SiigkGdvco"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_results = rouge.compute(predictions=candidates, references=references)\n",
        "\n",
        "pd.DataFrame([rouge_results])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCsVM1T2dvco"
      },
      "source": [
        "#### BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRZZKQ2Ndvco"
      },
      "outputs": [],
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "bleu_results = bleu.compute(predictions=candidates, references=references)\n",
        "\n",
        "pd.DataFrame([bleu_results])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Export Results"
      ],
      "metadata": {
        "id": "TJ_NOQiicBri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Candidates and References\n",
        "\n",
        "df = pd.DataFrame({'ref_post': references_post,\n",
        "                   'ref_title': references,\n",
        "                   'candidate_title': candidates\n",
        "                   })\n",
        "# Inspect DF\n",
        "print(df.head())\n",
        "\n",
        "# Export to CSV\n",
        "df.to_csv('drive/My Drive/W266/bart_results.csv', index=True, index_label='Index_ID')"
      ],
      "metadata": {
        "id": "9A6E0tssZMTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}