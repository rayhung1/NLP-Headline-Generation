{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "xvILg5xsh_KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install packages\n",
        "!pip install -q transformers==4.37.2\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q evaluate\n",
        "!pip install -q tensorflow==2.15\n",
        "!pip install -q rouge_score"
      ],
      "metadata": {
        "id": "U1qL94HGneD9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "print(f\"Tensorflow v{tf.__version__}\")\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, TFBartForConditionalGeneration\n",
        "\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "G4uHjAq0l9Uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10533d4f-6ffc-4775-f82e-910f6d32bda9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow v2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Colab to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4N5QZwQmbb5",
        "outputId": "b95801c8-5443-4e71-ad2a-719901dde971"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verify data exists in Google Drive dir\n",
        "!ls 'drive/My Drive/W266'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaMha8DNmjFg",
        "outputId": "de1f89f3-e82c-40ca-c07a-79fd60f9dba3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bart_results.csv  model_checkpoints    reddit_subset_cleaned.csv  train_pairs.csv\n",
            "gpt_results.csv   opt350m_results.csv  test_pairs.csv\t\t  valid_pairs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load Data"
      ],
      "metadata": {
        "id": "MjySjw3dmp3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load clean Reddit ds\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "df = pd.read_csv('drive/My Drive/W266/reddit_subset_cleaned.csv')\n",
        "\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "p8snJGQumomB",
        "outputId": "60aced64-c90b-4a58-c0eb-312c92a9c90a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                       title  \\\n",
              "0  google is invasive nonanonymized ad targeting a quick confirmation of previously suspected privacy issues   \n",
              "1                         potential job in web analytics need to analyze some data what are they looking for   \n",
              "2                                         how to identify which google analytics account is tracking my site   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     post  \n",
              "0  i am cross posting this from rcyberlaw hopefully you guys find it as interesting as i didit deals with google analytics so quite awhile ago i ordered a papa john is pizza online my job largely involves looking at ads that appear online so afterwards i was quick to notice i was getting a lot of papa johns ads especially at night being served through a google owned company doubleclick media yesterday one of these ads popped up again on youtube a place that typically serves using the adwords program not doubleclick so i decided to copy the url for those not in the advertising field making full use of google is analytics tool means that certain information about the advertising campaign is leaked in the url so let is break it apart gt heresscs hereampadurl first off we see sscs sscs is doubleclick is redirect variable so rather than directly serving adwords ads they overrode it to serve through doubleclick then redirect through what would otherwise be an adwords link this is tighter integration than is generally seen with adwordsdoubleclick the interesting part is the end variables utm_sourcegooglenetworkamputm_mediumdisplaycpcamputm_campaigngoogleremarketing displaycpcgooglenetwork confirmation that doubleclick is now more finely integrated with adwords googleremarketing huh let is take a look at the definition for remarketing gtusing past campaign information to target a particular message to an audience while in the past behavioral targetting has largely been based on the sum of your use this is an interestingthough no doubt more widespread than is known change in that explicitly targeting old customers though a massive network of sites just thought i would put this out there i am sure it is not new to a lot of people but at least to me it was interesting to see concepts like this actually put into practice on such a large scale ps i did a quick survey across several thousand domains and for the record right now the most common external resource locations on the internet aregoogle owned is bolded pageadgooglesyndicationcom googleadsgdoubleclicknet edgequantservecom addoubleclicknet bscorecardresearchcom smdnnet dgspecificclicknet viewatdmtcom ajaxgoogleapiscom partnergoogleadservicescom that is a lot of data  \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        i decided grad school physics was not for me and i am branching out into the job market a web analytics place is interested in me and i am interested in any kind of data analysis the exercise is to use a comparison of three or more months of data to prepare a to slide powerpoint presentation of any significant information about site visitors what they are doing how they arrive at our site that we could use to improve site performance as an acquisition source he said i should notell a story this is a field i am unfamiliar with so i am looking for any basic tips common pitfalls and expectations thanks i am quite familiar with data analysis in general  \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          hey all my gf is having trouble with ga and has not gotten any response in days from posting in the google help forums so i figure i would try here question as follows i have a client that we coded a website for and google analytics was plugged into it we would like to look at the statistics for the site and no one can identify what account is associated with the tracking code that is embedded i have pulled the user account number from the source code i am just not sure how to identify what the login associated with it is can anyone help this is a fairly urgent request thanks in advance for any help  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25e2f5ac-35fa-4e7d-b499-b27cc7e211d5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>post</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>google is invasive nonanonymized ad targeting a quick confirmation of previously suspected privacy issues</td>\n",
              "      <td>i am cross posting this from rcyberlaw hopefully you guys find it as interesting as i didit deals with google analytics so quite awhile ago i ordered a papa john is pizza online my job largely involves looking at ads that appear online so afterwards i was quick to notice i was getting a lot of papa johns ads especially at night being served through a google owned company doubleclick media yesterday one of these ads popped up again on youtube a place that typically serves using the adwords program not doubleclick so i decided to copy the url for those not in the advertising field making full use of google is analytics tool means that certain information about the advertising campaign is leaked in the url so let is break it apart gt heresscs hereampadurl first off we see sscs sscs is doubleclick is redirect variable so rather than directly serving adwords ads they overrode it to serve through doubleclick then redirect through what would otherwise be an adwords link this is tighter integration than is generally seen with adwordsdoubleclick the interesting part is the end variables utm_sourcegooglenetworkamputm_mediumdisplaycpcamputm_campaigngoogleremarketing displaycpcgooglenetwork confirmation that doubleclick is now more finely integrated with adwords googleremarketing huh let is take a look at the definition for remarketing gtusing past campaign information to target a particular message to an audience while in the past behavioral targetting has largely been based on the sum of your use this is an interestingthough no doubt more widespread than is known change in that explicitly targeting old customers though a massive network of sites just thought i would put this out there i am sure it is not new to a lot of people but at least to me it was interesting to see concepts like this actually put into practice on such a large scale ps i did a quick survey across several thousand domains and for the record right now the most common external resource locations on the internet aregoogle owned is bolded pageadgooglesyndicationcom googleadsgdoubleclicknet edgequantservecom addoubleclicknet bscorecardresearchcom smdnnet dgspecificclicknet viewatdmtcom ajaxgoogleapiscom partnergoogleadservicescom that is a lot of data</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>potential job in web analytics need to analyze some data what are they looking for</td>\n",
              "      <td>i decided grad school physics was not for me and i am branching out into the job market a web analytics place is interested in me and i am interested in any kind of data analysis the exercise is to use a comparison of three or more months of data to prepare a to slide powerpoint presentation of any significant information about site visitors what they are doing how they arrive at our site that we could use to improve site performance as an acquisition source he said i should notell a story this is a field i am unfamiliar with so i am looking for any basic tips common pitfalls and expectations thanks i am quite familiar with data analysis in general</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>how to identify which google analytics account is tracking my site</td>\n",
              "      <td>hey all my gf is having trouble with ga and has not gotten any response in days from posting in the google help forums so i figure i would try here question as follows i have a client that we coded a website for and google analytics was plugged into it we would like to look at the statistics for the site and no one can identify what account is associated with the tracking code that is embedded i have pulled the user account number from the source code i am just not sure how to identify what the login associated with it is can anyone help this is a fairly urgent request thanks in advance for any help</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25e2f5ac-35fa-4e7d-b499-b27cc7e211d5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-25e2f5ac-35fa-4e7d-b499-b27cc7e211d5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-25e2f5ac-35fa-4e7d-b499-b27cc7e211d5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5e245bb6-36ca-4ece-9b1b-50c84cbf2d40\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5e245bb6-36ca-4ece-9b1b-50c84cbf2d40')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5e245bb6-36ca-4ece-9b1b-50c84cbf2d40 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 25000,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24606,\n        \"samples\": [\n          \"how to best learn data science with noncsstats background for future physicians\",\n          \"new ideas for ml healthcare projects portfolio and capstones and advice on my ideas\",\n          \"is it possible to improve my regression accuracy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"post\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24783,\n        \"samples\": [\n          \"so i got my first data analyst job months ago after attending a bootcamp and a few months of resume sending the salary is pretty low to what i have been seeing other people talk aboutunder k in the texas area but was a big jump from the bucks an hour i was making before so i amwas happy with it i have since gotten quite a bit better as people do when working with something every day so i was wondering if i should ask for a raise at the month mark what happens if they disagree are they going to mark me down as a possibly disgruntled employee and find something silly to get rid of me for should i just apply to other places or does a month run on your first job look bad i would appreciate any advice\",\n          \"i would like to create a model that can identify sentences that are questions but i am having trouble finding public datasets to work with most of the datasets i have found have been used for questionquery classification assigning a topic to a question rather than identifying if a sentence itself is actually a question in a perfect world i would have a two column csv where each row is a string of text and a binary indicator of if that text is a question or not has anyone done or seen a similar project before or does anyone know of any datasets that might lend themselves to this task if i cannot find anything i have a few ideas to create the data on my own i just do not really want to have to take on that arduous task\",\n          \"hey guys i am not sure on what my path forward in y career should be i have been at my company for years this may and as the title says i have had different managers i feel like i was months to a year a way for becoming a senior analyst under my last manager but he put in his week notice today he was also the longest tenure boss our department has had at months i am starting to feel like the manager shuffle is beginning to negatively impact my career progression so i guess my real question is how would you proceed stick it out and wait to see how the new boss will be or jump ship and try to join a company as a senior analyst or regular analyst thanks for the advice\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data: 80% Train, 15% Test, 5% Validation\n",
        "pairs = []\n",
        "generate_pairs = False\n",
        "\n",
        "if generate_pairs:\n",
        "\n",
        "  np.random.shuffle(pairs)\n",
        "  num_valid_samples = int(0.10 * len(pairs))\n",
        "  num_train_samples = len(pairs) - 2 * num_valid_samples #allocating 80% of dataset for training\n",
        "\n",
        "  train_pairs = pairs[:num_train_samples]\n",
        "  valid_pairs = pairs[num_train_samples : int(num_train_samples + num_valid_samples * 1.5)]\n",
        "  test_pairs = pairs[int(num_train_samples + num_valid_samples * 1.5):]"
      ],
      "metadata": {
        "id": "864y8nsXmmTB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regenerate train, validaiton, test file\n",
        "generate_files = False\n",
        "\n",
        "# Save splits to separate csv files, to load only part at a time later\n",
        "train_file = 'drive/My Drive/W266/train_pairs.csv'\n",
        "valid_file = 'drive/My Drive/W266/valid_pairs.csv'\n",
        "test_file = 'drive/My Drive/W266/test_pairs.csv'\n",
        "\n",
        "if generate_files:\n",
        "  pd.DataFrame(train_pairs).to_csv(train_file, index=False)\n",
        "  pd.DataFrame(valid_pairs).to_csv(valid_file, index=False)\n",
        "  pd.DataFrame(test_pairs).to_csv(test_file, index=False)\n",
        "\n",
        "# Load the CSV files into lists of dictionaries\n",
        "train_pairs = pd.read_csv(train_file).to_dict('records')\n",
        "valid_pairs = pd.read_csv(valid_file).to_dict('records')\n",
        "test_pairs = pd.read_csv(test_file).to_dict('records')"
      ],
      "metadata": {
        "id": "LxrIpVE0jJDQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{len(pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(valid_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX-_NNJFmiOW",
        "outputId": "efac3eb0-3f6d-40ad-f28f-cfe6744e1c63"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 total pairs\n",
            "20000 training pairs\n",
            "3750 validation pairs\n",
            "1250 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few items of each list\n",
        "\n",
        "print(\"Training pairs:\")\n",
        "print(train_pairs[:5])\n",
        "\n",
        "print(\"\\nValidation pairs:\")\n",
        "print(valid_pairs[:5])\n",
        "\n",
        "print(\"\\nTest pairs:\")\n",
        "print(test_pairs[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnluX1agl6n9",
        "outputId": "283af43c-ed82-4760-b6b1-dcfc1a3f7639"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training pairs:\n",
            "[{'post': 'my goal is to detect an object that is being flashed in front of a camera therefore the input is a video converted into images frames and the sequence matters i am trying to figure out how to go about training it usually video object detection algorithms detect objects in each frame my problem is that the objects i am trying to classify are similar and the object is not fully visible in any single frame because of a hand holding it in order to correctly tell what the object is you have to look at multiple frames i found which looks at the data sequentially but i am not sure how to map it to my situation anyone else knows about any implementations of a similar problem or potential ways of solving it', 'title': 'video object detection detecting object in the video frames sequentially'}, {'post': 'hi everyone for my master is thesis at the vrije universiteit amsterdam i am researching the environment for it professionals to report wrongdoing related to software examples of wrongdoing related to software are privacy and security issues discrimination happening within the software eg favoring certain users by an algorithm or allowing hate speech against specific user groups enabling fraud or corruption endangering someone is health and safety eg motivating addictive behavior triggering depressing thoughts and damaging the environment eg algorithms with too many co emissions if you are an it professional eg software engineer software tester product owner software consultant data scientist cs researcher please help me out by filling out the survey it takes minutes to complete thanks', 'title': 'reporting wrongdoing related to software'}, {'post': 'hello i am a postgrad in neuroscience and this is my first time using ml for my thesis the basis of my project is to compare the accuracy of random forest svm and decision trees on clinical prediction of depression with that of logistic regression based on brain imaging measures fractional anisotropy and mean diffusivity of white matter tracts with a sample size of around k to k people so far i am a bit overwhelmed and i am sorry if this question seems na√Øve but i was wondering how to handle missing values and outliers from what i understand each model has different methods impute data delete rows etc that fit best but i am still not sure i feel confident enough to understand the nuances of it so i was considering to just delete the rows with missing data and winsorizing outliers is there a better alternative given my notsodeep knowledge background thank you', 'title': 'missing values and outliers question'}, {'post': 'hi i am working on a project for my portfolio trying to predict employee turnover binary via logistic regression there are about variables in the dataset that could potentially be related to turnover in my experience i studied psychological science it is best practice to only include variables in your model that you have reason to believe are related to your outcome variable for example through previous research or subject matter expertise however i see in youtube tutorials of logistic regression people just throwing every variable directly in the model should i be exploring the relationship between each possible predictor and the outcome variable to establish a relationship first before building my model for example via running a chisquare of independence test on a potential categorical predictor and my outcome variable or is this redundant as the logistic regression model will sus out the nonsignificant predictors tldr what are the best practices for feature selection in logistic regression should i do exploratory data analysis for every possible predictor before building my model to rationalize their inclusion in the model or just throw them all in thank you for bearing with me', 'title': 'to explore relationships between predictors and outcome variable before building model or not'}, {'post': 'i must warn before i begin that i am coming from the hard science sphere and am trying to integrate some novel data science so some of the things i talk about might make no sense please ask questions i have a database of variables that are collected continuously over time this database is essentially measuring the conditions of the interplanetary magnetic field too complex to model which drives numerous phenomena that occur in the earth is atmosphere etc these phenomena are of interest to researchers but only occur ever so often to find these you have to manually search years and years of data something that is simply not too practical i want to develop some sort of method to take a number of instances of a phenomenon and search the continuous database for more instances i think that this needs to be split into two tasks discover common patterns dips spikes sign changes or even more complex shapes between the inputted instances it will also have to discern the variables of interest because not all in the database will drive a single phenomenon moreover if the program can then output these relationships such as variables appear to drive the phenomenon this will be of interest to users use these relationships to search the continuous stream for more instances like i mentioned i am not very experienced in data science of course i am not asking anyone here to develop this for me or anything but it would be really helpful if someone could point me in the right direction eventually i would like to develop some sort of robust methodology that can be applied to many scientific fields', 'title': 'identifying patterns in time series data with multiple variables'}]\n",
            "\n",
            "Validation pairs:\n",
            "[{'post': 'i just came up with a data sciencey project idea but i need help with the math side of it i will take studies that have found a correlation between cannabis use in undeveloped brains and decreased cognitive capacity and for the sake of this example i will input variables average frequency of consumption and quantity consumed i will take this through a python script and i will output a mathematical function that connects two percentages the probability first percentage x that you will be y less capable than the average nonsmoker person in either shortterm memory attention span etc the problem is i know literally no statistics so i am kinda struggling with the math side is this what you would call a linear regression what is the actual algorithm for outputting this mathematical function i first thought i would have to just do some sort of arithmetic mean but it got more complicated when you had more values for example i can have this data set a bigger sample would give more accuracy but i am gonna give a sample of so this example is easier let f be the average frequency of use in days how much days do they wait in between smoke sessions let q be the average quantity they consume in each session in grams or whatever unit of measure does not matter user f q user f q user f q each user will also have a variable representing how much less capable they are than the average person on a certain mental task in reality i am probably gonna have a list of variables for each user so i compute a separate mathematical function for each mental ability memory attention span etc but for the sake of this example let is say they have just one more variable representing how worse they are from the average person at attention span let it be a user a user a user a now i have to use all this data to compute a mathematical function f s gt s f q p f q are from r p is from which takes in the percentage of probability that the output is gonna happen as well as f and q and outputs the percentage decrease in attention span from the normal person basically a now that i think of it it should work in reverse too inputting the percentage decrease and outputting the probability that will happen so this function also has to be bijective actually it is not bijective per se because i just reverse a and p i add a in the domain set s and take out p in the output anyway now what i need help with is computing the larger function that takes in two variables and outputs the function f and that larger function is fed before two lists of variables to have the proper data that will be the script i will write i am stumped because it is taking in three lists of variables and outputs if it was just two outputting it would be easier for example user f a user f a user f a it is way easier to do it now having to output just a if i input or i know what i am going to get if i output anything else between and i just have to find out the relation between the two closest variables from to f halved and a tripled this would mean that most likely if f quarters then a would be multiplied by if f is divided by a would be multiplied by etc that is for any variable between and between and i do the same with the other data is this a correct approach either way if you also have to add the output p besides the output a i am stuck then you also add the input q and i am even more stuck so what do i have to do i know how to compute derivatives and to work with matrices and determinants in case that is needed', 'title': 'how do i compute this percentage from the available data is this a linear regression data science project'}, {'post': 'my client comes up with a huge volume of reddit data and she wants to know what people are talking about a particular product she is expecting some insights for their next product planning the data is around k conversations for just days after removing promo and spam the dead line is just a few days ahead now how should we do it the team is reading each conversations and tagging them manually which is absolutely not the right way and framing keywords reddit data has huge conversations so keywords often captures irrelevant data any suggestions on how to overcome this or my clientnot a technical person has unrealistic expectations is this even possible to gain insights for a general product from huge amount of very detailed user generated content', 'title': 'help text analytics extracting insights from a huge user generated data'}, {'post': 'went to school for stats did undergrad research in economics department i really enjoy r and python forming my work around larger questions filtering and cleaning large datasets building and tuning predictive models i really needed work and took a mobile development job a few years ago out of school since i had experience with swift and java and have been there since i think i hate just being given requirements and told build this thing with no context or background given i feel like all the other engineers love it and they only care about cleanbeautifulconcisewhatever code and making sure the marginspaddings are just perfect having beautiful ui drooling over newest mobile frameworks etc frankly past a certain point i do not really give a fuck about that i guess i am more of a storyteller and enjoy being given some data and some questions that needed to be answered or explored and going to town i am think i am more interested in solving business problems i like tuning and improving modelsalthough have not really done it in like years since i have been in mobile i got into a really good graduate program which will certainly get me into solid data science positions at many companies sometimes i feel that it is just a copout though and i will run into the same shit working in a data science position can i expect a change of pace from mobile development to data scienceengineering roll', 'title': 'can i expect change of pace from se to ds'}, {'post': 'is there anywhere where i can find a free copy of wikipedia is xml data dumps i have got a web app which as of now crawls wikipedia to get data about page links i know that data retrieval is forbidden under wikimedia is bot policy and also that they regularly release data dumps they ask you to host your own copy if you need to dynamically load pages from another website my entire website is running on a gb digitalocean droplet which is not nearly enough space for a wikipedia data dump so i would like to find somewhere where one of these data dumps is publicly hosted does this exist i am not concerned about images etc just about the text of pages including links', 'title': 'request wikipedia xml data dump hosted'}, {'post': 'hi guys i am a senior at carnegie mellon university thinking about grad school i got accepted into carnegie mellon is master is of statistical practice which is a one year program that will run me around k a lot of money for me i am still figuring out my career goals and was looking for advice on what to do ampxb i am interviewing for full time data analytics roles now i know i will want an mba in the future because my goal and core skillset lies in business not programming or data analytics but i do not know i do think data science is really cool and the master is program definitely prepares you for it i also do not think i will be able to get into a good oneyear program again in the future if i do not accept it now ampxb so my questions are is carnegie mellon is grad program in statistical practice wellrecognized by the industry will it help me get interviews is grad degree in statistics worth the time and money how hard is it to switch from data analytics to data science how do i know data science is for me honestly speaking i am partly interested in it because it is kind of the hot thing right now but currently i am not good at programming or statistical theory i can manage in my stats classes though ampxb thank you so much', 'title': 'grad school or industry'}]\n",
            "\n",
            "Test pairs:\n",
            "[{'post': 'some background i have been learning python for over a year now and i know some sql a bit of r and i have completed some small projects using data science data engineering practices i also know how to work in excel but i do not really have experience using databases i am totally willing to make the time and money investment in something like a bootcamp and i have the means to do fulltime training but i do not want to do this if there is a better faster way to get into the industry what i really want to know is what can i do that will get me a job in the field asap is there some specific bootcamp that will make this happen if so what are the best bootcamps or some particular tech skill i could learn that would basically guarantee that i am hireable very soon if i something like learned microsoft sql server or tableau and given my other skills would this be likely to get me hired i have been looking into bootcamps like thinkful springboard and data application lab the concern i have about these is actually that i already know a lot of the stuff they teach and i am worried that these will be a waste of time and not elevate me to where i want to be i also worry about it taking months to complete these programs as they estimate i would like to be finished in no more than about months thoughts', 'title': 'i want to transition into the data scienceanalyst or related field rather than asking whether i should choose a particular boootcamp or learn some language i would like to hear opinions on what path should i choose that will land me a job of some kind in the field as soon as possible'}, {'post': 'hi all i have created a python module to extract sngrams which is different from traditional ngrams as it embodies linguistic syntactic trees thus making it less arbitrary than traditional ngrams as it goes without saying quality of input feature affects model performance this will help you improve your model accuracy even further built on language models of spacy it can help especially for text classification information extraction query understanding machine translation question answering systems below is an example from sngramextractor import sngramextractor ampxb sngram_objsngramextractortextmeta_tagoriginaltrigram_flagyes texteconomic news have little effect on financial markets outputsngram_objget_sngram printtext print isngram bigramoutput isnbigram print isngram trigramoutput isntrigram economic news have little effect on financial markets sngram bigram cloud_every has_cloud lining_a lining_silver has_lining sngram trigram has_lining_silver ampxb textevery cloud has a silver lining outputsngram_objget_sngram printtext print isngram bigramoutput isnbigram print isngram trigramoutput isntrigram ampxb every cloud has a silver lining sngram bigram cloud_every has_cloud lining_a lining_silver has_lining sngram trigram has_lining_silver ampxb pypi', 'title': 'psngram linguistic features for improving machine learning and deep learning model accuracy for the first time in python new release'}, {'post': 'i am looking at some of the modeling programs the company has been using and there are options to smooth binned data layered linear on bin linear on avg linear on log avg and variable gradient are these methods commonly used in any algorithms i am not sure how i would implement it into r ampxb variable gradient is the most confusing it groups data from neighboring bins with a bin factor being determined the level of smoothing is determined by the radius all bins contribute to the calculation of the bin factor with decreasing weight as the distance from the target bin increases the weight is equal to radius distance radius ampxb once you calculate the overall factor a calculation is performed for each bin weighted by an exposure with various radii and a selected credibility level the calculation that yields the largest distance from is then selected as the factor to be used for that bin ampxb over multiple iterations of the model the factors are blended until the model converges and we receive a final coefficient for that bin ampxb since it is a program i have no clue what is going on in the background to calculate the convergence or what type of model this is it ends up being a multiplicative model so i am guessing it is a log linked glm', 'title': 'does anyone use any type of gradient smoothing for binned data'}, {'post': 'let is say i have created a fraud detection model i already have a process that cleans the data and stores the trainable data in a table in a database and now i want to set up a recurring job that retrains my model every x time period how do people go about this when we want to retrain the model does not that data have to be loaded into the environment that the new model is going to be retrained i guess i am confused at how exactly the model takes the data from the database and starts training on it do people use spark to load it into the environment with the modeltobe then start the retraining process does not that mean that the retraining environment has to have enough space to cover that data being brought in apologies if this has already been asked but i have not seen a clear answer from this subredditfrom what i have found on google thanks so much for any assistance', 'title': 'setting up a model for retraining in production'}, {'post': 'i am working with a dataset of patient performancedata and patient demographics for people with a medical condition i am trying to assess the effect of a treatment on the patients and as i am trying to replicate a randomized experiment from a observational study i am performing matching between the treatment and control group when i try to perform mahalanobis distance matching however i keep getting the following warning on matlab warning matrix is close to singular or badly scaled results may be inaccurate rcond e the warning was caused because the determinant of my covariance matrix was very close to so when i calculated the inverse of this matrix as i need to do for finding the mahalanobis distance the matrix element values become very close to as well after reading about this warning a little bit i found that when this warning is displayed the results of mahalanobis distance matching is not very reliable i thought this was because my matrix became very sparse after i introduced onehotencoding to split the multicategorical features into many different features and hence some of my new features were for category values that had very few occurrences less than to address this issue i got rid of the category values with very low occurrences less than or binned them together with other lowoccurrence category values into slightly larger bins so their chances of occurring increased and my datamatrix was less sparse so i reduced the number of features from to and i plotted the histograms for all my features to see their distributions and this is what it looked like note that the element values have been normalized to by subtracting the feature mean and then divided by the feature standard deviation it looked visually satisfactory to me but i still kept getting the same warning in matlab when i tried finding the determinant of my new covariance matrix it was still very small e in a further attempt to fix this issue i checked whether removing all of the categorical features if we consider the onehot encoded features made things any better by making the matrix less sparse the new featurehistogram looked like this this time when i tried mahalanobis distance matching the warning was removed because the determinant of the covariance matrix was larger e so i tried to see if i could include one of the removed multicategorical features without facing the same problem as these are important features i would prefer to include when doing my matching i added a categorical feature with possible values and all the values very well distributed see histogram below but alas simply including these new onehotencoded features seems to cause the determinant of the covariance matrix to become so small that i start getting the same warning in matlab is there any fix to this issue or is it simply impossible to reliably perform mahalanobis distance matching on this dataset without leaving out the multicategorical values', 'title': 'is mahalanobisdistance matching between points not compatible with onehotencoded datasets'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Preprocessor and Data Generator"
      ],
      "metadata": {
        "id": "XUuyIqOWn6Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def shift_right(input_ids, sos_token_id=0):\n",
        "    # Create a tensor of start-of-sequence tokens\n",
        "    sos_tokens = tf.fill([input_ids.shape[0], 1], sos_token_id)\n",
        "\n",
        "    # Concatenate the start-of-sequence tokens to the beginning of the input_ids\n",
        "    # and remove the last token to keep the same length\n",
        "    shifted_input_ids = tf.concat([sos_tokens, input_ids[:, :-1]], axis=-1)\n",
        "\n",
        "    return shifted_input_ids.numpy()\n",
        "\n",
        "def preprocess_data(pairs, tokenizer, max_length=128):\n",
        "    post_text = [post for post, title in pairs]\n",
        "    post_encoded = tokenizer.batch_encode_plus(\n",
        "        post_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    post_input_ids = np.array(post_encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    post_attention_masks = np.array(post_encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "\n",
        "    title_text = [title for post, title in pairs]\n",
        "    title_encoded = tokenizer.batch_encode_plus(\n",
        "        title_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    label_ids = np.array(title_encoded['input_ids'], dtype='int32')\n",
        "\n",
        "    #decoder_input_ids = model._shift_right(label_ids)\n",
        "    decoder_input_ids = shift_right(label_ids)\n",
        "\n",
        "    return [post_input_ids, post_attention_masks, decoder_input_ids], label_ids"
      ],
      "metadata": {
        "id": "-U-4_qECmBUF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummarizeDataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 n_examples,\n",
        "                 data_filename,\n",
        "                 max_length=128,\n",
        "                 batch_size=16,\n",
        "                 shuffle=True):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.n_examples = n_examples\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples+1)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches in the full dataset\n",
        "        return self.n_examples // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = (idx + 1) * self.batch_size\n",
        "\n",
        "        # Indices to skip are the ones in the shuffled row_order before and\n",
        "        # after the chunk we'll use for this batch\n",
        "        batch_idx_skip = np.concatenate((self.row_order[:batch_start], self.row_order[batch_end:]))\n",
        "\n",
        "        #batch_idx_skip = self.row_order[:batch_start] + self.row_order[batch_end:]\n",
        "\n",
        "        df = pd.read_csv(self.data_filename, skiprows=lambda x: x in batch_idx_skip)\n",
        "        #df = pd.read_csv(self.data_filename, skiprows=batch_idx_skip)\n",
        "\n",
        "\n",
        "        pairs = df[['post', 'title']].values.astype(str).tolist()\n",
        "\n",
        "        DEBUG = False\n",
        "\n",
        "        if DEBUG:\n",
        "            print(f\"data_filename: {self.data_filename,}\")\n",
        "            print(f\"batch_end: {batch_end}\")\n",
        "            print(f\"batch_idx_skip: {batch_idx_skip}\")\n",
        "            print(f\"pairs: {pairs}\")\n",
        "\n",
        "            #print(f\"pairs: {pairs.head()}\")\n",
        "\n",
        "\n",
        "        batch_data, labels = preprocess_data(\n",
        "            pairs,\n",
        "            self.tokenizer,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        inputs = {\n",
        "            'input_ids': batch_data[0],\n",
        "            'attention_mask': batch_data[1],\n",
        "            'decoder_input_ids': batch_data[2]\n",
        "        }\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "\n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = list(np.random.permutation(self.row_order))"
      ],
      "metadata": {
        "id": "6SQJ90QtiEX6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Pretrained model\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CuXoHBp_pcMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
        "\n",
        "checkpoint = 'facebook/bart-large-cnn'\n",
        "model = TFBartForConditionalGeneration.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "4sYMoGc1pWTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71e3a97-d40c-4b1d-be63-a6acb79e30e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data generators for train and validation data, tensorflow version\n",
        "\n",
        "max_length = 64 #32\n",
        "batch_size = 16\n",
        "\n",
        "train_data_generator = SummarizeDataGenerator(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    n_examples=len(train_pairs),\n",
        "    data_filename=train_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_data_generator = SummarizeDataGenerator(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    n_examples=len(valid_pairs),\n",
        "    data_filename=valid_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_data_generator = SummarizeDataGenerator(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    n_examples=len(test_pairs),\n",
        "    data_filename=test_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "Ma1qh8u5iN1X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBartForConditionalGeneration\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_bart_training_model(max_length=64, learning_rate=0.0005):\n",
        "\n",
        "    input_ids = layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "    decoder_input_ids = layers.Input(shape=(max_length,), dtype=tf.int32, name='decoder_input_ids')\n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids).logits\n",
        "\n",
        "    training_model = tf.keras.models.Model(inputs=[input_ids, attention_mask, decoder_input_ids], outputs=[outputs])\n",
        "\n",
        "    # Compile the model with an optimizer, loss, and metrics\n",
        "    training_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "    return training_model\n"
      ],
      "metadata": {
        "id": "KfIUthJn3_Fl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bart_model = build_bart_training_model(max_length=64, learning_rate=0.0005)\n"
      ],
      "metadata": {
        "id": "NzLTJ38TYOa4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add a model checkpoint callback to save\n",
        "# the trained model weights after each epoch.\n",
        "\n",
        "checkpoint_dir = 'drive/MyDrive/W266/model_checkpoints/'\n",
        "checkpoint_filepath = checkpoint_dir + 'bart2_weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True\n",
        "    )\n"
      ],
      "metadata": {
        "id": "nowLPb1oYQ7v"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 1\n",
        "\n",
        "# fit model\n",
        "bart_model.fit(train_data_generator,\n",
        "                  validation_data=valid_data_generator,\n",
        "                  epochs=NUM_EPOCHS,\n",
        "                  callbacks=[model_checkpoint_callback],\n",
        "                  batch_size=8\n",
        ")"
      ],
      "metadata": {
        "id": "hlOPPYmjCpDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for test_input_text in ['Hello all - I have an upcoming live case interview at CVS for their data science role. Can yall please share your experience of how the interview went? \\\n",
        "                        Did it involve quantitative analysis or was it just qualitative in nature? \\\n",
        "                        Did they supplement the discussion with some data? \\\n",
        "                        Were they expecting a technical ML solution? Or did they only want to guage the candidates thought process and structured communication?']:\n",
        "\n",
        "    test_inputs = tokenizer(test_input_text, max_length=512, truncation=True, return_tensors='tf')\n",
        "    test_output_ids = model.generate(test_inputs['input_ids'], num_beams=4, max_length=56)\n",
        "\n",
        "    print([tokenizer.decode(out_ids, skip_special_tokens=True,\n",
        "                             clean_up_tokenization_spaces=False) for out_ids in test_output_ids])\n",
        "\n",
        "# Actual Title: CVS Data scientist case interview\n"
      ],
      "metadata": {
        "id": "XRR5FeYdncWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BURBWoFvnb-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluation"
      ],
      "metadata": {
        "id": "XDij82_nMXAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pairs[0]"
      ],
      "metadata": {
        "id": "Wr6OFHrSN_yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Candidates: these are the actual Reddit titles from the test set\n",
        "EXAMPLES_NUM = 10\n",
        "# Start Compute Units: 114.38 @ 4.91 per hour\n",
        "# Start Compute Units: 100.77 @ 4.91 per hour\n",
        "\n",
        "#EXAMPLES_NUM = len(test_pairs)\n",
        "print(f\"Generating {EXAMPLES_NUM} samples ... \")\n",
        "\n",
        "# Generate input ids for each tokenized post\n",
        "test_posts = [tokenizer(item['post'], max_length=512, truncation=True, return_tensors='tf') for item in test_pairs[:EXAMPLES_NUM]]\n",
        "#test_posts = [p_tokenizer(item['post'], max_length=512, truncation=True, return_tensors='tf') for item in test_pairs[:]]\n",
        "\n",
        "# Generating output ids for each tokenized post\n",
        "start_time = time.time()\n",
        "test_output_ids = [model.generate(post['input_ids'],\n",
        "                                    num_beams=3,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    num_return_sequences=1,  # returns the # of sequences for each post\n",
        "                                    max_length=128) for post in test_posts]\n",
        "\n",
        "end_time = time.time()\n",
        "run_time = end_time - start_time\n",
        "\n",
        "print(f\"Generate Time elapsed: {run_time} seconds.\\n\")\n",
        "# Ending Compute Units: 114.38\n",
        "\n",
        "# initialize list of candidates\n",
        "candidates = []\n",
        "\n",
        "# Decode each output in the batch of generated outputs\n",
        "for out_ids in test_output_ids:\n",
        "    # Decode each output in the batch of generated outputs\n",
        "    candidates_batch = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for ids in out_ids]\n",
        "    candidates.extend(candidates_batch)  # Extend the main candidates list with the batch\n",
        "\n",
        "# Inspect references list\n",
        "for idx, candidate in enumerate(candidates):\n",
        "  print(\"Candidate #\",idx,\":\\t \",candidate)\n",
        "  #print(\"\\t\",candidate)"
      ],
      "metadata": {
        "id": "su1k9PJ7-5Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# References: we will compare the generated titles against these actual test values\n",
        "start_time = time.time()\n",
        "\n",
        "# reference list\n",
        "references = [item['title'] for item in test_pairs[:EXAMPLES_NUM]]\n",
        "\n",
        "# original post\n",
        "references_post = [item['post'] for item in test_pairs[:EXAMPLES_NUM]]\n",
        "\n",
        "# Inspect references list\n",
        "for idx, reference in enumerate(references):\n",
        "    print(\"Reference #\",idx,\":\\t \",reference)\n",
        "\n",
        "end_time = time.time()\n",
        "run_time = end_time - start_time\n",
        "print(f\"Generate Time elapsed: {run_time} seconds.\\n\")"
      ],
      "metadata": {
        "id": "wuFmY_0wTsFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_results = rouge.compute(predictions=candidates, references=references)\n",
        "\n",
        "pd.DataFrame([rouge_results])"
      ],
      "metadata": {
        "id": "jH-sBuKYiffN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "bleu_results = bleu.compute(predictions=candidates, references=references)\n",
        "\n",
        "pd.DataFrame([bleu_results])"
      ],
      "metadata": {
        "id": "lr7T75KDihef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Candidates and References\n",
        "\n",
        "df = pd.DataFrame({'ref_post': references_post,\n",
        "                   'ref_title': references,\n",
        "                   'candidate_title': candidates\n",
        "                   })\n",
        "# Inspect DF\n",
        "print(df.head())\n",
        "\n",
        "# Export to CSV\n",
        "df.to_csv('drive/My Drive/W266/opt350m_results.csv', index=True, index_label='Index_ID')"
      ],
      "metadata": {
        "id": "mQSmryW3ikS8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}