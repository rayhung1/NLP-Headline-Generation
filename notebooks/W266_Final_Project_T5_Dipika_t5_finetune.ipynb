{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "480464b5552b44fd9cedf1d8ac1278c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5667e9f227094da4858770d5461ab7e4",
              "IPY_MODEL_b87cfe72a8a141a2b9f53466e92fb493",
              "IPY_MODEL_616c6c10c85243d1b27e2e878d1288ab"
            ],
            "layout": "IPY_MODEL_5c8e6d275dfa45f78a8ed4834c48573c"
          }
        },
        "5667e9f227094da4858770d5461ab7e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d7ae152c928489f9ec1136d3486328e",
            "placeholder": "​",
            "style": "IPY_MODEL_e98529d013b045839b91930aed7eecd2",
            "value": "Downloading builder script: 100%"
          }
        },
        "b87cfe72a8a141a2b9f53466e92fb493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d0e3bb682844a049d2ee1301ab3edb9",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6669f9160fc34ee5bad9f488dc4fcf20",
            "value": 6270
          }
        },
        "616c6c10c85243d1b27e2e878d1288ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35c6aceadcb04a04a987c44ae29fa724",
            "placeholder": "​",
            "style": "IPY_MODEL_acba7e889b274224bef4ed15a5f22d49",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 249kB/s]"
          }
        },
        "5c8e6d275dfa45f78a8ed4834c48573c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d7ae152c928489f9ec1136d3486328e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e98529d013b045839b91930aed7eecd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d0e3bb682844a049d2ee1301ab3edb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6669f9160fc34ee5bad9f488dc4fcf20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35c6aceadcb04a04a987c44ae29fa724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acba7e889b274224bef4ed15a5f22d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87795ebee40546d3b53798f72e74fb1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b922fd478184487da4c08856664a74fd",
              "IPY_MODEL_1c49c2c1dc224dc3a09e2840613bb52f",
              "IPY_MODEL_b9bb066837044a768d2e5864b33c17d8"
            ],
            "layout": "IPY_MODEL_f6e5c1f87822454989e00107589ef330"
          }
        },
        "b922fd478184487da4c08856664a74fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cacaea4847544e9caf5f065ab39fcd9b",
            "placeholder": "​",
            "style": "IPY_MODEL_8d8a1abd29a146f6a7a9a26a9f664bc7",
            "value": "Downloading builder script: 100%"
          }
        },
        "1c49c2c1dc224dc3a09e2840613bb52f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f71beeeeec4d4bd1bc0c44cd0f1e9aef",
            "max": 5937,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5174ffd4b1244b489121c70807c0e0f9",
            "value": 5937
          }
        },
        "b9bb066837044a768d2e5864b33c17d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb0432b5cbb4489ea4df9b071c5fd2ca",
            "placeholder": "​",
            "style": "IPY_MODEL_4436a4b72c65446983e738a2db565c1f",
            "value": " 5.94k/5.94k [00:00&lt;00:00, 322kB/s]"
          }
        },
        "f6e5c1f87822454989e00107589ef330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cacaea4847544e9caf5f065ab39fcd9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d8a1abd29a146f6a7a9a26a9f664bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f71beeeeec4d4bd1bc0c44cd0f1e9aef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5174ffd4b1244b489121c70807c0e0f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb0432b5cbb4489ea4df9b071c5fd2ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4436a4b72c65446983e738a2db565c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29385bb2395e48a698bc877b77f23784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16a32fc430824b37a43b704eeea7d899",
              "IPY_MODEL_7fc96d2bab58468f96bd651f543854ab",
              "IPY_MODEL_f3a4651a683447959e3132fa5eb94540"
            ],
            "layout": "IPY_MODEL_ac1bb13e17ac48e4b577b19f9346cd51"
          }
        },
        "16a32fc430824b37a43b704eeea7d899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841bbd2036ce41fd95d579c3972c0e03",
            "placeholder": "​",
            "style": "IPY_MODEL_1e2db60c9a934a9c8186032ca9621c6f",
            "value": "Downloading extra modules: "
          }
        },
        "7fc96d2bab58468f96bd651f543854ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a9c11f203034d239d5ba27866a8d559",
            "max": 1554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc32d3aa46a840a6b92d1b7131f4fa7b",
            "value": 1554
          }
        },
        "f3a4651a683447959e3132fa5eb94540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12d9b98e5d9d4bd4913feb699219e057",
            "placeholder": "​",
            "style": "IPY_MODEL_ed145b11bb784a53990de5b83b69c7f3",
            "value": " 4.07k/? [00:00&lt;00:00, 281kB/s]"
          }
        },
        "ac1bb13e17ac48e4b577b19f9346cd51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "841bbd2036ce41fd95d579c3972c0e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e2db60c9a934a9c8186032ca9621c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a9c11f203034d239d5ba27866a8d559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc32d3aa46a840a6b92d1b7131f4fa7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12d9b98e5d9d4bd4913feb699219e057": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed145b11bb784a53990de5b83b69c7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4cfae8482d742ae9f31c1a7621fdde6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_838f07fab82c4f87b4641aa0a24a3bd6",
              "IPY_MODEL_1d68874b660847bcb775d7f2d4159133",
              "IPY_MODEL_f1f646cf8fc94904b7a6c8209399ae48"
            ],
            "layout": "IPY_MODEL_4db990bc08bd4752826058e6f6e7984f"
          }
        },
        "838f07fab82c4f87b4641aa0a24a3bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_733ece9c725e4b0ea6851991f83f8bbd",
            "placeholder": "​",
            "style": "IPY_MODEL_cf947d3449e44bd9874ca3fd28952a82",
            "value": "Downloading extra modules: 100%"
          }
        },
        "1d68874b660847bcb775d7f2d4159133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a13302d76f694236bbd8488f4a346d38",
            "max": 3344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23768244cf3843f58bd0f13949d21b10",
            "value": 3344
          }
        },
        "f1f646cf8fc94904b7a6c8209399ae48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_155123b6e45c4c99b20080ad9890df1e",
            "placeholder": "​",
            "style": "IPY_MODEL_b9f2571d12cd4f1bab13ece37cc2af0d",
            "value": " 3.34k/3.34k [00:00&lt;00:00, 233kB/s]"
          }
        },
        "4db990bc08bd4752826058e6f6e7984f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "733ece9c725e4b0ea6851991f83f8bbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf947d3449e44bd9874ca3fd28952a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a13302d76f694236bbd8488f4a346d38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23768244cf3843f58bd0f13949d21b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "155123b6e45c4c99b20080ad9890df1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9f2571d12cd4f1bab13ece37cc2af0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "xvILg5xsh_KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install packages\n",
        "!pip install -q transformers==4.37.2\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q evaluate\n",
        "!pip install -q tensorflow==2.15\n",
        "!pip install -q rouge_score"
      ],
      "metadata": {
        "id": "U1qL94HGneD9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "G4uHjAq0l9Uz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Colab to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4N5QZwQmbb5",
        "outputId": "9a0d33a2-a1b4-465c-f049-8b975075e54f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verify data exists in Google Drive dir\n",
        "!ls 'drive/My Drive/W266/Final Project'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaMha8DNmjFg",
        "outputId": "82ed650e-78a7-4c16-e0ff-c75143912921"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " fine_tuning\t\t     t5_results.csv    W266_Final_Project_Data_Cleanup.ipynb\n",
            " model_checkpoints\t     test_pairs.csv    W266_Final_Project_Pegasus_Ray.ipynb\n",
            "'old dataset split'\t     train_pairs.csv  ' W266_Final_Project_T5_Dipika.ipynb'\n",
            " reddit_subset_cleaned.csv   valid_pairs.csv   W266_Final_Project_T5_Dipika_t5_finetune.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load Data"
      ],
      "metadata": {
        "id": "MjySjw3dmp3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths for the split CSV files\n",
        "train_file = 'drive/My Drive/W266/Final Project/train_pairs.csv'\n",
        "valid_file = 'drive/My Drive/W266/Final Project/valid_pairs.csv'\n",
        "test_file = 'drive/My Drive/W266/Final Project/test_pairs.csv'\n",
        "\n",
        "# Load the CSV files into lists of dictionaries\n",
        "train_pairs = pd.read_csv(train_file).to_dict('records')\n",
        "valid_pairs = pd.read_csv(valid_file).to_dict('records')\n",
        "test_pairs = pd.read_csv(test_file).to_dict('records')\n",
        "\n",
        "# Display the first few items of each list\n",
        "print(\"Training pairs:\")\n",
        "print(train_pairs[:5])\n",
        "\n",
        "print(\"\\nValidation pairs:\")\n",
        "print(valid_pairs[:5])\n",
        "\n",
        "print(\"\\nTest pairs:\")\n",
        "print(test_pairs[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnluX1agl6n9",
        "outputId": "57a02668-4892-4da2-e37a-ada2b5f930b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training pairs:\n",
            "[{'post': 'my goal is to detect an object that is being flashed in front of a camera therefore the input is a video converted into images frames and the sequence matters i am trying to figure out how to go about training it usually video object detection algorithms detect objects in each frame my problem is that the objects i am trying to classify are similar and the object is not fully visible in any single frame because of a hand holding it in order to correctly tell what the object is you have to look at multiple frames i found which looks at the data sequentially but i am not sure how to map it to my situation anyone else knows about any implementations of a similar problem or potential ways of solving it', 'title': 'video object detection detecting object in the video frames sequentially'}, {'post': 'hi everyone for my master is thesis at the vrije universiteit amsterdam i am researching the environment for it professionals to report wrongdoing related to software examples of wrongdoing related to software are privacy and security issues discrimination happening within the software eg favoring certain users by an algorithm or allowing hate speech against specific user groups enabling fraud or corruption endangering someone is health and safety eg motivating addictive behavior triggering depressing thoughts and damaging the environment eg algorithms with too many co emissions if you are an it professional eg software engineer software tester product owner software consultant data scientist cs researcher please help me out by filling out the survey it takes minutes to complete thanks', 'title': 'reporting wrongdoing related to software'}, {'post': 'hello i am a postgrad in neuroscience and this is my first time using ml for my thesis the basis of my project is to compare the accuracy of random forest svm and decision trees on clinical prediction of depression with that of logistic regression based on brain imaging measures fractional anisotropy and mean diffusivity of white matter tracts with a sample size of around k to k people so far i am a bit overwhelmed and i am sorry if this question seems naïve but i was wondering how to handle missing values and outliers from what i understand each model has different methods impute data delete rows etc that fit best but i am still not sure i feel confident enough to understand the nuances of it so i was considering to just delete the rows with missing data and winsorizing outliers is there a better alternative given my notsodeep knowledge background thank you', 'title': 'missing values and outliers question'}, {'post': 'hi i am working on a project for my portfolio trying to predict employee turnover binary via logistic regression there are about variables in the dataset that could potentially be related to turnover in my experience i studied psychological science it is best practice to only include variables in your model that you have reason to believe are related to your outcome variable for example through previous research or subject matter expertise however i see in youtube tutorials of logistic regression people just throwing every variable directly in the model should i be exploring the relationship between each possible predictor and the outcome variable to establish a relationship first before building my model for example via running a chisquare of independence test on a potential categorical predictor and my outcome variable or is this redundant as the logistic regression model will sus out the nonsignificant predictors tldr what are the best practices for feature selection in logistic regression should i do exploratory data analysis for every possible predictor before building my model to rationalize their inclusion in the model or just throw them all in thank you for bearing with me', 'title': 'to explore relationships between predictors and outcome variable before building model or not'}, {'post': 'i must warn before i begin that i am coming from the hard science sphere and am trying to integrate some novel data science so some of the things i talk about might make no sense please ask questions i have a database of variables that are collected continuously over time this database is essentially measuring the conditions of the interplanetary magnetic field too complex to model which drives numerous phenomena that occur in the earth is atmosphere etc these phenomena are of interest to researchers but only occur ever so often to find these you have to manually search years and years of data something that is simply not too practical i want to develop some sort of method to take a number of instances of a phenomenon and search the continuous database for more instances i think that this needs to be split into two tasks discover common patterns dips spikes sign changes or even more complex shapes between the inputted instances it will also have to discern the variables of interest because not all in the database will drive a single phenomenon moreover if the program can then output these relationships such as variables appear to drive the phenomenon this will be of interest to users use these relationships to search the continuous stream for more instances like i mentioned i am not very experienced in data science of course i am not asking anyone here to develop this for me or anything but it would be really helpful if someone could point me in the right direction eventually i would like to develop some sort of robust methodology that can be applied to many scientific fields', 'title': 'identifying patterns in time series data with multiple variables'}]\n",
            "\n",
            "Validation pairs:\n",
            "[{'post': 'i just came up with a data sciencey project idea but i need help with the math side of it i will take studies that have found a correlation between cannabis use in undeveloped brains and decreased cognitive capacity and for the sake of this example i will input variables average frequency of consumption and quantity consumed i will take this through a python script and i will output a mathematical function that connects two percentages the probability first percentage x that you will be y less capable than the average nonsmoker person in either shortterm memory attention span etc the problem is i know literally no statistics so i am kinda struggling with the math side is this what you would call a linear regression what is the actual algorithm for outputting this mathematical function i first thought i would have to just do some sort of arithmetic mean but it got more complicated when you had more values for example i can have this data set a bigger sample would give more accuracy but i am gonna give a sample of so this example is easier let f be the average frequency of use in days how much days do they wait in between smoke sessions let q be the average quantity they consume in each session in grams or whatever unit of measure does not matter user f q user f q user f q each user will also have a variable representing how much less capable they are than the average person on a certain mental task in reality i am probably gonna have a list of variables for each user so i compute a separate mathematical function for each mental ability memory attention span etc but for the sake of this example let is say they have just one more variable representing how worse they are from the average person at attention span let it be a user a user a user a now i have to use all this data to compute a mathematical function f s gt s f q p f q are from r p is from which takes in the percentage of probability that the output is gonna happen as well as f and q and outputs the percentage decrease in attention span from the normal person basically a now that i think of it it should work in reverse too inputting the percentage decrease and outputting the probability that will happen so this function also has to be bijective actually it is not bijective per se because i just reverse a and p i add a in the domain set s and take out p in the output anyway now what i need help with is computing the larger function that takes in two variables and outputs the function f and that larger function is fed before two lists of variables to have the proper data that will be the script i will write i am stumped because it is taking in three lists of variables and outputs if it was just two outputting it would be easier for example user f a user f a user f a it is way easier to do it now having to output just a if i input or i know what i am going to get if i output anything else between and i just have to find out the relation between the two closest variables from to f halved and a tripled this would mean that most likely if f quarters then a would be multiplied by if f is divided by a would be multiplied by etc that is for any variable between and between and i do the same with the other data is this a correct approach either way if you also have to add the output p besides the output a i am stuck then you also add the input q and i am even more stuck so what do i have to do i know how to compute derivatives and to work with matrices and determinants in case that is needed', 'title': 'how do i compute this percentage from the available data is this a linear regression data science project'}, {'post': 'my client comes up with a huge volume of reddit data and she wants to know what people are talking about a particular product she is expecting some insights for their next product planning the data is around k conversations for just days after removing promo and spam the dead line is just a few days ahead now how should we do it the team is reading each conversations and tagging them manually which is absolutely not the right way and framing keywords reddit data has huge conversations so keywords often captures irrelevant data any suggestions on how to overcome this or my clientnot a technical person has unrealistic expectations is this even possible to gain insights for a general product from huge amount of very detailed user generated content', 'title': 'help text analytics extracting insights from a huge user generated data'}, {'post': 'went to school for stats did undergrad research in economics department i really enjoy r and python forming my work around larger questions filtering and cleaning large datasets building and tuning predictive models i really needed work and took a mobile development job a few years ago out of school since i had experience with swift and java and have been there since i think i hate just being given requirements and told build this thing with no context or background given i feel like all the other engineers love it and they only care about cleanbeautifulconcisewhatever code and making sure the marginspaddings are just perfect having beautiful ui drooling over newest mobile frameworks etc frankly past a certain point i do not really give a fuck about that i guess i am more of a storyteller and enjoy being given some data and some questions that needed to be answered or explored and going to town i am think i am more interested in solving business problems i like tuning and improving modelsalthough have not really done it in like years since i have been in mobile i got into a really good graduate program which will certainly get me into solid data science positions at many companies sometimes i feel that it is just a copout though and i will run into the same shit working in a data science position can i expect a change of pace from mobile development to data scienceengineering roll', 'title': 'can i expect change of pace from se to ds'}, {'post': 'is there anywhere where i can find a free copy of wikipedia is xml data dumps i have got a web app which as of now crawls wikipedia to get data about page links i know that data retrieval is forbidden under wikimedia is bot policy and also that they regularly release data dumps they ask you to host your own copy if you need to dynamically load pages from another website my entire website is running on a gb digitalocean droplet which is not nearly enough space for a wikipedia data dump so i would like to find somewhere where one of these data dumps is publicly hosted does this exist i am not concerned about images etc just about the text of pages including links', 'title': 'request wikipedia xml data dump hosted'}, {'post': 'hi guys i am a senior at carnegie mellon university thinking about grad school i got accepted into carnegie mellon is master is of statistical practice which is a one year program that will run me around k a lot of money for me i am still figuring out my career goals and was looking for advice on what to do ampxb i am interviewing for full time data analytics roles now i know i will want an mba in the future because my goal and core skillset lies in business not programming or data analytics but i do not know i do think data science is really cool and the master is program definitely prepares you for it i also do not think i will be able to get into a good oneyear program again in the future if i do not accept it now ampxb so my questions are is carnegie mellon is grad program in statistical practice wellrecognized by the industry will it help me get interviews is grad degree in statistics worth the time and money how hard is it to switch from data analytics to data science how do i know data science is for me honestly speaking i am partly interested in it because it is kind of the hot thing right now but currently i am not good at programming or statistical theory i can manage in my stats classes though ampxb thank you so much', 'title': 'grad school or industry'}]\n",
            "\n",
            "Test pairs:\n",
            "[{'post': 'some background i have been learning python for over a year now and i know some sql a bit of r and i have completed some small projects using data science data engineering practices i also know how to work in excel but i do not really have experience using databases i am totally willing to make the time and money investment in something like a bootcamp and i have the means to do fulltime training but i do not want to do this if there is a better faster way to get into the industry what i really want to know is what can i do that will get me a job in the field asap is there some specific bootcamp that will make this happen if so what are the best bootcamps or some particular tech skill i could learn that would basically guarantee that i am hireable very soon if i something like learned microsoft sql server or tableau and given my other skills would this be likely to get me hired i have been looking into bootcamps like thinkful springboard and data application lab the concern i have about these is actually that i already know a lot of the stuff they teach and i am worried that these will be a waste of time and not elevate me to where i want to be i also worry about it taking months to complete these programs as they estimate i would like to be finished in no more than about months thoughts', 'title': 'i want to transition into the data scienceanalyst or related field rather than asking whether i should choose a particular boootcamp or learn some language i would like to hear opinions on what path should i choose that will land me a job of some kind in the field as soon as possible'}, {'post': 'hi all i have created a python module to extract sngrams which is different from traditional ngrams as it embodies linguistic syntactic trees thus making it less arbitrary than traditional ngrams as it goes without saying quality of input feature affects model performance this will help you improve your model accuracy even further built on language models of spacy it can help especially for text classification information extraction query understanding machine translation question answering systems below is an example from sngramextractor import sngramextractor ampxb sngram_objsngramextractortextmeta_tagoriginaltrigram_flagyes texteconomic news have little effect on financial markets outputsngram_objget_sngram printtext print isngram bigramoutput isnbigram print isngram trigramoutput isntrigram economic news have little effect on financial markets sngram bigram cloud_every has_cloud lining_a lining_silver has_lining sngram trigram has_lining_silver ampxb textevery cloud has a silver lining outputsngram_objget_sngram printtext print isngram bigramoutput isnbigram print isngram trigramoutput isntrigram ampxb every cloud has a silver lining sngram bigram cloud_every has_cloud lining_a lining_silver has_lining sngram trigram has_lining_silver ampxb pypi', 'title': 'psngram linguistic features for improving machine learning and deep learning model accuracy for the first time in python new release'}, {'post': 'i am looking at some of the modeling programs the company has been using and there are options to smooth binned data layered linear on bin linear on avg linear on log avg and variable gradient are these methods commonly used in any algorithms i am not sure how i would implement it into r ampxb variable gradient is the most confusing it groups data from neighboring bins with a bin factor being determined the level of smoothing is determined by the radius all bins contribute to the calculation of the bin factor with decreasing weight as the distance from the target bin increases the weight is equal to radius distance radius ampxb once you calculate the overall factor a calculation is performed for each bin weighted by an exposure with various radii and a selected credibility level the calculation that yields the largest distance from is then selected as the factor to be used for that bin ampxb over multiple iterations of the model the factors are blended until the model converges and we receive a final coefficient for that bin ampxb since it is a program i have no clue what is going on in the background to calculate the convergence or what type of model this is it ends up being a multiplicative model so i am guessing it is a log linked glm', 'title': 'does anyone use any type of gradient smoothing for binned data'}, {'post': 'let is say i have created a fraud detection model i already have a process that cleans the data and stores the trainable data in a table in a database and now i want to set up a recurring job that retrains my model every x time period how do people go about this when we want to retrain the model does not that data have to be loaded into the environment that the new model is going to be retrained i guess i am confused at how exactly the model takes the data from the database and starts training on it do people use spark to load it into the environment with the modeltobe then start the retraining process does not that mean that the retraining environment has to have enough space to cover that data being brought in apologies if this has already been asked but i have not seen a clear answer from this subredditfrom what i have found on google thanks so much for any assistance', 'title': 'setting up a model for retraining in production'}, {'post': 'i am working with a dataset of patient performancedata and patient demographics for people with a medical condition i am trying to assess the effect of a treatment on the patients and as i am trying to replicate a randomized experiment from a observational study i am performing matching between the treatment and control group when i try to perform mahalanobis distance matching however i keep getting the following warning on matlab warning matrix is close to singular or badly scaled results may be inaccurate rcond e the warning was caused because the determinant of my covariance matrix was very close to so when i calculated the inverse of this matrix as i need to do for finding the mahalanobis distance the matrix element values become very close to as well after reading about this warning a little bit i found that when this warning is displayed the results of mahalanobis distance matching is not very reliable i thought this was because my matrix became very sparse after i introduced onehotencoding to split the multicategorical features into many different features and hence some of my new features were for category values that had very few occurrences less than to address this issue i got rid of the category values with very low occurrences less than or binned them together with other lowoccurrence category values into slightly larger bins so their chances of occurring increased and my datamatrix was less sparse so i reduced the number of features from to and i plotted the histograms for all my features to see their distributions and this is what it looked like note that the element values have been normalized to by subtracting the feature mean and then divided by the feature standard deviation it looked visually satisfactory to me but i still kept getting the same warning in matlab when i tried finding the determinant of my new covariance matrix it was still very small e in a further attempt to fix this issue i checked whether removing all of the categorical features if we consider the onehot encoded features made things any better by making the matrix less sparse the new featurehistogram looked like this this time when i tried mahalanobis distance matching the warning was removed because the determinant of the covariance matrix was larger e so i tried to see if i could include one of the removed multicategorical features without facing the same problem as these are important features i would prefer to include when doing my matching i added a categorical feature with possible values and all the values very well distributed see histogram below but alas simply including these new onehotencoded features seems to cause the determinant of the covariance matrix to become so small that i start getting the same warning in matlab is there any fix to this issue or is it simply impossible to reliably perform mahalanobis distance matching on this dataset without leaving out the multicategorical values', 'title': 'is mahalanobisdistance matching between points not compatible with onehotencoded datasets'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify test data to subset for evaluation\n",
        "\n",
        "indexes = [408, 711, 1114, 699, 236, 262, 267, 951, 315, 647] #adding the survey indicies\n",
        "combined_indexes = indexes + list(range(90))  # Adding the first 90 indicies\n",
        "\n",
        "new_dataset = []  # Initialize a new list to store the selected elements\n",
        "\n",
        "for index in combined_indexes:\n",
        "    new_dataset.append(test_pairs[index])\n",
        "\n",
        "test_pairs = new_dataset\n",
        "\n",
        "test_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpMHw1z8MO0N",
        "outputId": "d62abb6b-9512-408a-bbe8-68232ed7eb39"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'post': 'suppose you are on a data science team and are given a dataset and problem statement after preliminary steps you know you need to train for example a classifier what does this process look like if you are trying to follow a principled data science workflow and organize yourself according to something like cookiecutter data science the goal is to have a reproducible workflow that is transparent so that anyone on your team could see how you arrived at your results and do it themselves this process naturally involves a lot of experimentation with pipeline steps and model selection as well as hyperparameter tuning one approach i could think of would be to do all of the experimentation with interleaved commentsdiscussion in a notebook say experimentsipynb and then once i pin down the best pipeline configuration for my problem reproduce the entire pipeline in a script modelpy which trains a model from the raw data and saves it are there better ways to do this it is hard to find examples of professionallevel implementations of tools like sklearn amidst the masses of beginnerlevel tutorials out there links to example projects would be appreciated',\n",
              "  'title': 'what does a good scikitlearn workflow look like'},\n",
              " {'post': 'i would like to request a datasite that ranks the best films of all time using the average of the films performances with academy awards box office and critic amp audience rating flickmetrixcom has a similar system but it only ranks the data using aggregate critic and audience scores thats why i believe adding in the other data will give us a more accurate idea of what the best movies are in the method im suggesting i was able to figure out gone with the wind is likely the best movie of all time its in weighted box office sales won oscars including of the main categories and has an on flickmatrix which has already compiled the other scores for me im just unable to create a system or dataset like flickmatrix that uses my system and i had to manually figure out the data for box office and academy awards and add it to flickmatrix data',\n",
              "  'title': 'dataset request ranking best films of all time'},\n",
              " {'post': 'i have been considering a midcareer switch from a nontechnical field to a more quantitative one i have some math and cs backgroundhopefully enough to get into a ms in data science program but probably not enough to get into a quantitative phd program an ideal path for me would be to do an ms in data science work for a few years in the field and evaluate whether i should do a phd eg in stats or computer science so that i can advance further in a subdiscipline like machine learning my question is assuming my background in math think singlevariable calc and linear algebra and cs think introlevel college is not enough for a phd program now will the ms in data science put me in a position to be a phd candidate in the future or do data science masters programs teach mostly professional skills and so would not be a good preparation for phd studies thank you',\n",
              "  'title': 'ds masters subsequent phd studies'},\n",
              " {'post': 'hi everyone i am a practicing attorney considering making the leap to data science my ultimate goal would be to work in financial servicesfinancial industrylegal analytics as that is where my experience is and i feel very comfortable in the corporatebankingfinancial world i have a few questions i was hoping some of you may be able to help me with has anyone else made the leap to data science from a nontechnical career if so how was the experience and are you glad you did it i would be especially interested in hearing experiences from former lawyers or current lawyers who have folded data science into their work what math would you recommend that i learn to prepare for a masters in data science my last math class was over years ago and i think it is fair to say that i am probably at the calc i level on a good day someone recommended that i learn python does anyone have any good recommendations books online class etc for selfteaching specific recommendations would be highly appreciated what are the job prospects for an older studententrylevel job seeker assume earlymid s when finishing the degree i am not worried about the school side so much since i have always been a good student passed two bar exams etc but am concerned that employers may view me as damaged goods because of a previous career thank you so much for your time',\n",
              "  'title': 'career change to data science'},\n",
              " {'post': 'hey everyone i want to become a data analyst and then down the road transition into a data science career i have an applied economics bachelor is and a supply chain management master is i have learned r and python during school and also javascript on my own i have javascript frontend apps projects is my bachelor is and master is combination a good combination for this path field i started learning sql power bi tableau python on my own at freecodecamp is there a roadmap or anything for becoming a data analyst also my previous javascript knowledge and projects would help me in any way getting my first data analyst job a last question a company reached out to me with a supply chain analyst job should i accept it or just focus on the general data analyst path sorry if i am asking stupid questions',\n",
              "  'title': 'how should i start'},\n",
              " {'post': 'hey everyone i am one of the creators of sieve and i am looking for general thoughts on this problem part of what we are trying to do is build easy ways for software engineers and data scientists to interact with rich data imagine making queries and aggregations on videos in the same way you might with numbers and text right now the way we are thinking about it is to turn videos into something that works with the structure of a database like mongodb everything in an image or a video is an object even the frame itself is an object with a large bounding box and each of these objects has some attributes and can be tracked over time with some form of object tracking given that the objects are tracked they can each basically be returned as a timeseries of each of the attributes associated with that object a separate part of our api is a system that automatically allows one to define entities like people dog car cell dirty kitchen or any other object by making api calls in which they are basically providing labeling data that automatically trains a fewshot classifier on these objects our longterm goal is to build a general index over visual data and we think building a great query language great ways to define entities within the query language are important this can be the simple abstraction such that anyone can process and understand visual data such that they do not even need to know computer vision or deep learning for that matter ampxb find all intervals where a person is walking slow get all parts of the video where motorcycles are entering the intersection with speed greater than find all intervals of video with a clean kitchen',\n",
              "  'title': 'building an api query language for rich data like images and video'},\n",
              " {'post': 'hey all i am hoping some of you can help me with a potentially sensitive question i am in the late stages of interviews at a firm for an analytics manager position i know it is not true data science but it is close it would involve a small number of direct reports it is a large fortune based in the se us i am not natively an american but i tend to pass for one i have noticed that all and i mean literally all of the folks i have met in my interviews up down and lateral in the analyticsdata teams are all eastern asian or indian i do not care about working with different cultures but it seemed a flag to me that there are no americans there should that concern me that working conditions are poor ie americans would leave under those conditions or is that typical in expanding analyticsdata science teams at this point',\n",
              "  'title': 'joining firm company culture concern'},\n",
              " {'post': 'i would like to take my career in a new direction with machine learning and am exploring the different pathways available one way is to go to school and get a masters degree the program i have been exploring is the masters of data science at berkeley the curriculum has what i want and with the distance learning i can be anywhere in the world and still get an education however the tuition for the program is there is also the masters program in machine learning at cmu and the machine learning masters program at university college london but these would require relocating since they are not available online the other path is to take classes online there are several available that i can take from places like dataquest linuxacademy sciencealert udemy etc i can pick and choose which ones i want and complete them at my own pace plus the total cost would be less than of the tuition at berkeley and since i am making a huge leap from being simply a troubleshooter to an ai architect i will need to show potential employers what i can do the plan is to post projects on github showing what i have worked on both in class and on my own i intend to do this regardless of which path i take i am hoping this can show the progression of my knowledge and be the bridge between what is listed on my cv and what i can do so now i am wonderingif the cost is not an issue which of these paths would work best in terms of impressing an employer will it be the degree or the drive that would pique someone is interest to hire me please let me know what you think',\n",
              "  'title': 'is a pricey masters degree worth it'},\n",
              " {'post': 'greetings i would like to ask for advice from this community i am entering the world of ai and while i am not completely new to probability programming or automation i have a lot to work on luckily there is alot of online content to help with studying but time is limited so i have to focus my efforts obviously i should learn ml algorithms and libraries for python only right now but i believe there is more to it than that what i would like to ask i do not come from a cs background the most i have done is programming some microcontrollersvectorial calculations with matlab is it beneficial to invest time in some software engineering course to make it easier to develop scripts or is it overkill as a ml engineer it would be a significant time investment is it possible to start a career in ml without data manipulation tools something like sql spark i intend to get to them when feasible after starting to work but is it a necessary precondition for the job i have not been able to find a conclusive answer by just looking at job ads or kaggle is python used in the industry or are other languages preferred i only really know c but i am not even going to try with it i want to thank everyone who takes the time to read this and even more those who answer',\n",
              "  'title': 'seeking advice for learning path'},\n",
              " {'post': 'hello i am a computer engineer and for one of my projects is ai detection for crime scene evidence analysis and for me to do that i need a photo or video datasets of crime scenes i tried searching everywhere i cannot find any database of crime scene photos does anybody know of anywhere i can find some photos or it forensic competitions that have such datasets i need it urgently please help a professor suggested using kaggle however i could not find anything plus i am not sure how i can use it the dataset does not even have to be real i just need anything to start working on my project thanks',\n",
              "  'title': 'crime scene datasetphoto and video database'},\n",
              " {'post': 'some background i have been learning python for over a year now and i know some sql a bit of r and i have completed some small projects using data science data engineering practices i also know how to work in excel but i do not really have experience using databases i am totally willing to make the time and money investment in something like a bootcamp and i have the means to do fulltime training but i do not want to do this if there is a better faster way to get into the industry what i really want to know is what can i do that will get me a job in the field asap is there some specific bootcamp that will make this happen if so what are the best bootcamps or some particular tech skill i could learn that would basically guarantee that i am hireable very soon if i something like learned microsoft sql server or tableau and given my other skills would this be likely to get me hired i have been looking into bootcamps like thinkful springboard and data application lab the concern i have about these is actually that i already know a lot of the stuff they teach and i am worried that these will be a waste of time and not elevate me to where i want to be i also worry about it taking months to complete these programs as they estimate i would like to be finished in no more than about months thoughts',\n",
              "  'title': 'i want to transition into the data scienceanalyst or related field rather than asking whether i should choose a particular boootcamp or learn some language i would like to hear opinions on what path should i choose that will land me a job of some kind in the field as soon as possible'},\n",
              " {'post': 'hi all i have created a python module to extract sngrams which is different from traditional ngrams as it embodies linguistic syntactic trees thus making it less arbitrary than traditional ngrams as it goes without saying quality of input feature affects model performance this will help you improve your model accuracy even further built on language models of spacy it can help especially for text classification information extraction query understanding machine translation question answering systems below is an example from sngramextractor import sngramextractor ampxb sngram_objsngramextractortextmeta_tagoriginaltrigram_flagyes texteconomic news have little effect on financial markets outputsngram_objget_sngram printtext print isngram bigramoutput isnbigram print isngram trigramoutput isntrigram economic news have little effect on financial markets sngram bigram cloud_every has_cloud lining_a lining_silver has_lining sngram trigram has_lining_silver ampxb textevery cloud has a silver lining outputsngram_objget_sngram printtext print isngram bigramoutput isnbigram print isngram trigramoutput isntrigram ampxb every cloud has a silver lining sngram bigram cloud_every has_cloud lining_a lining_silver has_lining sngram trigram has_lining_silver ampxb pypi',\n",
              "  'title': 'psngram linguistic features for improving machine learning and deep learning model accuracy for the first time in python new release'},\n",
              " {'post': 'i am looking at some of the modeling programs the company has been using and there are options to smooth binned data layered linear on bin linear on avg linear on log avg and variable gradient are these methods commonly used in any algorithms i am not sure how i would implement it into r ampxb variable gradient is the most confusing it groups data from neighboring bins with a bin factor being determined the level of smoothing is determined by the radius all bins contribute to the calculation of the bin factor with decreasing weight as the distance from the target bin increases the weight is equal to radius distance radius ampxb once you calculate the overall factor a calculation is performed for each bin weighted by an exposure with various radii and a selected credibility level the calculation that yields the largest distance from is then selected as the factor to be used for that bin ampxb over multiple iterations of the model the factors are blended until the model converges and we receive a final coefficient for that bin ampxb since it is a program i have no clue what is going on in the background to calculate the convergence or what type of model this is it ends up being a multiplicative model so i am guessing it is a log linked glm',\n",
              "  'title': 'does anyone use any type of gradient smoothing for binned data'},\n",
              " {'post': 'let is say i have created a fraud detection model i already have a process that cleans the data and stores the trainable data in a table in a database and now i want to set up a recurring job that retrains my model every x time period how do people go about this when we want to retrain the model does not that data have to be loaded into the environment that the new model is going to be retrained i guess i am confused at how exactly the model takes the data from the database and starts training on it do people use spark to load it into the environment with the modeltobe then start the retraining process does not that mean that the retraining environment has to have enough space to cover that data being brought in apologies if this has already been asked but i have not seen a clear answer from this subredditfrom what i have found on google thanks so much for any assistance',\n",
              "  'title': 'setting up a model for retraining in production'},\n",
              " {'post': 'i am working with a dataset of patient performancedata and patient demographics for people with a medical condition i am trying to assess the effect of a treatment on the patients and as i am trying to replicate a randomized experiment from a observational study i am performing matching between the treatment and control group when i try to perform mahalanobis distance matching however i keep getting the following warning on matlab warning matrix is close to singular or badly scaled results may be inaccurate rcond e the warning was caused because the determinant of my covariance matrix was very close to so when i calculated the inverse of this matrix as i need to do for finding the mahalanobis distance the matrix element values become very close to as well after reading about this warning a little bit i found that when this warning is displayed the results of mahalanobis distance matching is not very reliable i thought this was because my matrix became very sparse after i introduced onehotencoding to split the multicategorical features into many different features and hence some of my new features were for category values that had very few occurrences less than to address this issue i got rid of the category values with very low occurrences less than or binned them together with other lowoccurrence category values into slightly larger bins so their chances of occurring increased and my datamatrix was less sparse so i reduced the number of features from to and i plotted the histograms for all my features to see their distributions and this is what it looked like note that the element values have been normalized to by subtracting the feature mean and then divided by the feature standard deviation it looked visually satisfactory to me but i still kept getting the same warning in matlab when i tried finding the determinant of my new covariance matrix it was still very small e in a further attempt to fix this issue i checked whether removing all of the categorical features if we consider the onehot encoded features made things any better by making the matrix less sparse the new featurehistogram looked like this this time when i tried mahalanobis distance matching the warning was removed because the determinant of the covariance matrix was larger e so i tried to see if i could include one of the removed multicategorical features without facing the same problem as these are important features i would prefer to include when doing my matching i added a categorical feature with possible values and all the values very well distributed see histogram below but alas simply including these new onehotencoded features seems to cause the determinant of the covariance matrix to become so small that i start getting the same warning in matlab is there any fix to this issue or is it simply impossible to reliably perform mahalanobis distance matching on this dataset without leaving out the multicategorical values',\n",
              "  'title': 'is mahalanobisdistance matching between points not compatible with onehotencoded datasets'},\n",
              " {'post': 'hi all i am applying for a data science internship with a well known company in the sf area think uberairbnbpinterest etc one of their points under qualifications is amust be currently pursuing a master is or doctoral degree preferred fields of study are statistics math economics or related discipline this presents a few problems namely that i am a junior undergraduate studying cs that said i have more experience than the typical undergrad and from what i have seen the typical masters student with respect to data science here is what i have got going for me very proficient using python r and sql among many other data analysisscience tools just finished a summer internship at a wellknown tech company as a data science intern built the company is new anomaly detection system in r from the ground up and deployed it just started a fall data science internship in nyc with an ecommerce company will be working on customer segmentation with some sql reports and machine learning taking two data science masters courses tutoring for a masterslevel algorithms course i know i do not fit their exact description but do you think i should apply anyways i would love to intern here and would appreciate any tipsexperiences any of you have had on applying where you do not necessarily meet every single criteria thanks',\n",
              "  'title': 'education level on applications'},\n",
              " {'post': 'i m am a junior in college major in statistics i go to a non target school and work as an intern at the center for predictive analytics at my university upon graduating i will receive a full fellowship to continue doing analytic research for our math department thus i will not enter the full time job market for approximately years from now i am planning to do some part timeremote work during my years of grad school this semester i have started building the habit of hours per day for self studying and working on ds projects i am committed to keeping up a similar routine for the next years with the intent that i might achieve hours of data science experience within the next years i decided to follow the ds career path after reading a book about big data i think it is a good career for me as i am into data math etc but i am also in it for the money and and mostly motivated by achieving a high salary so that i can save up money and start my own business in my s maybe idk is it unreasonable for me to expect that i should be able to far exceed a salary of k assuming high col within a decade from now if now how many yoe should i expect to achieve similar salary',\n",
              "  'title': 'how many yoe should i expect to have before i hit kk salary range'},\n",
              " {'post': 'hello i am soon to be graduating student this spring in an undergraduate data science degree after doing extensive research i feel like my program had not emphasized math courses as much as i will only have calculus under my belt i have been looking at different graduate schools and i am looking at an ms in computer science i wonder though if my program has lacked sufficient areas for a program like this i was wondering if anyone here has had a similar issue i am starting to think my undergraduate degree in data science was a mistake and that i should have pursued a bs in either computer science or statistics here are some examples of what i have taken cs algorithms and cs data base management cs machine learning data science unsupervised learning data science management structured data data science other general data science things stats intermediate states stats regression stats principle study design stats experimental design by the time i will graduate i will have a minor in stats otherwise i could pursue a ms in stats or data science but i think i am wrong if i pursue those areas since with stats i would have to spend time taking the additional math before the more advance courses and i think the data science ms would just redoing things that my program already covered thoughts',\n",
              "  'title': 'a potential blind spot for new data science degrees'},\n",
              " {'post': 'a little background on myself i graduated from a good university with a degree in civil engineering then i worked years in construction management and hated it over that time period i took a few classes and began teaching myself programming and data science i created a website and started uploading all the project that i had completed it took a while to get a job but i finally landed a job as a data analyst and love it i had thought about getting a masters during my job hunt since it took so long but stuck it out and landed this job i want to continue my career in the data science direction and want to know how revered masters and phds are in this field a lot of times high level data scientists at great tech companies either for sure have a masters and a lot of times have phds my main draw back on getting these degrees is the money it would cost because i feel like i could honestly teach myself the same skill set without having to pay for the school all in all i have a good job working as a data analyst but how far can this experience im gaining get you without having a masters or phd i almost went into structural engineering career path after college but everyone i talked to said that you had to get a masters and that was that so im interested to see other peoples perspectivesthoughts on this for data science to sum it up is it work it to pay for mastersphd and what curriculum specifically would you get the most out of as it relates to data science',\n",
              "  'title': 'should i get a mastersphd'},\n",
              " {'post': 'i checked out the faqs but was hoping for some targeted feedback i am currently entering the th year of my phd in neuroscience on top of a bachelors in psych and a masters in neuro i fundamentally love my work but recent observations have convinced me that continuing on the path of academia is just not right for me and my family late last year i took up python and have completed a couple of small projects to help automate my lab and expedite data analysis it got me thinking about data science i figure i have two years to make myself into a something that some company somewhere will want how can i do it my thoughts are to get some mooc certificates complete a handful of projects in the lab that use data science to save timeimprove outcomes etc network by shouting out of my window at cars driving by i realize that my path is nontraditional but i am hoping there is a way to repackage many of the problemsolving and analytical skills that i have earned in my science education thus far as groundwork for a job in data science all feedback is welcome',\n",
              "  'title': 'two years until i apply for jobs what should i work on'},\n",
              " {'post': 'data science has tremendous growth opportunities and is one of the hot careers in the current world many businesses are thriving for skilled data scientists data science requires many skills to become an expert one of the important skills is python programming python is a programming language widely used in many fields it is considered as the king of the coding world data scientists extensively use this language and even beginners find it easy to learn the python language to learn this language there are many python data science courses that guide and train you in an effective way what is python python is an interpreted and objectoriented programming language it is an easily understandable language whose syntaxes can be grasped by a beginner quickly it was found by guido in it is supported in operating systems like linux windows macos and a lot more the python is developed and managed by the python software foundation the second version of python was released in it features list comprehension and reference counting this version was officially stopped functioning in currently only the python version x and later versions are supported why python is used in data science python is the most preferred programming language by the data scientists as it effectively resolves tasks it is one of the top data science tools used in various industries it is an ideal language to implement algorithms pythons scikitlearn is a vital tool that the data scientist find it useful while solving many machine learning tasks data science uses python libraries to solve a task python is very good when it comes to scalability it gives you flexibility and multiple solutions for different problems it is faster than matlab the main reason why youtube started working in python is because of its exceptional scalability features of python language python has a syntax that can be understood easily it has a vast library and community support we can easily test codes as it has interactive modes the errors that arise can be easily understood and cleared quickly it is free software and it can be downloaded online even there are free online python compilers available the code can be extended by adding modules these modules can also be implemented in other languages like c c etc it offers a programmable interface as it is expressive in nature we can code python anywhere the access to this language is simple so we can easily make the program working the different types of python libraries used for data science matplotlib matplotlib is used for effective data visualization it is used to develop line graphs pie charts histograms efficiently it has interactive features like zooming and planning the data in graphics format the analysis and visualization of data are vital for a company this library helps to complete the work efficiently numpy numpy is a library that stands for numerical python as the name suggests it does statistical and mathematical functions that effectively handles a large narray this helps in improving the data and execution rate scikitlearn scikit learn is a data science tool used for machine learning it provides many algorithms and functions that help the user through a constant interface therefore it offers active data sets and capable of solving realtime problems more efficiently pandas pandas is a library that is used for data analysis and manipulation even though the data to be manipulated is large it does the manipulation job easily and quickly it is an absolute best tool for data wrangling it has two types of data structures ie series and data frame series takes care of onedimensional data and the data frame takes care of twodimensional data scipy scipy is a popular library majorly used in the data science field it basically does scientific computation it contains many submodules used primarily in science and engineering fields for fft signal image processing optimization integration interpolation linear algebra ode solvers etc importance of data science data scientists are becoming more important for a company in the st century they are becoming a significant factor in public agencies private companies trades products and nonprofit organizations a data scientist plays as a curator software programmer computer scientist etc they are the central part of managing the collection of digital data according to our analysis we have listed below the major reasons why data science is important in developing the worlds economy data science helps to create a relationship between the company and the client this connection helps to know the customers requirements and work accordingly data scientists are the base for the functioning and the growth of any product thus they become an important part as they are involved in doing significant tasks ie data analysis and problemsolving there is a vast amount of data travelling around the world and if it is used efficiently it results in the successful growth of the product the resulting products have a storytelling capability that creates a reliable connection among the customers this is one of the reasons why data science is popular it can be applied to various industries like healthcare travel software companies etc big data analytics is majorly used to solve the complexities and find a solution for the problems in it companies resource management and human resource it greatly influences the retail or local sellers currently due to the emergence of many supermarkets and shops the customers approaching the retail sellers are drastically decreased thus data analytics helps to build a connection between the customers and local sellers are you finding it difficult to answer the questions in an interview here are some frequently asked data science interview questions on basic concepts q how to maintain a deployed model to maintain a deployed model we have to monitor evaluate compare rebuild q what is random forest model random forest model consists of several decision trees if you split the data into different sections and assign each group of data a decision tree the random forest models combine all the trees q what are recommendation systems a recommendation system recommends the products to the users based on their previous purchases or preferences there are mainly two areas ie collaborative filtering and contentbased filtering q explain the significance of pvalue pvalue lt rejects the nullhypothesis pvalue gt accepts nullhypothesis pvalue it will either except or deny the nullhypothesis q what is logistic regression logistic regression is a method to obtain a binary result from a linear combination of predictor variables q what are the steps in building a decision tree take the full data as the input split the dataset in such a way that the separation of the class is maximum split the input follow steps and to the separated data again stop this process after the complete data is separated best python data science courses many websites provide data science online courses here are the best sites that offer data science training based on python greatlearning coursera edx alison udacity skillathon konvinity simplilearn how data science courses help in a successful career postcovid pandemic the economic downfall due to covid impacts has lead to upskill oneself as the world scenarios are changing drastically adding skills to your resume gives an added advantage of getting a job easily the businesses are going to invest mainly in two domains ie data analysis of customers demand and understanding the business numbers it is nearly impossible to master in data science but this lockdown may help you become a professional by indulging in data science programs firstly start searching for the best data science course on the internet secondly make a master plan in such a way that you complete all the courses successfully many shortterm courses are there online that are similar to the regular courses but you can complete it within a few days for example analytix labs are providing these kinds of courses to upskill yourself so this is the right time where you are free without any work and passing time you can use this time efficiently by enrolling in these courses and become more skilled in data science than before these course providers also give a data science certification for the course you did this will help to build your resume data science is a versatile field that has a broad scope in the current world these data scientists are the ones who are the pillars of businesses they use various factors like programming languages machine learning and statistics in solving a realworld problem when it comes to programming languages it is best to learn python as it is easy to understand and has an interactive interface make efficient use of time in covid lockdown to upskill and build yourself',\n",
              "  'title': 'why python is used in data science how data science courses help in a successful career post covid pandemic'},\n",
              " {'post': 'to be as brief as possible i graduated with a bachelor is in political science almost a decade ago for some reason thinking that i would be okay just having a degree after many years of bouncing around i started taking some moocs which led me to eventually completing a master is in analytics degree i got hired on over a year ago by a fortune company to do wouldata science but it turns out that most of the tools required to do such work r python are not approved for use due to the open source component sas is approved but my department still has not received its licenses i work with billions of rows of data on a daily basis using teradatasql and find trends inconsistencies make recommendations etc but i am really missing the predictive side that i studied so hard to learn given the above what is the best way to transition into a role where i can do more predictive work i do not even need a wouldata scientist title but i just want to work on modeling and use machine learning etc should i put a github together and put some project work in there i just feel a bit hopeless as in without any onthejob predictive analytics work i will never be able to transfer into one of these roles in the future',\n",
              "  'title': 'how to transition from data analyst to a data science role'},\n",
              " {'post': 'from a press release in an effort to inject more financial transparency into college sports the knight commission on intercollegiate athletics has unveiled a revamped and innovative college athletics financial information cafi database the new resource provides unprecedented access to athletics revenues expenses and debt as well as institutionwide academic spending for more than public ncaa division i colleges and universities dating back to the free openaccess database provides telling documentation of major college sports finances at a time when institutions face lawsuits to direct more financial benefits to studentathletes for the first time database users are able to view rich graphics that demonstrate by institution conference or competitive subdivision where the money comes from and where the money goes in college sports',\n",
              "  'title': 'knight commission unveils new college sports financial database'},\n",
              " {'post': 'i work for an ecommerce company and we are getting to the point where automating much of our warehouse inventory management makes sense i came across this harvard business review article which though interesting is a little sparse on the details for instance it mentions that the team had to account for lowvolume product orders that befuddled its datahungry machinelearning algorithms but it does not go into any details how they went about that it does not have to be amazon but i would like to find some resources that would help me understand how best to go about tackling this problem and problems like it if you cannot think of any available resources how would you go about taking hands off the wheel',\n",
              "  'title': 'how to learn more about amazon is automation'},\n",
              " {'post': 'hello all so i come from a biology genetics background but i would lile some computer science baclground to increase career growth please forgive me for my lack in knowledge of these terminology but i want to make sure i make the right choice before committing to a master is program which might not help me in the long run usf health informatics i originally thought health informatics is the same as bioinformatics but now i am thinking they are not what career choices do i have with health informatics the advisor was not so clear in that department and provided vague career choices he said salary ranges kk usd but im still confused in what they actually do uf medical microbiology i knowvery close to what i am already in but they offer bioinformatics and unix bioinformatics as electives and i figured ill get a taste of what these courses entail and how they are used in the medical field the advisor here said that there is a possibility of getting a graduate cert in bioinformatics all depends on covid class settings but we didnt go too in depth on what bioinformatics can do career wise is there any other program field i should consider i am not a math wiz nor knowledgeable in coding but i am very curious about these fields what advice do you have for a sciencebiology lab tech like myself thanks in advance and stay safe everyone',\n",
              "  'title': 'confusion regarding bioinformatics and health informatics'},\n",
              " {'post': 'im currently in healthcare and have clinical and administration background i wanted to start self teaching myself sql and get my foot in the door as an analyst in healthcare ive truly always been scared at of math and numbers i never thought i would ever try to get into a role that required the word data or even analyst but i want to learn entry level skills and shift away from patients is self teaching yourself enough to be marketable for entry level data analyst jobs how hard would it be for someone who has never dived into sqlpython to teach themselves especially only having administration and clinical background are the beginner courses super hard to grasp at first im taking udemy courses and some good old youtube to help me start off any input',\n",
              "  'title': 'is it hard to become an entry level data analyst'},\n",
              " {'post': 'hey there will try to keep this short i am a master student in the internet of things with a focus on data analysis i have been looking for summer internships since two weeks with over applications i had only interview and the rest directly refused me i would say the interview went well it was more about introducing myself and them introducing the company and after week i got a call that i was rejected i am applying a tons of applications and mainly in the domain of data science and data analysis i really do not know what is wrong with my resume i included a link below you can find all details about me would love to have a feedback thank you',\n",
              "  'title': 'what is my wrong with my resume'},\n",
              " {'post': 'cross post from rcscareerquestions background i have been a data scientist for about years now i have a bachelors degree in economics i started out doing risk modeling for an insurance company about months ago i was hired by a small consulting company my role here is more of a machine learning engineer i create machine learning systems to automate business processes i am proficient in python sql and r ampxb current job issues machine learning is a hard field to do consulting work in most companies that have adequate data infrastructure already have a data science team those that do not have the appropriate infrastructure cannot benefit from machine learning as a result i am getting little experience in my current role so i am looking for a change for every project i have there are weeks of no work doing this time i get certs and do research right now i only have uipath certification ampxb general concerns about the field of data science i live in a medium sized city on the east coast not a ton of data science jobs additionally i do not enjoy the modeling aspects of the job nearly as much as the engineering i understand most commonly used models when to use them their pitfalls etc however my interest with them is very applied i do not have any desire to develop new machine learning algorithms only using preexisting tools to add business value i would like to use machine learning as one of many tools to automate business processes ampxb future steps i am thinking about leaving machine learning specific roles in favor of a software developer role but i do not know if i could close to my current pay k i can take a slight pay cut but not too significant ampxb questions could i get a decent job as a python software developer with my current experience should i just try to get a machine learning engineer at a different company any other input ampxb tldr i like the engineering side of machine learning the most in a dead end job what should my next step be',\n",
              "  'title': 'data scientist looking for a change'},\n",
              " {'post': 'hi everyone i will keep this short because i was just finishing up this post and i got a blue screen and lost everything i had typed for the past hour hahahah dear god here we go again i will laugh if the diagnostic test crashes my pc sorry for the format or misspellings in advance i just want it to be done basically i am going to start applying for jobs on data analysis and i need insight on whether what i am preparing myself with is useful or not some information about myself i am and a night time college student doing a bsc cis recently i just finished a course on sql and found the whole design and visualization of data to be super fun and interesting so much so that i have decided that this is something i want to pursue work experience was a part time job fixing and selling computers been doing this for about a year now and found it really boring since every thing that was damaged was the same things specially ram left it about a month ago and with my free time i have been finding more about the data analysis profession now then i will list what i have been doing with my free time to prepare for the job this will have things that i think are needed and those that i think would be fun to have some knowledge of afterwards ill link and paste the description of job listing that i feel has the most things in common with the rest job listings ampxb things that i have been teaching myself are statistics sql and excel i did take courses on these two but it was to general power bi classification decision trees and model evaluations starting this today i am looking to run some linear regressions to see the relationship between advertising costs and their expenditure i have lined up to learn six sigma and their methodologies but i am not sure if it worth learning this yet intermediate knowledge on visual c and basics of java down the line i will look into r or phyton i have seen vmware pop up in some job listing so i have it on the list of things to know about hadoop analytics and hbase saw some listing that had these and i want to know if they are needed ampxb and that is about it on things i am doing i saw somewhere some predictive analysis on determining the infected outburst of diseases and i thought a similar project could be fun to better understand the spread of diseases ampxb the insight i am looking for is into what preparations i should be focusing on for a job as a data analyst the following list will be an the job description of a job listing provide organization and management of case files review data completeness of information proper execution extract data from data base obtain additional information from other investigative agenciesdata base establishmaintain physical file prepare noticesadvertisements receive suspense petitions claims process sharing requests reconcile inconsistencies prepare declarations gather information and organize investigative package verify case files and case tracking system maintain internal status information on the disposition of all forfeited assets assure information is accurate and perform analytical computations necessary to process data conduct and reconcile inventories distribute and receive documents assist lead analyst or official in obtainingcollecting all documentsinformation to complete case file provide administrative information and assistance concerning case to other investigative agencies local law enforcement agencies us attorney and other doj processing units and higher headquarters extract data from agency data base for management and program reports perform word processing relevant to case documentation perform data entry relevant to case ampxb',\n",
              "  'title': 'looking for insight on data analysis on these job description'},\n",
              " {'post': 'im at a bit of a crossroads in my masters in data science ive got options when it comes to two groups of courses for the first i can take database systems which covers dbms architecture models sql data modeling and entity relationship diagrams i could also choose to take artificial intelligence which covers heuristic and stochastic searches logical and probabilistic reasoning game playing planning and reinforcement learning this one is the most pressing as i need to choose one for the fall second i can take machine learning which covers current research in the field derived from recently published literature pretty vague i could also take advanced data mining which covers clustering classification and pattern recognition this one concerns courses next year im hoping to get some insight on how choosing one or the other would benefit a career in data science and to see what professional data scientists would recommend for career options thank you',\n",
              "  'title': 'masters course selection'},\n",
              " {'post': 'i graduated in dec and had a data analytics job lined up in a small management consulting company in dc starting this summer got an email yesterday saying they will be rescinding my offer due to the covid pandemic tbh i kinda saw this coming and was applying at other companies i had an interview at oracle scheduled but they went under hiring freeze weeks ago and now they are ghosting my emails i have applied to over jobs and had referrals for a couple and havent heard back or rejected as far as skills go im pretty good at pythonsql and had a couple technical internships and decent side projects graduated from a nontarget with a cs degree and mostly applying to data analyticsjunior data scientists positions i network almost everyday on linkedin and apply to atleast jobs everyday would appreciate any advice',\n",
              "  'title': 'recent grad who had his offer revoked and interview cancelled'},\n",
              " {'post': 'hi all i am interested in learning what capabilities and techniques other data science teams have and i was wondering if i could post a quick survey here i think this is in line with the sub is policy especially since hopefully people is answers will be interesting clarification by you i mean either yourself or someone who can work with you do do this almost immediately eg not having to go to it or anything like that do you use other programming languages than python if so what do you use bi tools such as powerbi qlik etc do you have a direct connection to a database or do you just work through an api or library or something else if so what is the main database eg postgres ms sql do you have the ability to host dashboards eg using dash for internal to your company use do you have the ability to host dashboards for clients do you have the ability to set up an api for internal use do you have the ability to set up an api for public use which industry do you work in how large is the company just order of magnitude eg etc',\n",
              "  'title': 'what capabilities does your team have'},\n",
              " {'post': 'i have an undergrad degree in finance and have been working at an asset management firm for over years now while the experience has been awesome i realized my passion lies elsewhere and want to get into data science as a potential career change preferably to something closer to business intelligence or business analytics ive been looking at graduate degrees in business analytics like the one from nyu stern to help me transition i know this is not purely data science but i think it uses a lot of related skill sets i was hoping to get some advice from people here on whether there is a place for me in the analytics field especially coming from a non techengineering background would firms value my less traditional background from a data science standpoint',\n",
              "  'title': 'transitioning to ds from financeinvestment background'},\n",
              " {'post': 'i am working in the field of machine vision where accuracy and performance both play a major factor in deciding the approach towards a problem traditional rule based approaches work quite well in such cases i am gradually migrating towards deep learning due to its umpteen advantages where the results seem promising albeit with two huge caveats lack of training data in this field to be precise lack of erroneous data performance issues on inference accuracy and speed are required in equal proportion and cannot be compromised in industrial settings point plays a strong factor i have been dabbling with transfer learning techniques and using pretrained models to overcome this situation for simpler applications such as classification this suits and gives good results in other cases such as detection and localization i have tried using maskrcnn which gives really good results but poor inference speed means it is not production ready the worrying factor in both the cases is how slow detection and inference is compared to traditional vision algorithms a solution would be to buy machine vision software specifically from companies such as cognex halcon etc who sell deep learning bundles they are quite expensive and are to be used out of box with minimal modifications which does not suit me currently point is highly necessary in production lines where each iterationimage may take less than ms for execution deep learning gives a lot of opportunities in getting state of the art results with very less data in most of the situations but in general without inference optimization in using apps such as tensorrt the time metric does not give good results is there an approach in using open source that can solve both point and point creating a cnn from scratch is out of the question',\n",
              "  'title': 'overcome caveats on using deep learning for faster inference on limited performance availability'},\n",
              " {'post': 'hi as part of my cs masters thesis i am working with vector representations of words i am trying to increase the similarity between one word vector w and another w by transforming w however i want to do this in a way that an attempt is made to maintain the similarity between w and the other vectors wwn as best as possible i have spoken to my academic supervisors about this and they suggested using gradient descent as a way of finding a transformation matrix which can do this unfortunately i have no idea about how gradient descent could be applied for this or even if it can be used to solve the problem could someone help me understand how such a problem could be solved using gradient descent if not are there any other solutions to this problem thank you',\n",
              "  'title': 'increase similarity between two vectors whilst maintaining similarity to other vectors'},\n",
              " {'post': 'i am hoping somebody here has some experience with marketing mix models and canprovide some guidance i have experience working around mmms but i have always been at a firm that outsources the actual model building to a consultant i have never gotten close to the nitty gritty details of the model building process i am experienced in r or well versed enough to googlefu my way through most issues if i am successful this will be used as a forecasting tool not an roi measurement accuracy is necessary but nobody will be getting fired because of it is outcome i have gone through the unpleasant process of collecting and aligning weekly sales data with weekly media data impressions by media channel tv print digital for the past months i have also created a comprehensive list of sales promotions and major holidays that impact the business i am looking for advice on what process to take when attempting to build this thing here is my current roadmap determine seasonality holiday impacts and underlying trend of the data probably use the forecast package in r for this but i have also thoughts about just using dummy variables to let my model take care of that after i have deasonalized the data i will build the first draft of the model with media data will probably use a gam approach which i think will handle the minimum thresholds and diminishing returns that are common with advertising pressure a pretty good article about that is here i should note that i am not married to the gam approach review model fit and significant variables do some variable clustering and generally tweak the model apply some type of adstock tranformation to better account for the lag impact of the media i have found a few approaches online that look promising with this seeming to have promise rerun model and play with adstock functions until i have something worthwhile my question is do i need all of these steps or can i just dummy out the seasonal impacts and include the untransformed lagging media variables in my first pas of the model how do i go through the process to both account for the s curve of media impact and the adstock effect of the media',\n",
              "  'title': 'marketing mix model help'},\n",
              " {'post': 'hi all i am relatively new to machine learning i have using tensorflow and keras for a few months now and want to try and move on to some more complex problems i was wondering if i already had a model in place would a neural network be well suited to learning how to solve a rubik is cube i have practised using reinforcement learning and qlearning and it feels like those systems could be applied to solving a rubik is cube but i could be being stupid if this seems like to advanced a project another problem i would be interested in solving is a chess game this seems like the traditional ai problem but i worry that the reason why it is more well known is because of the complexity of it thanks in advance i would appreciate any feedback hope i am not being too naive in my initial thoughts',\n",
              "  'title': 'project help'},\n",
              " {'post': 'i am conducting some graduate research on a data visualization website and am looking for people to interview the interview should last minutes and i can compensate you paid through venmo or paypal ampxb the research is to make design recommendations to improve the site for users i am studying user experience design ampxb i am looking for people who are educators journalists or those involved in newsmedia those interested in data visualization or statistics andor people who have a keen interest in world news ampxb if you are interested tell me a little a bit about yourself via email at saraheliotuxatgmaildotcom and let me know your availability i will be conducting the interviews this thursday through friday',\n",
              "  'title': 'data enthusiasts paid interview graduate research project'},\n",
              " {'post': 'hi everyone i have an odd question i have several vectors representing muscles species vectors in each species and different orientations for each vector different time intervals for this work what matters is not the length of the vector it is the direction it is pointing does anyone know a good way for me to describeshow this so it is easy for other people to understand simple describing it as shifting n degrees around the dorsalventral axis m degrees around etc does not seem to be a good idea with so many vectors and different axes there is no way a person would get a good idea of what is happening if you have any advice i would definitely appreciate it',\n",
              "  'title': 'i need help representing data about d changes in vector orientation'},\n",
              " {'post': 'i cannot view any data in the behavior flow section for my ios app in google analytics i set up tracking months ago and the behavior flow section has always looked like this you may have applied a condition such as a date range an advanced segment or a goal for which there is no data i only want to view pathing between events the events are set up and recording properly there have been hundreds of thousands of sessions in this time frame so there should be sufficient data to pull from there is a ga document for mobile apps which states that events paths are automatically tracked in the behavior flow section i am seeing that i do not have any data for screens so maybe that could be the problem do i need data for screens to view the behavior flow even though i only want to view paths between events if so how do i set that up i thought screen tracking was set up automatically',\n",
              "  'title': 'why cannot i view any data in the behavior flow'},\n",
              " {'post': 'i am currently in the midst of interviewing for data science jobs in the bay area i have a master is in statistics but am having trouble talking about my experience i have browsed this subreddit but have not found answers to this specific question i get really nervous in interviews so i want to have written down what exactly i want to communicate about my past projects right now i am able to communicate the problem ie did the drug work the method logistic regression repeated measures mixed model etc and the outcome ie pain scores improved over time but struggled in my last interview when asked for more details because i did not know what details he might have wanted and when i asked he said something like just tell me more about that logistic regression i did not follow it this was a phone interview so i was not sure what level of detail i should go into what other details should i be ready to present what all the variables were how regression works how i coded the variables what the estimates were one of my issues is that the projects i am discussing have been lengthy and very complicated so should i choose smaller projects i am beginning to think presenting a homework problem where i did a basic linear regression or classification tree would be much easier to talk about but of course i would not say it was a homework problem tldr any advice on what exactly interviewers want to know about your past projects any insights to help me feel less intimidated',\n",
              "  'title': 'interviews what do they want to know about my past projects'},\n",
              " {'post': 'this is a long story then if you do not mind reading everything i write in this post and then give me any advice i will be very thankful i am a student of forestry engineering at a university in colombia in this career there is a lot of forest measurements and ecology analysis that make me fall in love with r and little by little make me fall in love with statistics and programming i realized after those subjects i wanted to combine ecologybiology with data science i started to search where to learn it and i found a lot of mooc is options datacamp udemy and so on every one of them selling the impressions that with those mooc is anyone could be a data scientist searching more deeply i realized that those courses just make an introduction of what really data science is i have to practice a lot on real problems on real situations different of practicing with the iris dataset or the titanic dataset and i am okay with it but keep reading i will explain why i have fo find another strategy since i am in south america the currency of my country is devaluated so investing my little budget on those mooc is is expensive i do not want to invest a lot of money in courses that i cannot use to apply for a job later or in which i have to study at a different university to really study data science i realized that the best way to be a professional data scientist is that i have to study at another university but i do not have the money at this moment the thing is that if a have to practice a lot search real datasets and accumulate projects more than certifications of a complete career track on those mooc is then i will have to learn from other sites books i wish free and other courses i have read a comment on quora that a course with more practical skills challenges exams is python for data science and machine learning bootcamp then it will be great to see the free course of deep learning in coursera the person who said that believed that those courses are better than datacamp dataquest so i have to start over there but i want another opinion and if that is a great introduction then what is next how can i keep learning when i complete those courses in the future i want to save money for studying this career at another university in another country but meanwhile where should i learn and gain experience with data statistics and programming plus my country made an alliance with a mooc of latin america called platzi to give free courses in ai platzi says that with their courses you cand find a job but i am studying their courses and there are only videos the practice and challenges are all by my self i will finish all these courses on platzi because it is a great opportunity but where can i find more information to practice and become a person that can find a job with it and save money to study those areas professionally noe i know that my practice and portfolio is more important than the certifications so most of the mooc is are practically lying to their students when they say that they cand find jobs with their career tracks and courses thank you for reading all that thank you for your advice and sorry if my engish is not clear or if a have a lot of errors',\n",
              "  'title': 'confused and lost at where to start'},\n",
              " {'post': 'hi fellows im a yearsold senior from brazil and i need to decide either to enroll in an economics bs or a computer science bs i recently started to learn coding and decided i want to study data science in deep during undergraduate studies i havent yet decided wich field but been reading about analytics bi machine learning and deep learning i think that building a business foundation and learn coding by my own could give me valuable insights and job opportunities especially internships and trainee programs on the other hand im afraid that it would limit my possibilities in terms of knowledge and credential itself id be glad if you could share your experiences and advices with me',\n",
              "  'title': 'bs in economics or bs in computer science'},\n",
              " {'post': 'i am trying to feature scale my data and used the following code sc standardscaler x_train scfit_transformx_train ampxb i am getting the following error ampxb traceback most recent call last file ltstdingt line in ltmodulegt file homeanoushkajlocallibpythonsitepackagessklearnbasepy line in fit_transform return selffit fit_paramstransformx file homeanoushkajlocallibpythonsitepackagessklearnpreprocessing_datapy line in fit return selfpartial_fitx y file homeanoushkajlocallibpythonsitepackagessklearnpreprocessing_datapy line in partial_fit force_all_finiteallownan file homeanoushkajlocallibpythonsitepackagessklearnutilsvalidationpy line in check_array array npasarrayarray orderorder dtypedtype file usrlibpythondistpackagesnumpycorenumericpy line in asarray return arraya dtype copyfalse orderorder valueerror could not convert string to float us citizen gtgtgt x_test sctransformx_test traceback most recent call last file ltstdingt line in ltmodulegt file homeanoushkajlocallibpythonsitepackagessklearnpreprocessing_datapy line in transform check_is_fittedself file homeanoushkajlocallibpythonsitepackagessklearnutilsvalidationpy line in check_is_fitted raise notfittederrormsg name typeestimator__name__ sklearnexceptionsnotfittederror this standardscaler instance is not fitted yet call fit with appropriate arguments before using this estimator ampxb can anyone please explain me how to fix it this is my first predictive model so i do not have a lot of experience please help',\n",
              "  'title': 'why am i getting error while performing fit transform'},\n",
              " {'post': 'i have been working on a timeseries model using rnns and have seemed to have limited success i was hoping to get some help clarifying a few questions note that my model is not for prediction instead it is more like a recurrent variational autoencoder where i am trying to learn latent features of the sequence if i assume my data is a d sequence of length n n n how do i divide the data up into batches i know minibatches are typically of size batch_size seq_length num_features but do i split the data into minibatches using an overlapping sliding window approach or a nonoverlapping window eg minibatch one goes from does minibatch two go from or my gut feeling is that the former is more appropriate when you are using a nonrecurrent nn to model the data eg you will only evaluate the loss on the last element of the sequence as you need the rest of the data for context hence in order to model the entire sequence you need to ensure that every item of the sequence is input such that it is the last element whereas with an rnn as you have a state that you can feed in you are able to evaluate each element of the sequence and can therefore feed in the data much more efficiently in nonoverlapping blocks edit also note that i only have a single sequence of say length',\n",
              "  'title': 'a few questions on modelling time series data with recurrent neural networks'},\n",
              " {'post': 'hello im mainly using r for my scripts and benchling to take notes i want to know how to have an organized and reproducible workflow now i have a folder with scripts that have the next structure _functions_to_clean_datar _clean_datar _function_to_analyze_datar _analyze_datar but sometimes i have to make some corrections or add more data for that i need to rerun or change some scripts that i used earlier and take notes on benching this is messy since after that i cant run my scripts in order and have the same final results since i need data that i got for example in script to make the corrections in script it would better use r markdown or what should i do please excuse my english',\n",
              "  'title': 'how can i have an organized workflow in r'},\n",
              " {'post': 'im a university student majoring mathmatics in undergraduate school and ill go graduate school in math or statistics i want to be a data scientist that managing data making results now im studying programming such as r python and math linear regression real analysis i have questions the first is im thinking about mooc in data science and therere moocs one is data analyst in udacity second is data science specialization in coursera both has pros and cons i cannot decide which one is good for me you should think that im a student and ill go to graduate school the second question is beside mooc im not sure about what i have to studyshould i study computer science like algorithm graph theory or math statistics',\n",
              "  'title': 'moocs for university student'},\n",
              " {'post': 'i am trying to model a simple neural net to classify data amongst classes the data is quite high dimensional with rows and columns with the last column being the labels which have encoded into integral values for classification purposes i am referring to the proposed architecture in where it is used on iris data to get a conventional learning curve but my learning curve is coming to be something unusual which i have not seen before ampxb does my learning curve graph signify that the model performs poorly as it does not go down like a conventional one using the gradient descent optimizer or is it just some point i am missing any comments in this regard would be appreciated ps the accuracy i am getting based on the above model is close to thanks',\n",
              "  'title': 'can a learning rate graph look unusual and weird'},\n",
              " {'post': 'how do you manage different models over different datasets and repeated experiments for instance i hash the hyperparams and create a directory with that hash and put everything that belong to that experiment under that directory i run or runs with same hyperparams and take the average of those for that particular model and dataset this works well for a model a on dataset a but at times i find it difficult to compare experiments of different ie comparing model a and dataset a model a on dataset b or model b on dataset a currently i am trying to intregrate dvc for data versioning in my workflow but before jumping in with two legs i wanted to know how others do it',\n",
              "  'title': 'how do you manage multiple experiments in ml'},\n",
              " {'post': 'i checked my google analytics account after about a month i was surprised to see a significant uptick in pageviews and visits however i saw that many of most visited webpages are not something i have hosted for example the typical urls supported on my website are bmicalculator bodyfatpercentagecalculator and so one but the ones i seeing now are sharebuttonto compliancedonxyz so it seems that these new pages are almost some random urls when i try to access these on my site mysiteblahsharebuttonto i get but google analytics reports page views for this page over the last month what is happening and what should i do to avoid these situations is my site compromised',\n",
              "  'title': 'pages i am not hosting are showing in google analytics'},\n",
              " {'post': 'hi i have been working in data analytics for the retail arm of a bank about years now i have a keen interest in social sciencedevelopment econ grad and have been thinking of switching for some time now could not because of some personal reasons background econ grad skills include python sql tableau stata plus some research exp i have questions any advice on how i make the switch now if i want to move to an organisation like the gates foundation any social science datasets that i could work on to get my hands dirty and of course add in my resume this is not the stereotypical change in career plan question so any advicesuggestionscriticisms would be a big helpful',\n",
              "  'title': 'advice on switching career to development sector'},\n",
              " {'post': 'q blocks cloud affordable supercomputers for highperformance computing applications like ml model training running simulations big data analytics or creating the next deep fake powerful gpus cuda amp tensor core gpus for ai data science amp design lightningfast computing results costeffective x costeffective package gpu hours data security state of the art security standards dockerized volumes sha encryption salts gpu power launching a gpu powered the virtual computer on q blocks peer to peer computing platform is really easy here is a quick demo train ai models faster select a gpu instance select an ai framework of your choice select access method cli or jupyter lab get a supercomputer at your service one more thing for get gpu hours why qblocks x better breakthrough computing paradigm the community of people with a deep love for supercomputing crafted with love for the crazy ones what is q blocks imagine the uber of computing a way to use millions of underutilized sources of computing to build your next ai model gone are the days when supercomputers were just limited to scientific institutes or governments by connecting together these computing sources spread across the worldq blocks envisions bringing access to a supercomputer in your hands get access to free compute hours contact information facing any trouble in getting started send us a contact form message water street vancouver bc vb b canada ampxb about q blocks q blocks is building affordable supercomputers using peer to peer technology built by a team of scientists and engineers to help our tribe get superfast computing results read the vision for a brief background and the secret master plan',\n",
              "  'title': 'affordable supercomputers for ai data science amp design'},\n",
              " {'post': 'héllo all i am python and javascript and scheme developer looking for a project idea a few months or years back i started a project to help me with graphdb and conceptnet with the hope to put it to good use for doing some nlp stuff that was a graphdb embedded in python that is very easy to install or very easy to run similar to sqlite with a gremlin api anyway after some time fiddling around text summarization question answering wikification and reading about opencog i figured out that that nlp does not really need my project and most of the work done in data science evolves around crunching a lot of matrix operations best done in ram or with the help of gpus they are a few like opencog that do use relational model for both their kb and algorithms but i found no other examples so there is no need for my biggerthanram relational data store i diven into the world of datascience a bit deeper i discovered that one of the weaker point of the datascience work flow was wrangling basically some kind of extracttransformload or extractloadtransform if you prefer the tasks handled by google dataprep but that niche is already taken there is even free alternatives i would like to help the datascience community with some working software but i do not know what to do by the way would you find it useful to be able to version a graph and maintain multiple versions of it in the same manner that is done with source code with git',\n",
              "  'title': 'a python developer with too much free time needs your input'},\n",
              " {'post': 'st year comp sci undergrad and self teaching ml basics so will be prone to misinterpreting things ampxb i understand that a big problem with neural networks are that they are black box models it is difficult to understand how they achieve outcomes given certain input data this is an issue when nn models could effect the health and safety of individuals ampxb what i am wondering is that if we could describe a mathematical form of the function encoded by neural network models would it help researchers gain insight into how they work ampxb i guess that the mathematical form of the function would be just as cryptic as especially for deep learning models there are a lot of inputs but i have no idea which is why i am asking',\n",
              "  'title': 'would having a mathematically described function that approximates the function encoded in a neural network help researchers understand how a neural network gets its output'},\n",
              " {'post': 'two data scientists walk into a library at the end of a long day data scientist to the librarian can i get a copy of this book on statistical methods goes on to share the name of the obscure book data scientist to data scientist theyll never be able to find that book the librarian clacks away on the keyboard for a couple of seconds before replying found it here are the details of its author publishing house and borrowing history oh and someone left a comment saying they found it super useful for understanding logistic regressions i can grab it for you in a jiffy data scientist to data scientist ummmm why cant the same thing happen with our data that is what a data catalog is here for with the need of data and metadata management and collaboration data catalogs are increasingly becoming relevant read what are data catalogs and why should data teams care about them here',\n",
              "  'title': 'what is a data catalog and why should you even care'},\n",
              " {'post': 'i wonder if there is such a way to do this i have code that runs on google colab but even with gpu enabled it is training super slow so i am wondering if i can do that on gcp instead i can have an instance with a p gpu however it is such a pain to install cuda the drivers and everything that comes with it idk why but there is a gazillion ways to install them anyways it is a pain even using the public images like cdeeplearningtfentcuvubuntu after installing when when you try to do import tensorflow as tf printnum gpus available lentfconfigexperimentallist_physical_devicesgpu the output is is there a proper way to do this or just manually install everything also can someone verify that tensorflow runs gpu by default like no need to set it like witih keras or tf',\n",
              "  'title': 'how to get google colab environmentimage to gcp'},\n",
              " {'post': 'hi all i want to let you know about a project i had been working on called flashaiio which addresses some of the operational issues i came across when delivering models to clients or at the workplace i prefer spending my time building great models instead of thinking about the infrastructure complexities of hosting and serving them so i put together a service to do exactly that so if you want to enable clients colleagues or apps to send inference requests to your models flashai lets you do this via web requests serve your models without any hassle the workflow is straight forward train your model locally then upload your model file to flashaiio and then send inference requests to your model currently this service supports scikitlearn tensorflow and pytorch models try it out at flashaiio please let me know what you think and if you have any suggestions for other features',\n",
              "  'title': 'host and serve your scikitlearn tensorflow and pytorch models within minutes'},\n",
              " {'post': 'baidu has answered this question empirically but i do not have a good background in math so i do not understand the answer gt many studies theoretically predict that generalization error learning curves take a powerlaw form εm αmltsupgtβltsubgtgltsubgtltsupgt here ε is generalization error m is the number of samples in the training set α is a constant property of the problem and βltsubgtgltsubgt or is the scaling exponent that defines the steepness of the learning curvehow quickly a model family can learn from adding more training samplesltsupgtltsupgt unfortunately in real applications we find empirically that βg usually settles between and exponents that are unexplained by prior theoretical work here is the same text as a screenshot in case any of the math notation does not display properly for example for image classification on imagenet gt the top classification error exponent is βltsubgtgltsubgt on the other hand the exponent for top classification error is βltsubgtgltsubgt how can this be expressed for a nonmathy layperson for example how much improvement in accuracy results from a x increase in training data x x',\n",
              "  'title': 'in layperson is terms how much does deep learning performance scale with training examples'},\n",
              " {'post': 'honestly i would like to hear how the industry deals with these issues product line x needs a product npv evaluation and a pricing model while i have transactions on the aggregate product line x i do not have a reliable transaction data for product x in product line x i notify my boss who asks for a solution i propose that the servicing team input transaction data into the system when they service the product servicing team agrees to implement the new procedure into their practices to my boss and the cfo the servicing team does not carry out the process my boss knows and he has not been pushing for any follow up for months now i have moved onto other models and projects but as i run into these problems more and more often i find myself getting apathetic towards everything and any hope of interdepartmental solution this morning i just could not deal with the guy next cube over talking about this concert he went to last week for the th time and just went home there have been better opportunities but i just cannot quit on the work that i have been building for so long now',\n",
              "  'title': 'business process change implementation'},\n",
              " {'post': 'i recently started looking for new roles after years working for the same company and bit out of touch with the market i came across some job postings with title senior analyst data scientist or senior data scientist analyst either it should be senior analyst or data scientist or just senior data scientist the job description lists everything a ds should know mode building stats experimentation cloud etc i recently interviewed for one of those rolesfortune non tech but the tc they offered seemed more like a normal senior analyst kind role around k base for a mcol city i was assuming for a ds role with yoe would be around k k base am i targeting the wrong kind of jobs or its the normal compensation for ds right now',\n",
              "  'title': 'what exactly is senior analyst data scientist role i am confused with responsibilities and corresponding tc'},\n",
              " {'post': 'i am looking for online resources for this type of issue i am facing but the problem is i do not know what this issue is called or if it even has a name so i have a movie dataset each row observation is a unique movie and one of the columns is a string list of genres you have stuff like scifi adventure zombie war scifi etc this list is not always the same lengths for each row using the way i am taught in my classes is to take all the genres mentioned in the whole column and just onehotencode them but what i am worried is that this will just create a large amount of columns and new movies can always add new genres i was wondering about grouping up the genres in a more workable amount i could use pca on the fully onehot encoded genre columns set but pca creates unreadable columns and it does not tell you what genres were grouped into which i would like to have scifi timetravel together or pirate historical together and since the lists are so arbitrarily organized if a new batch of data with a new genre comes in the pca would have to be redone right i want to see some online resources for different ways of dealing with this than just pca please note my example is arbitrary it could also be a clothing dataset with lists like jacket denim loosefitting etc',\n",
              "  'title': 'dealing with when observations have a variable list of traits'},\n",
              " {'post': 'need help creating a data set ill be using analogies below to help avoid industry specific jargon usage i have a group of friends who all have different food preferences and budgets min and max theyd pay for dinner as well as food they like and food they dislike i need to create a dataset for their preferences so that if on a given night i say id like to go get italian and pay i would be able to pivot the data and see whose budget im within and who likes italian food and who dislikes italians food the problem is i cant figure out how to organize the preference section of the data set while min and max price are easily added as a single cell in a spread sheet for each person some of the individuals like foods and dislike what is the best way to organize the data so i can easily analyze the data set to figure out which individuals i would and would not like to ask to go to dinner with any videos or help would be greatly appreciated thanks for any help that comes my way',\n",
              "  'title': 'creating a data set from scratch having trouble'},\n",
              " {'post': 'hi all hope this is the right place to ask i am yo and have a background in computer science bsc and software engineering msc currently i am working as a software engineer but am not really happy with the work i am doing as it does not feel very challenging i have been looking at more math and statistics including ml related topics and feel like this would be very interesting for me to dive into is there anyone that went through a similar situation i feel like it might be a bit late for me to still start on this as i have already finished my studies what do you think are topics an in which order i should jump into to get the knowledge i need i have some basic knowledge on calculus linear algebra and statistical methods but this all needs to be refreshed any recommendations on how to approach this possible resources to read would also be very welcome i also like working on projects to get more of a feel on it any ideas for small problemsapplications i could work on to put the things i learned into practice',\n",
              "  'title': 'advice for a beginner'},\n",
              " {'post': 'activation functions might seem to be a very small component in the grand scheme of hundreds of layers and millions of parameters in deep neural networks yet their importance is paramount activation functions not only help with training by introducing nonlinearity but they also help with network optimization in this article we will explore the paper by google brain titled searching for activation functions the paper proposes a novel activation function called swish which was discovered using a neural architecture search nas approach and showed significant improvement in performance compared to standard activation functions like relu or leaky relu we will first take a look at the motivation behind the paper followed by a dissection of the structure of swish and its similarities to silu sigmoid weighted linear unit we will then go through the results when swish is applied to several nlp tasks along with the pytorch code to train your own deep neural networks with swish topics covered include motivation swish explained pytorch code notable results conclusion reference article link',\n",
              "  'title': 'article the swish activation function'},\n",
              " {'post': 'i am currently in a dual major for computer information systems and data analytics at my state university i am highly feeling that i am not learning a lot in my data analytics courses and want to try to do something through my work that would be valuable to the company while being able to learn more on my own my intended idea was to understand customer attrition by creating first creating a customer ranking system based on things they do in my company is program they use for example for a fleet maintenance program a customer may be scored on the number of work orders they create their parts purchase orders etc these would have some type of weight attached to them that would have to be decided essentially i would use these scores to predict which customers are at risk of leaving us is this something that is possible using ml and what advice would you have for me our customer data is in an aws environment',\n",
              "  'title': 'creating a customer attrition model for my work'},\n",
              " {'post': 'yesterday i asked a few people about creating a program that can automatically categorize reddit submission into appropriate subreddit using title text the first version is ready and it works a lot of people helped me yesterday and thanks to comments by udeltasheep and uolbaa i was able to create it it is very bad code right now since i just learned python programming and my knowledge of ml is very limited mainly sentdex videos but i feel super happy that i was able to create this program here is how i did it please bear with me because i know i am a total noob first run this google big query select subreddit title from fhbigqueryreddit_posts_ fhbigqueryreddit_posts_ fhbigqueryreddit_posts_ fhbigqueryreddit_posts_ fhbigqueryreddit_posts_ fhbigqueryreddit_posts_ fhbigqueryreddit_posts_ where subreddit in programming business wouldesign entertainment iscience isecurity worldnews politics amobile istartups google amicrosoft bitcoin facebook amazon amovies gadgets notechnology linux gaming apple wouldesign amusic then export the results into a table and export it again to a csv file to your google storage bucket then install google data labs and create a notebook then use the following program to run it i have still got a lot to learn and my next challenge is to create a small webpage so that people can enter the title in a web form and it shows the correct subreddit using it',\n",
              "  'title': 'p automatic reddit categorizer update first version working'},\n",
              " {'post': 'i recently signed an offer to start working as a data scientist after the summer i am extremely excited as the role is known for being ml stats data engineering heavy dashboarding type projects will not be part of the role as they fall under the other teams my background is masters level noncs so i have not formally learnt data structures and algorithms i know the basics of the most important data structures lists sets hashmaps queues contigous data structures etc and intuitions about their corresponding timespace complexity they were needed for some advanced courses eg combinatorial optimisation w c together with some sorting search algorithms binary search dfs bfs but here typically only what was needed was covered for now until i graduate i am looking at areas i can improve before i start working aside from dsampa i have also identified extra cloud skills i have entry level azure certs already but i can go for more cicd testing frameworks and webapi skills for model deployment i do kind of reason dsampa while writing code but maybe actually formally learning about them would make me a far better programmer should i potentially just try and wing it with my current knowledge what do you think are they really that important aside from leetcode you get on interviews which i no longer have to do in the past i have always avoided selflearning these as i believed my time was better spent on extra math stat than on dsampa',\n",
              "  'title': 'is data structures and algorithms worth spending my time on before i start my job'},\n",
              " {'post': 'hello all i am a marketing analyst with a large company that has many brands i am their first analyst so i have been responsible for a lot of data wrangling across all of their different marketing channels along with our new digital marketing manager and have been responsible for doing all of their campaign reporting after spending a couple months doing manual exports from each marketing tool we use while getting my feet under me i am finally sick of having to log in and pull data from different marketing tools in an effort to get all of our marketing data into one place i have been working with a developer to build a database on amazon redshift to take in our data from a a variety of sources like pardot and google analytics to finally make reporting easier i am running into one sticking point though what tool can i use to get all of my social media data into one place to import into my database we were thinking about using hootsuite but it does not appear to have a reporting api is anyone aware of a similar product that does i really want to avoid having to build out an individual connection for each brand is linkedin facebook and twitter profile i guess sprout social has a reporting api does anyone have any experience with that i am pretty new to marketing analytics but have been working as an analyst in market research and revenue management for years i am well versed in salesforce google analytics sql powerbi and excel and have been working withlearning python for the last half year ampxb thanks all',\n",
              "  'title': 'social media management platform with reporting api'},\n",
              " {'post': 'hello i trying to figure out how the kernel trick gives rise to a decision boundary in my particular case i am looking at string kernels where strings are classified i get that kernelization helps to make decision boundaries for nonlinearly seperably data but whenever they are explaining they usually only seem to show how the dot product give you a similarity score between two data points not how it partitions a set of data into two categories for example in this above link the lili commentator simply gives an example of what looks like two datapoints in d space x and y and calculates the dot product between them but i do not see an explanation about how this product can be used to determine similarity between the two points or how it can be extended into a decision boundary between two groups of points does someone have some insight in regards to this',\n",
              "  'title': 'how do kernels in svms give rise to decision boundaries'},\n",
              " {'post': 'hello everyone going to try to keep it as brief as possible to start a bit about my background i graduated with a bs in petroleum eng and did well in school been more than a year and still having a difficult time landing an engineering position without any experience ive been recommended by many people to start a masters program but im having a troubling time deciding what to pursue some have told me to go into ms data science while i continue to search for an opportunity few others have recommended to pursue masters in mechanicalchemical which would open more engineering opportunities now there is quite a few people who have recommended ms in data science and i want to possibly enroll in one of these programs preferably an online program oil and gas is moving rapidly into automation and relying heavily on data science given my undergrad degree directly related to oil and gas i think a ms data science could complement it it would make me more competitive while also allowing me to move into another industry should the oil industry go to shit there is also the possibility of pursuing a masters in chem eng degree and just learning data science on the side through certifications i am having a trouble time determining which route i should pursue if you could share some suggestions or insights on this matter id really appreciate it thank you all in advance ampxb tldr bs in engineering grad but cannot find an opportunity should i pursue ms in data science maybe an online program or ms in chemical engineering while getting data science certs on the side',\n",
              "  'title': 'looking for post undergrad advice'},\n",
              " {'post': 'q what kind of industry job can i get with interest in text analysis what should i learn more ampxb i have background in graduate studies in social sciences so i have background in using statistical analysis for data analysis but i do not have background in big data analysis nor adept at using statistical software to process big data ampxb current skills r and python i cannot say i am very fluent but i have studied it for a while so i am used to the software for python i am currently more interestedlearning the language to conduct text parsing and analysis ampxb experience i have graduate level research experience where i extracted government recordsused text analysis to gain qualitative information ampxb would a data science certificate necessary to enter the industry are text analysis jobs all tied to big datatext mining if this is so i am considering one year program in big data analysis',\n",
              "  'title': 'what kind of industry job can i get with interest in text analysis'},\n",
              " {'post': 'using normal linear regression methods recommend items users have weights assigned as per the movies they like movies have weights assigned as per genre among many other features can train bothways predict weight for movies or users and get a combined cost function predicting features here as well hence called collaborative filtering also there may be unknown values as all users have not rated all movies so changes in cost function is eminent accordingly train model to predict weights of users movies and output to compare with is the user ratings to account for new users normalise the data by taking means and hence also transform the output so each new user has mean as default rating rather than which would not help to train the model once trained among several things model can be used to predict rating for other movies and suggest the one which is most familiar to the ones user liked',\n",
              "  'title': 'day recommender systems'},\n",
              " {'post': 'hi all very new to the data science world and was wondering if someone could point me in a direction any direction the problem i am trying to solve is i have a market place at practically every instancepoint in time my supply outweighs my demand only a portion of my supply is sellable the portion of my supply that is sellable can also go back into the supply pool similar to how a consultant can take on multiple projects at once for every isupply there is an ideal set of matches on the wouldemand side what are some basic models that can help me manage my isupply so that they can always find their ideal wouldemand counterpart i am not looking for an answer but maybe a few models i should explore eg is this a multiple regression problem should i do tfidf and then match using something else etc thanks',\n",
              "  'title': 'best models for managing inventory supplydemand'},\n",
              " {'post': 'hi all like many i would like to enter into data science field but i am in s so i understand i am in the danger zone with respect to career development i read upon few articles on what to learn but it is confusing there is no clear curriculum if you want to do it without enrolling into school i have intermediate programming skills nothing fancy i have high school math skills excluding calculus i knew it back in the day but not anymore i think i can understand logic decently ok i have time and i can put in effort so all you wise data scientist people kindly tell me what to learn math programming to get started as a beginner data scientist if you include resources to learn from as well that would be awesome many many thanks',\n",
              "  'title': 'curriculum to get started'},\n",
              " {'post': 'i am a sole data analyst at a startup and there are many days when i start my day without knowing what i am necessarily going to be doing usually a bug in the data is reported to me and so i spend the days fixing that or someone asks for a report adhoc and i make it for them but as i have improved the infrastructure now there is not as much bugs popping up and i have automated many reports so i do not get asked to do as many anymore and i am trying to find new tasks to do so my question is what is your routine like as a data analyst say you want to be forwardlooking and plan for the next week or month how do you decide on a goal and roadmap',\n",
              "  'title': 'what is your routine like as a data analyst'},\n",
              " {'post': 'i am not a data scientist but i feel like you all could help me out i am trying to collect data for all firstlevel administrative divisions for all countries and i am having a hard time i started with this web page and just scraped all the data there however the formatting is such that i would have to manually go through to separate all of the data into the correct divisions i was thinking about getting data from this wikipedia page but the data is so dispersed that i would invest more time into engineering the data collection than it would take to manually separate the first source is data ampxb where can i just get a dataset of all countries and their firstlevel divisions ie just state and not counties cities etc',\n",
              "  'title': 'get data for all firstlevel administrative divisions for all countries'},\n",
              " {'post': 'hello ampxb currently we are using logistic regression as statistical method that predicts if loan taken by the customer will be bad or good depending on the probability of default and cutoff point that we have set the problem is that the current model is a bit biased towards some type of clients and i want to rework it from scratch since previous model have been working for a long time all sampled data is already passed through the prism of the model so i basically only see approved customers that may be less than of all customers the question is what should i do if i will make my new model based on only of data that came trough the prism of the model i will face problems several problems relative low default rate rare events predictors that were included in previos model and were really strong wont affect current sample and wont be included new potentially weaker predictors will be included in the model but old predictors from previos model wont so it may lead to worse model in general that only affects my sample of data etc what is industry best practice where should i get all the data if using sampled data is not possible because of a problem i described above thank you in advance',\n",
              "  'title': 'logistic regression for predicting goodbad customers scoring model development'},\n",
              " {'post': 'so i did not want to be too specific as the above is my general framework but i am not really a web developer and would call myself a data analyst at the moment not a scientist yet however i am happy to provide more details if that would help in your responses i am basically wondering what the architecture of this would look like and if there are any resourcesinfographics as to how to get started specifically step b would that be a mysql database to house the data in the interim before i manipulate it what are some good resources for the interaction of front and back end web design gosh i hope all that makes sense i am happy to clarify and thank you all in advance for your time',\n",
              "  'title': 'tapping this sub for some resources i am looking to a tap into some api feeds b do stuff with the resulting data graph manipulate etc c deliver it in the form of a data product web mobile etc'},\n",
              " {'post': 'hi i am new to machine learning so any help would be greatly appreciated ampxb i am trying to build a basic model that detects anomalies by comparing a rate value against max and min thresholds and classifying them appropriately i have features in my dataset rate of change max threshold min threshold i tried using decision tree and random forest algorithms and they all seem to be going by only the rate of change feature when i look at the feature_importance it shows up as so it doesnt look like it even considers max and min thresholds is it possible to tweak the feature priorities i would prefer the model to look at the threshold values as well is there a way to boostmodify feature selectionimportance',\n",
              "  'title': 'question about feature importanceselection'},\n",
              " {'post': 'could anyone help with when i use the pretrained model for englishtogerman translation at why am i getting some random translation output phungarchlinux opennmtpy python translatepy model available_modelstransformerendewmtpyonmtaveragedepochpt src datasrctesttxt output predtxt replace_unk verbose sent orlando bloom and amiranda kerr istill love each other pred nein viel leicht nicht pred score sent actors orlando bloom and amodel amiranda kerr want noto go notheir iseparate ways pred seh r interessant und interessant pred score sent however in an interview bloom has isaid nothat he and kerr istill love each other pred seh r interessant ist auch die tatsache dass das ganze noch nicht vollständig umgesetzt wurde pred score sent amiranda kerr and orlando bloom are parents noto notwoyearold flynn pred seh r interessant und interessant pred score',\n",
              "  'title': 'random translation output with pretrained opennmtpy model'},\n",
              " {'post': 'i am trying to predict a time series of a discrete number of amplitudes from continuous curves of the same time resolution i have a feature extractor that comes close and just rounding it works pretty well but sometimes it predicts the wrong label roughly of the time however the feature extractor also has an additional couple of hundreds of meaningful time series builtin so i was thinking i could augment the rounding heuristic with them to improve the predictions but how would i go about combining them though i also have a decent set of supervised examples of how the time series should turn out example the red line is rounded from the blue line and should therefore follow the green line completely but that does not always work because of noise in the blue line',\n",
              "  'title': 'how do i combine several time series into one given a dataset of how the end result should be'},\n",
              " {'post': 'your isp internet service provider can see each individual website that you view and in some cases can actually see the data and parts of the website that you are viewing recently in the uk they passed a law requiring isps to keep every individuals search history for years and this is happening in other countries when there is not even a legal requirement you can stop your isp from breaching your privacy using a few different methods the first of which is using a vpn this costs a monthly subscription fee however this not only protects your browsing history but can also be used to torrent privately without any risks or to protect your ip address from malicious people online nordvpn is one of the largest vpn companies however many other providers can be found online the second method is using the brave browser it is similar to chromefirefox but it has a built in adblocked and built in tor support which hides what websites your are visiting from your isp you simply download the browser and click the settings in the top right and click new private window with tor this is my preferred method as it not only protects your privacy but also blocks ads while you are browsing',\n",
              "  'title': 'do not allow your isp and government to save your browsing history'},\n",
              " {'post': 'i am trying to do the below linear regression in tensorflow but my output is all inf and nans my input dataset has to be yx noise where x is a normal distribution of size and noise is gaussian with mu and sigma output loss w b loss w b loss e w b loss e w e b loss inf w e b e loss inf w e b e loss inf w e b e loss inf w e b e loss inf w inf b inf loss inf w nan b nan loss nan w nan b nan loss nan w nan b nan loss nan w nan b nan import tensorflow as tf from sklearnmodel_selection import train_test_split import numpy as np import pandas as pd from matplotlib import pyplot as plt noisenprandomnormalastypenpfloat x_datanprandomuniformastypenpfloat y_datax_datanoiseastypenpfloat pltscatterx_datay_datas pltshow xtfplaceholdershapedtypetffloat ytfplaceholdershapedtypetffloat learning w and b over the epochs wtfget_variablenameweightdtypetffloatshapeinitializertfzeros_initializer btfget_variablenamebiasdtypetffloatshapeinitializertfzeros_initializer y_pred tfaddtfmultiplyx wb loss tfreduce_meantfsquarey_pred y optimizertftraingradientdescentoptimizerlearning_rateminimizeloss epochs with tfsession as sess inittfglobal_variables_initializer sessruninit for e in rangeepochs _csessrunoptimizerlossfeed_dictx x_datay y_data printlosscwsessrunwbsessrunb pltscatterx_data y_data ro labeloriginal data pltplotx_data sessrunw x_data sessrunb labelfitted line pltlegend pltshow',\n",
              "  'title': 'linear regression on tensorflow all nans'},\n",
              " {'post': 'i built a cnn to classify different classes it performs well on most of the classes giving approx accu per class current each class has images but in the future there is a possibility that i might get more data for each class for instance i get more data for some class how should i retrain the model should i retrain the entire thing ie with old and new data should i retrain the model with only new data here i fear that as the model will get new data for a single class the model can possibly forget what it has already learned or might affect the accuracies of other classes if anyone has worked on this problem before please help',\n",
              "  'title': 'retraining cnn with new data'},\n",
              " {'post': 'i am graduating with a masters in management and will be spending the next few months learning some technical data analytics skills i have signed up for a month bootcamp that will teach me data science through the pandas library in python starting from a beginner coding level however looking at the jobs i will be applying to business intelligence most jobs list that they will love it if i know python but they expect me to operate in sql or in some cases r i have only done a small hour course in sql to learn the basics i am curious to know given i will spend the next three months learning data analysis through the pandas library how hard will it be to pick up sql later are the skills very transferrable apologies if this is a sillyobvious question',\n",
              "  'title': 'question about learning sql after learning data analysisbased python pandas library'},\n",
              " {'post': 'hi everyone to start off i am not exactly that good at selflearning entire fields of subjects i taught myself some python and sas sas was more learned on the job tbh but never really to the point of doing my own data science projects okay so i am a year old recent graduate in economics as of last year and i have just started a parttime msc in econ but with a specialization in data science and numerical methods basically a lot of econometrics and classes on mlbig data manipulationnlp it would take me years to complete the program i have also just begun a job as an economist at the government my previous job was waaaaay off course the job will involve a lot of sas programming and producing statistics on international trade data as i have been told naturally i would love to get a job that is more in line with dsml out of school so i was wondering if acquiring experience as an economist would be relevant when i will be applying for ds jobs right now i have no experience with ml and like i said selflearning is not exactly my forte so i figured that taking courses related to the field would provide with a solid springboard towards acquiring all the necessary skills plus i will be able to use the projects done in school for future interviews am i missing something has anyone else done a similar path from economist to data scientist for example am i wasting my time and will my experience be relevant for that future career switch thank you',\n",
              "  'title': 'economist that just started a msc in econ specializing in data science and numerical methods should i just drop it and focus on selflearning everything'},\n",
              " {'post': 'hi there i have been wondering about this for a while without focusing too much on the specific algorithm used are there practical applications of synthetic datasets i know deep learning in general needs vast amounts of samples that are not always easy to find so what if there could be a tool that can generate high quality examples be it images sounds texts or videos for example single letters or group of words on all kinds of backgrounds maybe animated d rendered objects and multiple scenarios angle lights words pronounced on different background noises and so on and so forth of course the trained deep neural network would then be finally trained on a realworld dataset what do you think',\n",
              "  'title': 'do you think generated datasets when realworld ones are scarce can help training and if so who could find them useful'},\n",
              " {'post': 'i am trying several different word vectorizations for a binary classification problem the series x consists of lemmatized spacy docs row per review using x and y ie the ratings i used traintest split for what i believe is the correct setup for sklearn is tfidf vectorizer ampxb tfidf vectors x_train_tfidf x_trainmaplambda x strx x_test_tfidf x_testmaplambda x strx ampxb cv tfidfvectorizer ampxb x_train_vec cvfit_transformx_train_tfidf x_test_vec cvtransformx_test_tfidf ampxb and this goes into whatever classification algorithm i am using i am a bit more confused on the general flow that should be done using spacy is own word embeddings this is what i originally did spacy vectors x ispacy_vecs xapplylambda x xvector spacy_array nparraylistx ispacy_vecs dtype npfloat ampxb clf_lr_spacy logisticregression ampxb spacy_clf clf_lr_spacyfitspacy_array y spacy_pred spacy_clfpredictspacy_array ampxb it works but the overall method does not seem sound to me that is that i am not withholding any data as a test set at the same time i do not believe there is a fit or transform for the spacy vectors so i thought it made more sense to train the vectors on the entire corpus ampxb my question concerns the methodology here have i set up the spacy or tfidf for that matter vectors correctly is it proper to get all the embeddings and then split the data',\n",
              "  'title': 'spacy word vectors and sklearn'},\n",
              " {'post': 'hi there ampxb i have recently been going through tutorials on creating multitouch markov attribution models in r here ampxb from this i have created a set of users about that have a path to whether they converted or not some users may even have multiple conversions for example user a gt b gt c gt a gt conversion user a gt a gt c user c gt b gt a gt a gt conversion user a gt b gt c gt gt c these users have followed different paths with touch points in various marketing channels a b c to either converting or potentially dropping from their search i also created the time between each touchpoint so for example my dataset has user a lt b lt c lt a lt conversion lt i have been trying to find a source on lead scoring models in machine learning but have not really had any luck i have been thinking of using the transition matrix from that the number of days in the path and the number of touch points to assign a sort of lead score but it all seems sort of arbitrary is there some way to analyze the entire paths of every user and predict how close they are to converting let is say a conversion is i do not really have labels for nonconverting users since when they do not convert their just sort of in an indeterminate state can anyone point me in some sort of direction on this i am a bit lost thanks',\n",
              "  'title': 'lead scoring model for marketing'},\n",
              " {'post': 'hello i am in bit of a situation right now i have an interview coming up for a data analyst role hiring manager reached out to me for the phone interview and it went well after i cleared the phone interview recruiter started taking care of the process i have asked the recruiter atleast times what to expect in the coding round she has been avoiding that question the interview date has not been set up either i gave thursday as my availability and they said we will send you the invite by monday they said something like this before as well but did not send anything now i have been preparing sql really well and i know how to work with dataframe using rpython i do not know how to do any leetcode type python questions since the recruiter has not been responding should i email the hiring manager on monday about what to expect although i will not learn something in days that i do not have any knowledge of but it will be good to know what to expect so that i am not shocked any help would be really appreciated i am a little stressed right now because it seems like a really good opportunity thanks',\n",
              "  'title': 'is it okay to email hiring manager if the recruiter is not responding'},\n",
              " {'post': 'hey there i am messing around with google data studio for work but i have stumbled upon a problem and i am not sure how to solve it we have many campaigns all in two languages dutch and french i tried making a filter so that the person viewing the report can select to view only the data of french campaigns or only the dutch but i have not found a way to do this the way i want to ideally i would make two custom dimensions one for the nl campaigns and one for fr i tried to look into it but the formula confuses me gt_lt does anyone knows what formula to use for this i know it has to be something with contains fr but nothing seems to work for me p',\n",
              "  'title': 'google data studio create language button'},\n",
              " {'post': 'greetings background i am not from a data science background i am a phd in humanities who stumbled on text analytics then quickly fall in love with it with little knowledge my research is now heavily based on topic models using the stm package in r my supervisor has no idea what am i doing since she knows nothing about data science and thought i am a normal humanities kid when she accepted me when i explore the field of topic modeling i found lots of interesting topic models i would like to try such as hdp keywordassisted tm embedded tm etc however there was little choice in r even python seems to only relying on gensim which certainly do not contain all the model so i am curious why does the r community seem to have little support for topic modeling furthermore is there a way to use the model that has not been developed into packages in my analysis i am learning python do not just tell me to switch to python xd',\n",
              "  'title': 'newbie why so little machine learning model package in r'},\n",
              " {'post': 'hi everyone i have put a lot of work into getting a deeplabv model trained and it is working well but the output sometimes misses sections of the object and i am trying to figure out how i can postprocess the segmentation masks to improve the segmentation result my use case is industrial but the image below illustrates the issue where the segmentation mask misses parts of the object circled in red i tried grabcut using the segmentation mask as marks to improve the segmentation result but i could not get it working my thinking is that i should be able to do something like grow the mask to nearby pixels that are similar does anybody have any ideas or advice on how i can postprocess my segmentation results to make them better thanks',\n",
              "  'title': 'need help postprocessing image segmentation masks'},\n",
              " {'post': 'i am using the following keras implementation of the wavenet architecture ampxb when my input shape is x y with z number of filters in all types of convolutional layers d casual the output is of shape x z i would like my output to be of shape x and so i changed the number of filters for the very last convd layer to be and got my desired output shape ampxb if numbers help more than variables input number of filters output desired output ampxb my fear is that in doing so i am losing learned information is this true is there another way to get the desired output ampxb any ideas will be much appreciated thanks in advance',\n",
              "  'title': 'wavenet output dimensions'},\n",
              " {'post': 'hello i am a physics teacher who is been looking for a career change for some time now and i have settled on data science i am wondering if there is anyone here who is made a career change into the field that could possibly help me out bonus points if you also came from teaching haha my main issue is i am not sure how to build up practical experience and demonstrate it to a potential employer i am currently working through dataquest and after i have completed some of that i was planning on trying to build up a portfolio of projects but from there i really do not have a plan and if i am being honest i am not even sure that is the best way to develop my skills i am also looking to know your story of how you landed your first data science job how did you do it did you find your job through a connection in any case how did you demonstrate your competence with no experience in the field looking forward to hearing people is responses thanks either way',\n",
              "  'title': 'has anyone here made a career change into data science what was your experience like'},\n",
              " {'post': 'hello i am currently doing my capstone project for my postsecondary data science certificate the project which is based on modelling virus outbreaks in north america is due by april the problem for me is that i am currently working full time and i get a limited minute in person window to see my supervisor each week for project clarificationsuggestions i do not really know which data science tools best describe modeling my problem since for my certificate program i have been taking gaps in between classes due to the fact i was doing my undergrad as well as doing the certificate program ampxb by taking certificate classes one at a time instead of altogether it has become difficult for me to refresh my knowledge on the subject since a lot of the relevant coursework i have done were a long time ago i have been making an attempt to go back to my coursework assignmentslabslectures to help me with the project as well as going over modules on linkedinlearning but i feel like i need more direction with the limited interaction i get from my supervisor i would like to know if there are any resources online that can help me in regards to my capstone',\n",
              "  'title': 'need data science project supervisionresources'},\n",
              " {'post': 'hey everyone i have a background in linguistics and i am quite new to the field of machine learning there is something that has been bugging me for the past few days that i hope you can help me get a better understading of namely the difference between an algorithm and a model in his blog jason brownlee says that he likes to think of the model as the specific representation learned from data and the algorithm as the process for learning it however i struggle to grasp the difference between the two as well as which one comes first let is consider this snippet of code model decisiontreeclassifier modelfitx_train y_train in the first line i instantiate a specific model in the second one i train it however is the decision tree created by the algorithm or does it already exist as some sort of blueprint for the algorithm to operate on i apologize for the lack of clarity',\n",
              "  'title': 'difference between model and algorithm'},\n",
              " {'post': 'hello my name is simon wright i need your help to create an ai to play eve online there are bots that can play but all they do is mine asteroids which is boring and ruins the game is economy i am years old but i want to learn i have a step plan step have bot watch streams of the game to learn step have bot learn to farm the npc enemies until it gets good step have it participate in real pvp battles this is an experiment mabey when it gets good a bunch of getting together to create an all ai fleet edit is eve online to complacated for reinforcement learning',\n",
              "  'title': 'eve ai'},\n",
              " {'post': 'firstly im in the uk unsure if that makes a difference im currently studying a joint economics and finance degree exact same number of econometrics modules as pure econ how viable is a career as a data scientist for me and what would you recommend i do in my free time to flesh myself out im currently in my second year and currently studying dynamic regressions with timeseries data only the second week of the module so a lot more to come first semester covered ols multiple linear regression gaussmarkov assumptions and some tests for heteroskedasticity however its spelled i achieved around in this module for the highest grade in the uk university system and econometricsstats have been by far my strongest modules so far im comfortable with multivariate calculus and have done enough constrained optimisation that every time i close my eyes i see a lagrangian my course doesnt teach r and instead uses stata and eviews which i think theyre going to phase out within a couple years but im currently working through a book to learn it once ive done this im gonna have a go at an extensive project that i can do over time using r would i possibly be looked over what else could i do to strengthen my chances im already planning on taking every econometrics module i can which should be about half in my final year',\n",
              "  'title': 'entering data science with economicseconometricsfinance background'},\n",
              " {'post': 'suppose i have an amazon product name dataset comprised of smartphone name and their accessories such as iphone x gb oppo a black airpods pro phone case iphone x clear huawei p pro tempered glass protection cheap softcase for oneplus t blue black two tone oneplus t iphone pro max best case design when a buyer checks out with a phone i would want to recommend its accessories so when the buyer puts in an iphone x gb to their basket i would want the recommendation engine to recommend airpods pro and phone case iphone x clear but not the cheap softcase for oneplus t because it is for another phone and preferrably not the case for iphone too as it would be incompatible is there a way to solve this i have been thinking to use some sort of nlp to process the various product name especially for the accessories as they can be varied but i am puzzled at how to really implement it any suggestions',\n",
              "  'title': 'gadget accessories recommender system need some ideas amp questions to answer'}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Preprocessor and Data Generator"
      ],
      "metadata": {
        "id": "XUuyIqOWn6Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(pairs, tokenizer, model, max_length=128):\n",
        "    post_text = [post for post, title in pairs]\n",
        "    post_encoded = tokenizer.batch_encode_plus(\n",
        "        post_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    post_input_ids = np.array(post_encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    post_attention_masks = np.array(post_encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "\n",
        "    title_text = [title for post, title in pairs]\n",
        "    title_encoded = tokenizer.batch_encode_plus(\n",
        "        title_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    label_ids = np.array(title_encoded['input_ids'])\n",
        "    decoder_input_ids = model._shift_right(label_ids)\n",
        "\n",
        "    return [post_input_ids, post_attention_masks, decoder_input_ids], label_ids"
      ],
      "metadata": {
        "id": "-U-4_qECmBUF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummarizeDataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 n_examples,\n",
        "                 data_filename,\n",
        "                 max_length=128,\n",
        "                 batch_size=16,\n",
        "                 shuffle=True):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.n_examples = n_examples\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples+1)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches in the full dataset\n",
        "        return self.n_examples // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = (idx + 1) * self.batch_size\n",
        "\n",
        "        # Indices to skip are the ones in the shuffled row_order before and\n",
        "        # after the chunk we'll use for this batch\n",
        "        batch_idx_skip = self.row_order[:batch_start] + self.row_order[batch_end:]\n",
        "        df = pd.read_csv(self.data_filename, skiprows=batch_idx_skip)\n",
        "\n",
        "        pairs = df[['post', 'title']].values.astype(str).tolist()\n",
        "\n",
        "        batch_data = preprocess_data(\n",
        "            pairs,\n",
        "            self.tokenizer,\n",
        "            self.model,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        return batch_data\n",
        "\n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "\n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = list(np.random.permutation(self.row_order))"
      ],
      "metadata": {
        "id": "6SQJ90QtiEX6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Pretrained model\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CuXoHBp_pcMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained tensorflow model\n",
        "\n",
        "model_name = 't5-base'\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model = TFT5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "4sYMoGc1pWTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921450af-dd0e-4ff9-afd2-ff2a67b3f739"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data generators for train and validation data, tensorflow version\n",
        "\n",
        "max_length = 64\n",
        "batch_size = 16\n",
        "\n",
        "train_data_generator = SummarizeDataGenerator(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    model=t5_model,\n",
        "    n_examples=len(train_pairs),\n",
        "    data_filename=train_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_data_generator = SummarizeDataGenerator(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    model=t5_model,\n",
        "    n_examples=len(valid_pairs),\n",
        "    data_filename=valid_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "Ma1qh8u5iN1X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_t5_training_wrapper_model(t5_model, max_length, learning_rate=0.00005):\n",
        "    input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(max_length), dtype=tf.int32, name='attention_mask')\n",
        "    decoder_input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='labels')\n",
        "\n",
        "    t5_logits = t5_model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)[0]\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask, decoder_input_ids],\n",
        "                                  outputs=[t5_logits])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "a6B0Ey-jiRXF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_wrapper = build_t5_training_wrapper_model(t5_model, max_length, learning_rate=0.00005)"
      ],
      "metadata": {
        "id": "NzLTJ38TYOa4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add a model checkpoint callback to save\n",
        "# the trained model weights after each epoch.\n",
        "\n",
        "checkpoint_dir = 'drive/My Drive/W266/Final Project/model_checkpoints/'\n",
        "checkpoint_filepath = checkpoint_dir + 't5_reddit_weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True)\n"
      ],
      "metadata": {
        "id": "nowLPb1oYQ7v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now call .fit on the model_wrapper, passing in the data generators and the\n",
        "# model checkpoint callback\n",
        "\n",
        "model_wrapper.fit(train_data_generator,\n",
        "                  validation_data=valid_data_generator,\n",
        "                  epochs=3,\n",
        "                  callbacks=[model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7qSlAhNywpU",
        "outputId": "1220b4c4-c618-4c9b-bcb9-d98b8c3f4f67"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1250/1250 [==============================] - 901s 659ms/step - loss: 0.7471 - accuracy: 0.8696 - val_loss: 0.6486 - val_accuracy: 0.8792\n",
            "Epoch 2/3\n",
            "1250/1250 [==============================] - 815s 652ms/step - loss: 0.6195 - accuracy: 0.8820 - val_loss: 0.6449 - val_accuracy: 0.8800\n",
            "Epoch 3/3\n",
            "1250/1250 [==============================] - 823s 658ms/step - loss: 0.5479 - accuracy: 0.8906 - val_loss: 0.6546 - val_accuracy: 0.8796\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f91e854a5c0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluation"
      ],
      "metadata": {
        "id": "XDij82_nMXAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pairs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dszK_jBzfOqh",
        "outputId": "aa73de3b-c4f7-45b4-afdf-eca17eccd8d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'post': 'some background i have been learning python for over a year now and i know some sql a bit of r and i have completed some small projects using data science data engineering practices i also know how to work in excel but i do not really have experience using databases i am totally willing to make the time and money investment in something like a bootcamp and i have the means to do fulltime training but i do not want to do this if there is a better faster way to get into the industry what i really want to know is what can i do that will get me a job in the field asap is there some specific bootcamp that will make this happen if so what are the best bootcamps or some particular tech skill i could learn that would basically guarantee that i am hireable very soon if i something like learned microsoft sql server or tableau and given my other skills would this be likely to get me hired i have been looking into bootcamps like thinkful springboard and data application lab the concern i have about these is actually that i already know a lot of the stuff they teach and i am worried that these will be a waste of time and not elevate me to where i want to be i also worry about it taking months to complete these programs as they estimate i would like to be finished in no more than about months thoughts',\n",
              " 'title': 'i want to transition into the data scienceanalyst or related field rather than asking whether i should choose a particular boootcamp or learn some language i would like to hear opinions on what path should i choose that will land me a job of some kind in the field as soon as possible'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'create headline for post: '\n",
        "\n",
        "for test_input_text in ['Hello all - I have an upcoming live case interview at CVS for their data science role. Can yall please share your experience of how the interview went? \\\n",
        "                        Did it involve quantitative analysis or was it just qualitative in nature? \\\n",
        "                        Did they supplement the discussion with some data? \\\n",
        "                        Were they expecting a technical ML solution? Or did they only want to guage the candidates thought process and structured communication?']:\n",
        "\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='tf')\n",
        "    test_output_ids = t5_model.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True,\n",
        "                               clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcF2_j1heaMg",
        "outputId": "4709daf1-7c19-47a7-acb0-cfb91889b3e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py:838: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['upcoming live case interview']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the saved model weights\n",
        "checkpoint_dir = 'drive/My Drive/W266/Final Project/model_checkpoints/'\n",
        "\n",
        "checkpoint_filepath = checkpoint_dir + 't5_reddit_weights.03-0.88.hdf5'\n",
        "model_wrapper.load_weights(checkpoint_filepath)"
      ],
      "metadata": {
        "id": "jylQ0HsQzAIe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Still works?\n",
        "for test_input_text in ['Hello all - I have an upcoming live case interview at CVS for their data science role. Can yall please share your experience of how the interview went? \\\n",
        "                        Did it involve quantitative analysis or was it just qualitative in nature? \\\n",
        "                        Did they supplement the discussion with some data? \\\n",
        "                        Were they expecting a technical ML solution? Or did they only want to guage the candidates thought process and structured communication?']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='tf')\n",
        "    test_output_ids = t5_model.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True,\n",
        "                               clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gABnQvc-dNs8",
        "outputId": "2f41e1f3-2fb8-47e6-e87d-6b39cec94928"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py:838: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['upcoming live case interview']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Candidates: these are the actual Reddit titles from the test set\n",
        "EXAMPLES_NUM = 10\n",
        "\n",
        "test_posts = [t5_tokenizer(item['post'], max_length=512, truncation=True, return_tensors='tf') for item in test_pairs[:EXAMPLES_NUM]]\n",
        "\n",
        "# Generating output ids for each tokenized post\n",
        "test_output_ids = [t5_model.generate(post['input_ids'],\n",
        "                                    num_beams=3,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    num_return_sequences=1,  # returns the # of sequences for each post\n",
        "                                    max_length=128) for post in test_posts]"
      ],
      "metadata": {
        "id": "fnQforboLWK1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t5_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDbjLj9DkfdH",
        "outputId": "d6590114-b47d-48a8-e78b-08f2d7c922a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tft5_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " shared (Embedding)          multiple                  24674304  \n",
            "                                                                 \n",
            " encoder (TFT5MainLayer)     multiple                  109628544 \n",
            "                                                                 \n",
            " decoder (TFT5MainLayer)     multiple                  137949312 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 222903552 (850.31 MB)\n",
            "Trainable params: 222903552 (850.31 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXAMPLES_NUM = len(test_pairs)\n",
        "\n",
        "print(f\"Generating {EXAMPLES_NUM} samples ... \")\n",
        "\n",
        "# Generate input ids for each tokenized post\n",
        "test_posts = [t5_tokenizer(item['post'], max_length=512, truncation=True, return_tensors='tf') for item in test_pairs[:EXAMPLES_NUM]]\n",
        "\n",
        "# Generating output ids for each tokenized post\n",
        "start_time = time.time()\n",
        "test_output_ids = [t5_model.generate(post['input_ids'],\n",
        "                                    num_beams=3,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    num_return_sequences=1,  # returns the # of sequences for each post\n",
        "                                    max_length=128) for post in test_posts]\n",
        "\n",
        "end_time = time.time()\n",
        "run_time = end_time - start_time\n",
        "\n",
        "print(f\"Generate Time elapsed: {run_time} seconds.\\n\")\n",
        "\n",
        "# initialize list of candidates\n",
        "candidates = []\n",
        "\n",
        "# Decode each output in the batch of generated outputs\n",
        "for out_ids in test_output_ids:\n",
        "    # Decode each output in the batch of generated outputs\n",
        "    candidates_batch = [t5_tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for ids in out_ids]\n",
        "    candidates.extend(candidates_batch)  # Extend the main candidates list with the batch\n",
        "\n",
        "# Inspect references list\n",
        "for idx, candidate in enumerate(candidates):\n",
        "  print(\"Candidate #\",idx,\":\\t \",candidate)\n",
        "  #print(\"\\t\",candidate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhL8Op59lT2G",
        "outputId": "dd10639d-f17a-4d54-aa33-19434b71e9b6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 100 samples ... \n",
            "Generate Time elapsed: 576.8904876708984 seconds.\n",
            "\n",
            "Candidate # 0 :\t  how do you organize your results\n",
            "Candidate # 1 :\t  request flickmatrix movie score data\n",
            "Candidate # 2 :\t  should i do a phd in data science\n",
            "Candidate # 3 :\t  what are the job prospects of a data scientist\n",
            "Candidate # 4 :\t  data analyst job or not\n",
            "Candidate # 5 :\t  what is the best way to build an api for describing and defining entities in a video\n",
            "Candidate # 6 :\t  is it normal to have a bad working environment for an analytics manager\n",
            "Candidate # 7 :\t  what is the best way to get a masters in ds\n",
            "Candidate # 8 :\t  is it a good idea to learn python without using other languages\n",
            "Candidate # 9 :\t  looking for crime scene datasets\n",
            "Candidate # 10 :\t  what is the best bootcamp for getting a job in the field asap\n",
            "Candidate # 11 :\t  python sngram objs\n",
            "Candidate # 12 :\t  what is the name of this model\n",
            "Candidate # 13 :\t  how to set up a recurring job in retraining\n",
            "Candidate # 14 :\t  question about mahalanobis distance matching\n",
            "Candidate # 15 :\t  should i apply for a data science internship\n",
            "Candidate # 16 :\t  how many yoe should i expect from a career in data science\n",
            "Candidate # 17 :\t  ms in data science\n",
            "Candidate # 18 :\t  phds in data science\n",
            "Candidate # 19 :\t  how do i get a job in data science for my phd\n",
            "Candidate # 20 :\t  python for data science\n",
            "Candidate # 21 :\t  what is the best way to get a master is in data science\n",
            "Candidate # 22 :\t  college athletics financial information database\n",
            "Candidate # 23 :\t  harvard business review question\n",
            "Candidate # 24 :\t  bioinformaticians vs data science\n",
            "Candidate # 25 :\t  how hard is it to learn sql for someone who has never been a data analyst\n",
            "Candidate # 26 :\t  i got rejected for a data analyst internship and ms in ds need help\n",
            "Candidate # 27 :\t  should i try to get a job in data science instead of engineering\n",
            "Candidate # 28 :\t  what is the best way to get a job in data analysis\n",
            "Candidate # 29 :\t  which course should i take for my masters in data science\n",
            "Candidate # 30 :\t  rescinding an offer for a data analyst position\n",
            "Candidate # 31 :\t  what is your data science team like\n",
            "Candidate # 32 :\t  is it possible to get into data science from a nontechengineering background\n",
            "Candidate # 33 :\t  is there a way to solve the problem of inference in deep learning\n",
            "Candidate # 34 :\t  how to apply gradient descent to a vector representation\n",
            "Candidate # 35 :\t  retraining adstock model to account for the lag impact of media\n",
            "Candidate # 36 :\t  is there a model to solve rubik is cube\n",
            "Candidate # 37 :\t  looking for people to interview for a data visualization website\n",
            "Candidate # 38 :\t  how do i explain the direction of a vector in pytorch\n",
            "Candidate # 39 :\t  how do i set up a behavior flow for my app\n",
            "Candidate # 40 :\t  what should i be prepared for a data science interview\n",
            "Candidate # 41 :\t  what is the future of data science\n",
            "Candidate # 42 :\t  economics or computer science bs\n",
            "Candidate # 43 :\t  python ltstdingt error when using sklearn\n",
            "Candidate # 44 :\t  how to divide a timeseries into batches\n",
            "Candidate # 45 :\t  how do i organize my scripts\n",
            "Candidate # 46 :\t  udacity vs moocs for data science\n",
            "Candidate # 47 :\t  is my learning curve normal or does it just mean that my model is performing worse than a normal one\n",
            "Candidate # 48 :\t  how do you manage different models on different datasets\n",
            "Candidate # 49 :\t  google analytics pageviews not displaying upticks\n",
            "Candidate # 50 :\t  advice on transitioning from retail to data analytics\n",
            "Candidate # 51 :\t  q blocks cloud gpu for ml applications\n",
            "Candidate # 52 :\t  looking for a project idea for opencog python and gremlin\n",
            "Candidate # 53 :\t  can we talk about a mathematical form of the function encoded by neural networks\n",
            "Candidate # 54 :\t  what is the name of the book statistical methods and why should data teams care about them\n",
            "Candidate # 55 :\t  how to install cuda on gcp\n",
            "Candidate # 56 :\t  delivering models to clients using flashaiio\n",
            "Candidate # 57 :\t  how can i express a nonmathy layperson is knowledge of generalization error learning curve\n",
            "Candidate # 58 :\t  what is the industry is approach to dealing with npv transactions\n",
            "Candidate # 59 :\t  senior analyst or senior data scientist\n",
            "Candidate # 60 :\t  best way to group a large number of genres into one hot encoding\n",
            "Candidate # 61 :\t  how do i organize a data set\n",
            "Candidate # 62 :\t  what should i do to prepare for my first job as a data scientist\n",
            "Candidate # 63 :\t  swish a new approach to searching for activation functions\n",
            "Candidate # 64 :\t  is it possible to use ml to predict customer data in aws\n",
            "Candidate # 65 :\t  python cnn algorithm to categorical reddit submission\n",
            "Candidate # 66 :\t  what are the most important skills for a data scientist\n",
            "Candidate # 67 :\t  what is the best way to get social media data into a database\n",
            "Candidate # 68 :\t  how does the dot product give us a decision boundary\n",
            "Candidate # 69 :\t  ms in data science\n",
            "Candidate # 70 :\t  what kind of industry job should i get with interest in text analysis\n",
            "Candidate # 71 :\t  how to train a model to predict weights of movies\n",
            "Candidate # 72 :\t  best models for isupply management\n",
            "Candidate # 73 :\t  how to get into data science without enrolling in school\n",
            "Candidate # 74 :\t  what is your routine as a data analyst\n",
            "Candidate # 75 :\t  need help finding data for firstlevel administrative divisions\n",
            "Candidate # 76 :\t  what is the industry best practice to get all the data from previos model\n",
            "Candidate # 77 :\t  what is the architecture of mysql database\n",
            "Candidate # 78 :\t  how to tweak feature priority for feature selection\n",
            "Candidate # 79 :\t  help with english togerman translation\n",
            "Candidate # 80 :\t  rounding heuristic for time series prediction\n",
            "Candidate # 81 :\t  isps can be used to hide your ip data\n",
            "Candidate # 82 :\t  pytorch tensorflow error\n",
            "Candidate # 83 :\t  how to retrain a cnn model\n",
            "Candidate # 84 :\t  how hard is it to get a job in data science\n",
            "Candidate # 85 :\t  econometrics vs data science\n",
            "Candidate # 86 :\t  is there practical applications of synthetic datasets\n",
            "Candidate # 87 :\t  spacy vs tfidf\n",
            "Candidate # 88 :\t  multitouch markov attribution\n",
            "Candidate # 89 :\t  how to prepare for a data analyst interview\n",
            "Candidate # 90 :\t  how to filter out of dutch campaigns in google data studio\n",
            "Candidate # 91 :\t  topic modeling in python\n",
            "Candidate # 92 :\t  how to postprocess the segmentation masks\n",
            "Candidate # 93 :\t  keras wavenet output shape\n",
            "Candidate # 94 :\t  career change from physics to data science\n",
            "Candidate # 95 :\t  what are the best data science tools for my project\n",
            "Candidate # 96 :\t  difference between an algorithm and a model\n",
            "Candidate # 97 :\t  need help creating an ai to play eve online\n",
            "Candidate # 98 :\t  what else can i do to help me become a data scientist\n",
            "Candidate # 99 :\t  help with nlp recommendation engine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# References: we will compare the generated titles against these actual test values\n",
        "start_time = time.time()\n",
        "\n",
        "# reference list\n",
        "references = [item['title'] for item in test_pairs[:EXAMPLES_NUM]]\n",
        "\n",
        "# original post\n",
        "references_post = [item['post'] for item in test_pairs[:EXAMPLES_NUM]]\n",
        "\n",
        "# Inspect references list\n",
        "for idx, reference in enumerate(references):\n",
        "    print(\"Reference #\",idx,\":\\t \",reference)\n",
        "\n",
        "end_time = time.time()\n",
        "run_time = end_time - start_time\n",
        "print(f\"Generate Time elapsed: {run_time} seconds.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSKCgyw-DtSj",
        "outputId": "e0ececde-a9e8-4d2c-a5de-bed9231c8b09"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference # 0 :\t  what does a good scikitlearn workflow look like\n",
            "Reference # 1 :\t  dataset request ranking best films of all time\n",
            "Reference # 2 :\t  ds masters subsequent phd studies\n",
            "Reference # 3 :\t  career change to data science\n",
            "Reference # 4 :\t  how should i start\n",
            "Reference # 5 :\t  building an api query language for rich data like images and video\n",
            "Reference # 6 :\t  joining firm company culture concern\n",
            "Reference # 7 :\t  is a pricey masters degree worth it\n",
            "Reference # 8 :\t  seeking advice for learning path\n",
            "Reference # 9 :\t  crime scene datasetphoto and video database\n",
            "Reference # 10 :\t  i want to transition into the data scienceanalyst or related field rather than asking whether i should choose a particular boootcamp or learn some language i would like to hear opinions on what path should i choose that will land me a job of some kind in the field as soon as possible\n",
            "Reference # 11 :\t  psngram linguistic features for improving machine learning and deep learning model accuracy for the first time in python new release\n",
            "Reference # 12 :\t  does anyone use any type of gradient smoothing for binned data\n",
            "Reference # 13 :\t  setting up a model for retraining in production\n",
            "Reference # 14 :\t  is mahalanobisdistance matching between points not compatible with onehotencoded datasets\n",
            "Reference # 15 :\t  education level on applications\n",
            "Reference # 16 :\t  how many yoe should i expect to have before i hit kk salary range\n",
            "Reference # 17 :\t  a potential blind spot for new data science degrees\n",
            "Reference # 18 :\t  should i get a mastersphd\n",
            "Reference # 19 :\t  two years until i apply for jobs what should i work on\n",
            "Reference # 20 :\t  why python is used in data science how data science courses help in a successful career post covid pandemic\n",
            "Reference # 21 :\t  how to transition from data analyst to a data science role\n",
            "Reference # 22 :\t  knight commission unveils new college sports financial database\n",
            "Reference # 23 :\t  how to learn more about amazon is automation\n",
            "Reference # 24 :\t  confusion regarding bioinformatics and health informatics\n",
            "Reference # 25 :\t  is it hard to become an entry level data analyst\n",
            "Reference # 26 :\t  what is my wrong with my resume\n",
            "Reference # 27 :\t  data scientist looking for a change\n",
            "Reference # 28 :\t  looking for insight on data analysis on these job description\n",
            "Reference # 29 :\t  masters course selection\n",
            "Reference # 30 :\t  recent grad who had his offer revoked and interview cancelled\n",
            "Reference # 31 :\t  what capabilities does your team have\n",
            "Reference # 32 :\t  transitioning to ds from financeinvestment background\n",
            "Reference # 33 :\t  overcome caveats on using deep learning for faster inference on limited performance availability\n",
            "Reference # 34 :\t  increase similarity between two vectors whilst maintaining similarity to other vectors\n",
            "Reference # 35 :\t  marketing mix model help\n",
            "Reference # 36 :\t  project help\n",
            "Reference # 37 :\t  data enthusiasts paid interview graduate research project\n",
            "Reference # 38 :\t  i need help representing data about d changes in vector orientation\n",
            "Reference # 39 :\t  why cannot i view any data in the behavior flow\n",
            "Reference # 40 :\t  interviews what do they want to know about my past projects\n",
            "Reference # 41 :\t  confused and lost at where to start\n",
            "Reference # 42 :\t  bs in economics or bs in computer science\n",
            "Reference # 43 :\t  why am i getting error while performing fit transform\n",
            "Reference # 44 :\t  a few questions on modelling time series data with recurrent neural networks\n",
            "Reference # 45 :\t  how can i have an organized workflow in r\n",
            "Reference # 46 :\t  moocs for university student\n",
            "Reference # 47 :\t  can a learning rate graph look unusual and weird\n",
            "Reference # 48 :\t  how do you manage multiple experiments in ml\n",
            "Reference # 49 :\t  pages i am not hosting are showing in google analytics\n",
            "Reference # 50 :\t  advice on switching career to development sector\n",
            "Reference # 51 :\t  affordable supercomputers for ai data science amp design\n",
            "Reference # 52 :\t  a python developer with too much free time needs your input\n",
            "Reference # 53 :\t  would having a mathematically described function that approximates the function encoded in a neural network help researchers understand how a neural network gets its output\n",
            "Reference # 54 :\t  what is a data catalog and why should you even care\n",
            "Reference # 55 :\t  how to get google colab environmentimage to gcp\n",
            "Reference # 56 :\t  host and serve your scikitlearn tensorflow and pytorch models within minutes\n",
            "Reference # 57 :\t  in layperson is terms how much does deep learning performance scale with training examples\n",
            "Reference # 58 :\t  business process change implementation\n",
            "Reference # 59 :\t  what exactly is senior analyst data scientist role i am confused with responsibilities and corresponding tc\n",
            "Reference # 60 :\t  dealing with when observations have a variable list of traits\n",
            "Reference # 61 :\t  creating a data set from scratch having trouble\n",
            "Reference # 62 :\t  advice for a beginner\n",
            "Reference # 63 :\t  article the swish activation function\n",
            "Reference # 64 :\t  creating a customer attrition model for my work\n",
            "Reference # 65 :\t  p automatic reddit categorizer update first version working\n",
            "Reference # 66 :\t  is data structures and algorithms worth spending my time on before i start my job\n",
            "Reference # 67 :\t  social media management platform with reporting api\n",
            "Reference # 68 :\t  how do kernels in svms give rise to decision boundaries\n",
            "Reference # 69 :\t  looking for post undergrad advice\n",
            "Reference # 70 :\t  what kind of industry job can i get with interest in text analysis\n",
            "Reference # 71 :\t  day recommender systems\n",
            "Reference # 72 :\t  best models for managing inventory supplydemand\n",
            "Reference # 73 :\t  curriculum to get started\n",
            "Reference # 74 :\t  what is your routine like as a data analyst\n",
            "Reference # 75 :\t  get data for all firstlevel administrative divisions for all countries\n",
            "Reference # 76 :\t  logistic regression for predicting goodbad customers scoring model development\n",
            "Reference # 77 :\t  tapping this sub for some resources i am looking to a tap into some api feeds b do stuff with the resulting data graph manipulate etc c deliver it in the form of a data product web mobile etc\n",
            "Reference # 78 :\t  question about feature importanceselection\n",
            "Reference # 79 :\t  random translation output with pretrained opennmtpy model\n",
            "Reference # 80 :\t  how do i combine several time series into one given a dataset of how the end result should be\n",
            "Reference # 81 :\t  do not allow your isp and government to save your browsing history\n",
            "Reference # 82 :\t  linear regression on tensorflow all nans\n",
            "Reference # 83 :\t  retraining cnn with new data\n",
            "Reference # 84 :\t  question about learning sql after learning data analysisbased python pandas library\n",
            "Reference # 85 :\t  economist that just started a msc in econ specializing in data science and numerical methods should i just drop it and focus on selflearning everything\n",
            "Reference # 86 :\t  do you think generated datasets when realworld ones are scarce can help training and if so who could find them useful\n",
            "Reference # 87 :\t  spacy word vectors and sklearn\n",
            "Reference # 88 :\t  lead scoring model for marketing\n",
            "Reference # 89 :\t  is it okay to email hiring manager if the recruiter is not responding\n",
            "Reference # 90 :\t  google data studio create language button\n",
            "Reference # 91 :\t  newbie why so little machine learning model package in r\n",
            "Reference # 92 :\t  need help postprocessing image segmentation masks\n",
            "Reference # 93 :\t  wavenet output dimensions\n",
            "Reference # 94 :\t  has anyone here made a career change into data science what was your experience like\n",
            "Reference # 95 :\t  need data science project supervisionresources\n",
            "Reference # 96 :\t  difference between model and algorithm\n",
            "Reference # 97 :\t  eve ai\n",
            "Reference # 98 :\t  entering data science with economicseconometricsfinance background\n",
            "Reference # 99 :\t  gadget accessories recommender system need some ideas amp questions to answer\n",
            "Generate Time elapsed: 0.05757713317871094 seconds.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_results = rouge.compute(predictions=candidates, references=references)\n",
        "\n",
        "pd.DataFrame([rouge_results])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "480464b5552b44fd9cedf1d8ac1278c4",
            "5667e9f227094da4858770d5461ab7e4",
            "b87cfe72a8a141a2b9f53466e92fb493",
            "616c6c10c85243d1b27e2e878d1288ab",
            "5c8e6d275dfa45f78a8ed4834c48573c",
            "8d7ae152c928489f9ec1136d3486328e",
            "e98529d013b045839b91930aed7eecd2",
            "0d0e3bb682844a049d2ee1301ab3edb9",
            "6669f9160fc34ee5bad9f488dc4fcf20",
            "35c6aceadcb04a04a987c44ae29fa724",
            "acba7e889b274224bef4ed15a5f22d49"
          ]
        },
        "id": "cX8GOLiWMLpf",
        "outputId": "72c2bf1b-a4e0-445c-f358-ce0766b97f46"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "480464b5552b44fd9cedf1d8ac1278c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     rouge1   rouge2    rougeL  rougeLsum\n",
              "0  0.239797  0.08335  0.219164   0.220178"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01d0e762-f57b-40b3-86b6-c9d6797e3235\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rouge2</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>rougeLsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.239797</td>\n",
              "      <td>0.08335</td>\n",
              "      <td>0.219164</td>\n",
              "      <td>0.220178</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01d0e762-f57b-40b3-86b6-c9d6797e3235')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-01d0e762-f57b-40b3-86b6-c9d6797e3235 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-01d0e762-f57b-40b3-86b6-c9d6797e3235');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.239797284265594,\n        \"max\": 0.239797284265594,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.239797284265594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.08335024505497905,\n        \"max\": 0.08335024505497905,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.08335024505497905\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.2191643666502201,\n        \"max\": 0.2191643666502201,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.2191643666502201\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeLsum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.22017793601766072,\n        \"max\": 0.22017793601766072,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.22017793601766072\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "bleu_results = bleu.compute(predictions=candidates, references=references)\n",
        "\n",
        "pd.DataFrame([bleu_results])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "87795ebee40546d3b53798f72e74fb1c",
            "b922fd478184487da4c08856664a74fd",
            "1c49c2c1dc224dc3a09e2840613bb52f",
            "b9bb066837044a768d2e5864b33c17d8",
            "f6e5c1f87822454989e00107589ef330",
            "cacaea4847544e9caf5f065ab39fcd9b",
            "8d8a1abd29a146f6a7a9a26a9f664bc7",
            "f71beeeeec4d4bd1bc0c44cd0f1e9aef",
            "5174ffd4b1244b489121c70807c0e0f9",
            "bb0432b5cbb4489ea4df9b071c5fd2ca",
            "4436a4b72c65446983e738a2db565c1f",
            "29385bb2395e48a698bc877b77f23784",
            "16a32fc430824b37a43b704eeea7d899",
            "7fc96d2bab58468f96bd651f543854ab",
            "f3a4651a683447959e3132fa5eb94540",
            "ac1bb13e17ac48e4b577b19f9346cd51",
            "841bbd2036ce41fd95d579c3972c0e03",
            "1e2db60c9a934a9c8186032ca9621c6f",
            "9a9c11f203034d239d5ba27866a8d559",
            "dc32d3aa46a840a6b92d1b7131f4fa7b",
            "12d9b98e5d9d4bd4913feb699219e057",
            "ed145b11bb784a53990de5b83b69c7f3",
            "d4cfae8482d742ae9f31c1a7621fdde6",
            "838f07fab82c4f87b4641aa0a24a3bd6",
            "1d68874b660847bcb775d7f2d4159133",
            "f1f646cf8fc94904b7a6c8209399ae48",
            "4db990bc08bd4752826058e6f6e7984f",
            "733ece9c725e4b0ea6851991f83f8bbd",
            "cf947d3449e44bd9874ca3fd28952a82",
            "a13302d76f694236bbd8488f4a346d38",
            "23768244cf3843f58bd0f13949d21b10",
            "155123b6e45c4c99b20080ad9890df1e",
            "b9f2571d12cd4f1bab13ece37cc2af0d"
          ]
        },
        "id": "sD0KcsxHTSTR",
        "outputId": "e2bd0ac2-3bea-4280-c655-b3dae6baf5d3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87795ebee40546d3b53798f72e74fb1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29385bb2395e48a698bc877b77f23784"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4cfae8482d742ae9f31c1a7621fdde6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       bleu                                         precisions  \\\n",
              "0  0.059214  [0.2523041474654378, 0.09244791666666667, 0.03...   \n",
              "\n",
              "   brevity_penalty  length_ratio  translation_length  reference_length  \n",
              "0         0.903587       0.90795                 868               956  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a0828a1-dade-41a8-8e2c-2e6b964b4de7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bleu</th>\n",
              "      <th>precisions</th>\n",
              "      <th>brevity_penalty</th>\n",
              "      <th>length_ratio</th>\n",
              "      <th>translation_length</th>\n",
              "      <th>reference_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.059214</td>\n",
              "      <td>[0.2523041474654378, 0.09244791666666667, 0.03...</td>\n",
              "      <td>0.903587</td>\n",
              "      <td>0.90795</td>\n",
              "      <td>868</td>\n",
              "      <td>956</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a0828a1-dade-41a8-8e2c-2e6b964b4de7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3a0828a1-dade-41a8-8e2c-2e6b964b4de7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3a0828a1-dade-41a8-8e2c-2e6b964b4de7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.05921403259417755,\n        \"max\": 0.05921403259417755,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.05921403259417755\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precisions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"brevity_penalty\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9035873550278737,\n        \"max\": 0.9035873550278737,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9035873550278737\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"length_ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9079497907949791,\n        \"max\": 0.9079497907949791,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9079497907949791\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"translation_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 868,\n        \"max\": 868,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          868\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 956,\n        \"max\": 956,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          956\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Candidates and References\n",
        "\n",
        "df = pd.DataFrame({'ref_post': references_post,\n",
        "                   'ref_title': references,\n",
        "                   'candidate_title': candidates\n",
        "                   })\n",
        "# Inspect DF\n",
        "print(df.head())\n",
        "\n",
        "# Export to CSV\n",
        "df.to_csv('drive/My Drive/W266/t5_fine_tune_results.csv', index=True, index_label='Index_ID')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2i9HqgV2ea6",
        "outputId": "9a0efeb2-d8ab-4219-a91d-72981daba0a5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            ref_post  \\\n",
            "0  suppose you are on a data science team and are...   \n",
            "1  i would like to request a datasite that ranks ...   \n",
            "2  i have been considering a midcareer switch fro...   \n",
            "3  hi everyone i am a practicing attorney conside...   \n",
            "4  hey everyone i want to become a data analyst a...   \n",
            "\n",
            "                                         ref_title  \\\n",
            "0  what does a good scikitlearn workflow look like   \n",
            "1   dataset request ranking best films of all time   \n",
            "2                ds masters subsequent phd studies   \n",
            "3                    career change to data science   \n",
            "4                               how should i start   \n",
            "\n",
            "                                  candidate_title  \n",
            "0                how do you organize your results  \n",
            "1            request flickmatrix movie score data  \n",
            "2               should i do a phd in data science  \n",
            "3  what are the job prospects of a data scientist  \n",
            "4                         data analyst job or not  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AEn2bpzYEbC3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}